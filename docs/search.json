[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Yahya!\nThis blog is where I share my learning journey in machine learning, coding, and projects I’m working on."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html",
    "href": "posts/when-numbers-lie/index.html",
    "title": "When Numbers Lie",
    "section": "",
    "text": "I came to machine learning because I believed in its elegance—the way a few lines of code could make predictions, optimize decisions, and maybe even eliminate human bias. Studying philosophy alongside computer science, I have always wondered if utilizing the machine, supported with some rigorous math, can eliminate some tendencies in some humans to perpetuate injustice and unfairness. But somewhere along the way, this class made me ask: Whose data is this model trained on? What decisions does it reinforce? And who gets left behind when we define “fairness” with math?\nThese questions became even more urgent as I read Arvind Narayanan’s 2022 James Baldwin Lecture, The Limits of the Quantitative Approach to Discrimination (Narayanan 2022). Narayanan argues that current quantitative methods often serve to justify the status quo more than they challenge it. That’s a bold statement—especially for a field that prides itself on objectivity and precision. But as I sat with this claim, I realized: it resonates. At a time when the U.S. government is investing over $500 billion into machine learning (Reuters Staff 2025)—much of it flowing into public systems that affect millions of lives—it’s critical to examine what ethical foundation, if any, these models are built on. Especially during a time where the government is displaying tendencies to practice radical, and often problematic, interpretations of justice, using means that I consider unethical and are against what states can do. Is machine learning another tool for them to justify their actions?\nThis isn’t a theoretical concern. The U.S. government has reportedly begun employing AI-powered agents to monitor the social media of international students, scanning for signs of “support” for terrorist organizations, but also any signs of criticizing the complicity of the government in war crimes happening right now in the Middle East (Fried 2025; Weiss and Parker 2025). This process—untransparent, unauditable, and unchallengeable—has led to the flagging and deportation of students based on ambiguous indicators and culturally uncontextualized speech. What’s happening here is exactly what Narayanan warns about: using mathematical tools to assess fairness based on the same logics that have historically led to unfairness. These models are embedded with political and cultural assumptions, yet shielded from scrutiny by a veil of statistical legitimacy.\n\n\nNarayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves.\n\n\n\nTo be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist.\n\n\n\nA more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art.\n\n\n\nTo me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me.\n\n\n\nSo, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "href": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "title": "When Numbers Lie",
    "section": "",
    "text": "Narayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "href": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "href": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "title": "When Numbers Lie",
    "section": "",
    "text": "A more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#redefining-fairness",
    "href": "posts/when-numbers-lie/index.html#redefining-fairness",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "href": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "title": "When Numbers Lie",
    "section": "",
    "text": "So, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html",
    "href": "posts/understanding-feature-separability/index.html",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#abstract",
    "href": "posts/understanding-feature-separability/index.html#abstract",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#discussion",
    "href": "posts/understanding-feature-separability/index.html#discussion",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "Discussion",
    "text": "Discussion\nThrough this analysis, I gained insights into the importance of feature selection in classification tasks. My IoU-based approach helped quantify feature separability, leading to better-informed choices. Visualization techniques, such as pair plots and decision boundaries, provided valuable interpretability into the model’s behavior. The performance comparison between Logistic Regression, Decision Trees, and Random Forest demonstrated that Decision Trees models generalize best, minimizing overfitting while maintaining high accuracy. One key takeaway is that some species are inherently harder to classify due to overlapping feature distributions, reinforcing the need for careful feature engineering and model selection. Future improvements could involve non-linear models like SVMs and better feature selection to further enhance classification accuracy. This project shows how data-driven feature selection and model evaluation can lead to meaningful and accurate species classification."
  },
  {
    "objectID": "posts/Auditing-bias/index.html",
    "href": "posts/Auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, BasicProblem\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression"
  },
  {
    "objectID": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "href": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "title": "Auditing Bias",
    "section": "patterns of disparity",
    "text": "patterns of disparity\nTo explore deeper patterns of disparity, we now perform an intersectional analysis by examining the proportion of individuals earning over $50K across combined gender and race subgroups. We add the RAC1P column (race) from the original ACS dataset to our DataFrame and map it to readable race labels using race_map.\nWe then create a new column that combines gender and race into a single intersectional group (e.g., “M & White”, “F & Black”). By grouping on this combined variable and computing the mean of the label, we obtain the proportion of high-income individuals within each intersectional group.\nFinally, we visualize these proportions with a bar chart to better observe the disparities across different gender-race combinations. This helps highlight how overlapping identities can influence economic outcomes and inform fairness analysis in the model.\n\ndf_intersection = df.copy()\ndf_intersection[\"RAC1P\"] = acs_data.loc[df.index, \"RAC1P\"]\n\n# Map RAC1P to human-readable race labels.\ndf_intersection[\"Race\"] = df_intersection[\"RAC1P\"].map(race_map)\n\n# Create an intersectional grouping variable using both Gender and Race.\ndf_intersection[\"Gender_Race\"] = df_intersection[\"Gender\"] + \" & \" + df_intersection[\"Race\"]\n\n# Compute the proportion of positive labels for each intersectional group.\nintersectional_stats = df_intersection.groupby(\"Gender_Race\")[\"label\"].mean()\nprint(\"\\nProportion of positive labels by Gender & Race group:\")\nprint(intersectional_stats)\n\n# Visualize the intersectional trends with a bar chart.\nplt.figure(figsize=(12, 6))\nsns.barplot(x=intersectional_stats.index, y=intersectional_stats.values)\nplt.xlabel(\"Gender & Race Group\")\nplt.ylabel(\"Proportion with label == 1\")\nplt.title(\"Intersectional Proportions by Gender and Race\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nProportion of positive labels by Gender & Race group:\nGender_Race\nF & Black                               0.143632\nF & Native                              0.137795\nF & Native Hawaiian/Pacific Islander    0.150943\nF & Other                               0.125604\nF & Two or More                         0.000000\nF & White                               0.136007\nM & Black                               0.256418\nM & Native                              0.240816\nM & Native Hawaiian/Pacific Islander    0.204545\nM & Other                               0.287402\nM & Two or More                         0.250000\nM & White                               0.263107\nName: label, dtype: float64\n\n\n\n\n\n\n\n\n\nThe results of the intersectional analysis show consistent disparities in high-income proportions across gender and race combinations. For females, the proportion of individuals earning over $50K ranges from 12.56% to 15.09% across most racial groups, with White females at 13.60% and Black females at 14.36%. Notably, females identifying as Two or More races have a value of 0%, likely due to a very small sample size.\nFor males, the rates are significantly higher across the board. For example, White males are at 26.31%, Black males at 25.64%, and Other males at 28.74%. These findings reinforce the earlier observed gender gap and also highlight how racial identity further compounds disparities. This intersectional breakdown is crucial for understanding how multiple identity factors can interact to affect economic outcomes.\nThis is why choosing gender might be a good thing to explore bias through in this blog post.\nHere we train and tune a RandomForestClassifier by performing a hyperparameter search over the max_depth parameter. We loop through a range of depths in steps of 2 and use 5-fold cross-validation to evaluate model performance at each depth.\nFor each candidate max_depth, we compute the average cross-validation accuracy and track it in a dictionary. To avoid unnecessary computation, we stop the search early if the performance begins to drop.\nAt the end, we identify the best-performing max_depth based on the highest mean cross-validation accuracy. This tuned depth will be used to train our final model.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# Define the candidate max_depth values.\nstep, max_val = 2, 20\n\nmax_depth_values = [int(i * step) for i in range(1, max_val + 1)]\nresults = {}\n\nfor depth in max_depth_values:\n    model = RandomForestClassifier(max_depth=depth, random_state=42)\n    \n    # Perform 5-fold cross-validation on the training data.\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    mean_score = np.mean(cv_scores)\n    results[depth] = mean_score\n    \n    print(f\"Max Depth: {depth} - Mean CV Accuracy: {mean_score:.3f}\")\n    scores = list(results.values())\n    \n    #we're finding the minimum number of max_depth values to test\n    if len(scores) &gt; 2 and scores[-1] &lt; scores[-2]:\n        break\n\n\nbest_depth = max(results, key=results.get)\nprint(\"\\nBest max_depth:\", best_depth, \"with a Mean CV Accuracy of:\", results[best_depth])\n\nMax Depth: 2 - Mean CV Accuracy: 0.801\nMax Depth: 4 - Mean CV Accuracy: 0.842\nMax Depth: 6 - Mean CV Accuracy: 0.843\nMax Depth: 8 - Mean CV Accuracy: 0.843\nMax Depth: 10 - Mean CV Accuracy: 0.844\nMax Depth: 12 - Mean CV Accuracy: 0.845\nMax Depth: 14 - Mean CV Accuracy: 0.844\n\nBest max_depth: 12 with a Mean CV Accuracy of: 0.8447350223172189\n\n\nUsing the optimal max_depth found from cross-validation, we now train a final RandomForestClassifier on the full training data. After fitting the model, we evaluate its performance on the test set.\nWe calculate several key performance metrics: - Accuracy: the proportion of correct predictions. - Positive Predictive Value (PPV): the precision, or the proportion of predicted positives that are actually positive. - False Negative Rate (FNR): the proportion of actual positives that were misclassified as negatives. - False Positive Rate (FPR): the proportion of actual negatives that were misclassified as positives.\nThese metrics provide a comprehensive view of the model’s overall performance.\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n# start and fit a new model using the best max_depth from tuning\nbest_model = RandomForestClassifier(max_depth=best_depth, random_state=42)\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_hat = best_model.predict(X_test)\n\n# Calculate overall accuracy\naccuracy = accuracy_score(y_test, y_hat)\n\n# Compute confusion matrix: returns [[TN, FP], [FN, TP]]\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n\n# Calculate positive predictive value (PPV) i.e. precision\nppv = precision_score(y_test, y_hat)\n\n# Calculate false negative rate (FNR): FN / (FN + TP) while avoiding dividing by zero\nfnr = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n\n# Calculate false positive rate (FPR): FP / (FP + TN)\nfpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\n\nprint(\"Overall Accuracy:\", accuracy)\nprint(\"Positive Predictive Value (PPV):\", ppv)\nprint(\"False Negative Rate (FNR):\", fnr)\nprint(\"False Positive Rate (FPR):\", fpr)\n\nOverall Accuracy: 0.8486220076443372\nPositive Predictive Value (PPV): 0.6697517879680269\nFalse Negative Rate (FNR): 0.5829185223997904\nFalse Positive Rate (FPR): 0.04885790751229228\n\n\nThe model achieves an overall accuracy of 84.86%, indicating strong performance in correctly classifying income levels. The positive predictive value (PPV) is 66.98%, meaning that when the model predicts someone earns over $50K, it’s correct about two-thirds of the time.\nHowever, the false negative rate (FNR) is relatively high at 58.29%, suggesting that the model frequently fails to identify individuals who do earn more than $50K. The false positive rate (FPR) is much lower at 4.89%, meaning the model rarely misclassifies low-income individuals as high earners.\nThis trade-off indicates that while the model is cautious about predicting high income (low FPR), it may be overly conservative, leading to many missed positives (high FNR)."
  },
  {
    "objectID": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "href": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "title": "Auditing Bias",
    "section": "Model Fairness across gender groups",
    "text": "Model Fairness across gender groups\nTo evaluate the model’s fairness across gender groups, we compute key performance metrics separately for males and females. For each subgroup, we calculate:\n\nAccuracy: overall correctness within the group.\nPPV (Precision): how often predicted high-income individuals are actually high-income.\nFNR (False Negative Rate): how often actual high-income individuals are missed.\nFPR (False Positive Rate): how often low-income individuals are incorrectly classified as high-income.\n\nThese metrics are stored in a dictionary and then converted into a DataFrame for easy viewing. We also extract the FNR, FPR, and PPV for each group (male = 1, female = 2) for use in later fairness visualizations and audits.\n\n\n# Dictionary to hold metrics keyed by group\nmetrics_dict = {}\n\n# Get the unique groups from your test set\nunique_groups = np.unique(group_test)\n\n# Loop over each subgroup in group_test\nfor grp in unique_groups:\n    # Create a mask for the current group\n    mask = (group_test == grp)\n    \n    # Subset the true labels and predictions for this group\n    y_true_grp = y_test[mask]\n    y_pred_grp = y_hat[mask]\n    \n    # Calculate accuracy for the subgroup\n    grp_accuracy = accuracy_score(y_true_grp, y_pred_grp)\n    \n    # Calculate the confusion matrix: [[TN, FP], [FN, TP]]\n    tn, fp, fn, tp = confusion_matrix(y_true_grp, y_pred_grp).ravel()\n    \n    # Calculate PPV (precision) for the subgroup\n    grp_ppv = precision_score(y_true_grp, y_pred_grp)\n    \n    # Calculate False Negative Rate (FNR): FN / (FN + TP)\n    grp_fnr = fn / (fn + tp)\n    \n    # Calculate False Positive Rate (FPR): FP / (FP + tn)\n    grp_fpr = fp / (fp + tn)\n    \n    # Store the results in a dictionary keyed by group label (e.g., 1 or 2)\n    metrics_dict[grp] = {\n        \"Accuracy\": grp_accuracy,\n        \"PPV\": grp_ppv,\n        \"FNR\": grp_fnr,\n        \"FPR\": grp_fpr\n    }\n\n# Convert the dictionary to a DataFrame for display\ndf_group_metrics = pd.DataFrame.from_dict(metrics_dict, orient=\"index\")\ndf_group_metrics.index.name = \"Group\"\ndf_group_metrics.reset_index(inplace=True)\n\nprint(df_group_metrics)\n\n# Optionally, you can directly pull out metrics for each group:\nfnr_m = metrics_dict[1][\"FNR\"] if 1 in metrics_dict else None\nfpr_m = metrics_dict[1][\"FPR\"] if 1 in metrics_dict else None\nppv_m = metrics_dict[1][\"PPV\"] if 1 in metrics_dict else None\n\nfnr_f = metrics_dict[2][\"FNR\"] if 2 in metrics_dict else None\nfpr_f = metrics_dict[2][\"FPR\"] if 2 in metrics_dict else None\nppv_f = metrics_dict[2][\"PPV\"] if 2 in metrics_dict else None\n\nprint(\"\\nMale FNR:\", fnr_m, \" FPR:\", fpr_m, \" PPV:\", ppv_m)\nprint(\"Female FNR:\", fnr_f, \" FPR:\", fpr_f, \" PPV:\", ppv_f)\n\n   Group  Accuracy       PPV       FNR       FPR\n0      1  0.817064  0.774603  0.608504  0.038629\n1      2  0.879570  0.551477  0.534743  0.057487\n\nMale FNR: 0.6085038106698757  FPR: 0.03862894450489663  PPV: 0.7746031746031746\nFemale FNR: 0.5347432024169184  FPR: 0.05748709122203098  PPV: 0.5514771709937332\n\n\nThe subgroup performance metrics reveal meaningful disparities between males and females:\n\nAccuracy is higher for females (87.96%) than for males (81.71%).\nPPV (Precision) is significantly higher for males (77.46%) than for females (55.15%), indicating that when the model predicts high income, it is more correct for males.\nFNR (False Negative Rate) is worse for males (60.85%) than for females (53.47%), meaning the model misses more high-income males.\nFPR (False Positive Rate) is slightly better for males (3.86%) compared to females (5.75%), suggesting the model more often incorrectly labels females as high-income.\n\nThese results suggest a gender-based imbalance in prediction quality especially in precision which may reflect or amplify real-world income disparities and requires careful consideration when interpreting model fairness.\nHere we calculate the prevalence of high income (i.e. the proportion of individuals earning over $50K) separately for males and females in the test set. This is done by taking the mean of the binary target (y_test) within each gender group. These values will be used in the fairness analysis to determine feasible combinations of false positive and false negative rates under fixed PPV, as described in Chouldechova (2017).\n\np_m = (y_test[group_test == 1]).mean()  # Prevalence for Males\np_f = (y_test[group_test == 2]).mean()  # Prevalence for Females\np_m, p_f\n\n(0.2532249873031996, 0.13188564598067537)\n\n\nwe fix the positive predictive value (PPV) across groups by setting it to the lower of the two observed PPVs. This ensures a consistent standard of predictive parity when plotting feasible combinations of false negative and false positive rates. The common_ppv will be used to generate the fairness trade-off lines for each group.\n\ncommon_ppv = min(ppv_m, ppv_f)\nprint(\"Using common PPV =\", common_ppv)\n\nUsing common PPV = 0.5514771709937332\n\n\nTo visualize the fairness trade-offs described in Chouldechova (2017), we plot feasible combinations of false negative rate (FNR) and false positive rate (FPR) for each gender group under a fixed PPV (set to the lower of the two observed PPVs).\nWe define a function based on Equation (2.6) from the paper to compute FPR as a function of FNR, prevalence, and PPV. Using this, we generate lines for males and females by sweeping FNR values from 0 to 1.\nWe then plot: - The feasible FNR–FPR line for each group. - The observed FNR and FPR as points on the plot.\nThis visualization illustrates the trade-off between FNR and FPR under predictive parity constraints. For example, to equalize FPR between groups, one might need to significantly increase the FNR in one group, which highlights the inherent tension between different fairness criteria.\n\ndef feasible_fpr(fnr_array, p, ppv):\n    \"\"\"\n    Given an array of FNR values in [0, 1],\n    returns the corresponding FPR values from Chouldechova (2017), eq. (2.6).\n    FPR = [p * (1 - FNR) * (1 - PPV)] / [1 - p]\n    \"\"\"\n    return (p * (1 - fnr_array) * (1 - ppv)) / (1 - p)\n\n# We'll sweep FNR from 0 to 1 for plotting\nfnr_grid = np.linspace(0, 1, 200)\n\n# Compute the feasible lines for each group, \n# using the *common* PPV for both\nfpr_line_m = feasible_fpr(fnr_grid, p_m, common_ppv)\nfpr_line_f = feasible_fpr(fnr_grid, p_f, common_ppv)\n\n# Plot the feasible lines\nplt.figure(figsize=(7,5))\n\nplt.plot(fnr_grid, fpr_line_m, label=\"Feasible line (Males)\", color=\"blue\")\nplt.plot(fnr_grid, fpr_line_f, label=\"Feasible line (Females)\", color=\"orange\")\n\n# Plot the observed points for each group\nplt.scatter(fnr_m, fpr_m, color=\"blue\", marker=\"o\", s=80, \n            label=\"Observed (Males)\")\nplt.scatter(fnr_f, fpr_f, color=\"orange\", marker=\"o\", s=80, \n            label=\"Observed (Females)\")\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) Combinations under Fixed PPV = {:.3f}\".format(common_ppv))\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe observed FPR for females is higher than for males, which suggests that the model is more likely to incorrectly classify low-income females as high-income.\nThe FNR for males is higher than for females, meaning the model is missing more actual high-income males.\nIf we wanted to equalize FPR between groups, we would have to increase the FNR for males, moving it further along the blue line. This trade-off shows the conflict or tension between different fairness goals.\n\nThis visualization illustrates that achieving equalized error rates across groups requires making trade-offs that may disproportionately impact different subgroups.\n\n\nConcluding Discussion\nThe ability to predict income levels has several potential applications in both commercial and governmental settings. Companies in finance, such as banks and credit card providers, could use this model to assess credit-worthiness, loan eligibility, or target specific financial products. Marketing agencies might use similar predictions to segment consumers for advertising high-end products or services. Government agencies could employ such models for economic policy analysis, workforce development, or social welfare program distribution.\nHowever, deploying this model at a large scale carries significant risks, particularly concerning fairness and bias. Our bias audit revealed disparities in predictive performance across gender groups. Notably, the model has a higher false positive rate (FPR) for females, meaning it more often misclassifies lower-income women as high-income. Conversely, it has a higher false negative rate (FNR) for males, meaning it more frequently fails to recognize high-income males. If deployed in real-world scenarios such as hiring or loan approvals, this could systematically disadvantage certain groups, reinforcing existing economic inequalities.\nExamining different types of bias, our model does not satisfy error rate balance, as FNR and FPR differ between genders. Additionally, the calibration of the model is problematic—males have a higher precision (PPV) than females, meaning the model is more confident in its positive predictions for men than for women. This suggests potential bias in how income is modeled, reflecting either societal disparities or weaknesses in the dataset itself.\nBeyond bias, there are other concerns with deploying such a model. One key issue is data representativeness—the ACS dataset might not fully capture income distributions across different racial or socioeconomic groups. Additionally, there’s a risk of automation bias, where decision-makers might overly rely on model predictions without questioning their validity. Finally, privacy concerns arise when using sensitive demographic data for predictions, as such models could be exploited for discriminatory profiling.\nTo mitigate these issues, several steps could be taken: - Fairness constraints: Adjusting the decision threshold per group to balance FPR and FNR. - Re-weighting techniques: Ensuring training data better reflects underrepresented groups. - Explainability measures: Making the model’s predictions more interpretable to reduce blind reliance. - Human oversight: Keeping final decision-making in human hands rather than automating high-stakes outcomes.\nfinally, while predictive models can be powerful tools, deploying them responsibly requires continuous auditing, transparency, and fairness interventions to prevent unintended harm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Machine Learning Blog",
    "section": "",
    "text": "When Numbers Lie\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\nMachine Learning\n\n\nBias\n\n\nFairness\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Feature Separability in Penguin Classification\n\n\n\n\n\n\nMachine Learning\n\n\nProjects\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\nNo matching items"
  }
]