[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Yahya!\nThis blog is where I share my learning journey in machine learning, coding, and projects I’m working on."
  },
  {
    "objectID": "posts/adam/main.html",
    "href": "posts/adam/main.html",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "",
    "text": "In this blog post, I explore how different optimization algorithms behave when training a logistic regression model—from the basics of gradient descent to the more advanced Newton’s Method and Adam. What started as a simple implementation turned into a deeper dive as I ran into unexpected issues—like Newton’s Method converging painfully slowly until I figured out the importance of normalizing the Hessian. I used both synthetic data for visualization and real-world data from the Cleveland Heart Disease dataset to test performance. Along the way, I compared convergence speeds, loss reduction, and training time, learning how each optimizer has its own quirks and strengths. I end my exploration with rather an unexpected result of newton beating adam in training speed. This is either due to Newton’s method relying on the second gradient and how that could be more helpful in synthetic data, or that there is a flaw in my implementation. This post documents that journey, including what worked, what didn’t, and what I learned in the process. You can find my implementation of Logistic Regression at this GitHub link."
  },
  {
    "objectID": "posts/adam/main.html#abstract",
    "href": "posts/adam/main.html#abstract",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "",
    "text": "In this blog post, I explore how different optimization algorithms behave when training a logistic regression model—from the basics of gradient descent to the more advanced Newton’s Method and Adam. What started as a simple implementation turned into a deeper dive as I ran into unexpected issues—like Newton’s Method converging painfully slowly until I figured out the importance of normalizing the Hessian. I used both synthetic data for visualization and real-world data from the Cleveland Heart Disease dataset to test performance. Along the way, I compared convergence speeds, loss reduction, and training time, learning how each optimizer has its own quirks and strengths. I end my exploration with rather an unexpected result of newton beating adam in training speed. This is either due to Newton’s method relying on the second gradient and how that could be more helpful in synthetic data, or that there is a flaw in my implementation. This post documents that journey, including what worked, what didn’t, and what I learned in the process. You can find my implementation of Logistic Regression at this GitHub link."
  },
  {
    "objectID": "posts/adam/main.html#implementing-newton-and-adam-optimizers",
    "href": "posts/adam/main.html#implementing-newton-and-adam-optimizers",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Implementing Newton and Adam Optimizers",
    "text": "Implementing Newton and Adam Optimizers\nTo experiment with optimization algorithms beyond standard gradient descent, I implemented both Newton’s Method and the Adam Optimizer for logistic regression.\n\n\nNewton’s Method\nNewton’s Method uses second-order information to guide the weight update. Specifically, the update rule is:\n\\[\n\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\, \\mathbf{H}^{-1} \\nabla L(\\mathbf{w})\n\\]\nwhere:\n\n\\(\\nabla L(\\mathbf{w})\\) is the gradient of the loss\n\\(\\mathbf{H}\\) is the Hessian matrix of second derivatives\n\\(\\alpha\\) is a learning rate (dampening factor)\n\nI compute the Hessian using the following formula:\n\\[\n\\mathbf{H}(\\mathbf{w}) = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{D} \\mathbf{X}\n\\]\nwhere \\(D\\) is a diagonal matrix with entries \\(\\sigma(s_k)(1 - \\sigma(s_k))\\), and \\(\\sigma\\) is the sigmoid function.\nInitially, I did not normalize the Hessian by the number of samples \\(n\\), and I observed that Newton’s method converged very slowly. After normalizing it (i.e., dividing by \\(n\\)), convergence improved significantly.\n\n\n\nAdam Optimizer\nThe Adam Optimizer is a first-order method that adapts the learning rate for each parameter. It maintains two moving averages:\n\nFirst moment estimate \\(m_t\\) (mean of gradients)\nSecond moment estimate \\(v_t\\) (uncentered variance)\n\nThe update rule is:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(\\mathbf{w}) \\\\\nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(\\mathbf{w}))^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n\\mathbf{w} &= \\mathbf{w} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\end{aligned}\n\\]\nAdam typically performs well in practice even without tuning, especially when using mini-batches.\n\nBoth implementations now serve as part of my module, and I use them to compare optimization strategies on both synthetic and empirical datasets."
  },
  {
    "objectID": "posts/adam/main.html#setup",
    "href": "posts/adam/main.html#setup",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Setup",
    "text": "Setup\nWe start by importing the necessary libraries required for our experiments. I am also importing additional tools from sklearn to help me process my empirical data later on. Autoreload is enabled to automatically refresh any changes made to the external logistic.py file, which contains the implementations of the LogisticRegression model and the GradientDescentOptimizer, NewtonOptimizer, and AdamOptimizer classes.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\nimport torch\nimport numpy as np\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nTo better understand how our optimizers perform, I begin with a synthetic classification dataset. The classification_data function generates two classes of data points with Gaussian noise added for variability. Each point is given a bias term by appending a column of ones to the feature matrix.\nI also define two helper functions: draw_line and plot_perceptron_data. These will allow me to visualize the decision boundary of the learned model and how well it separates the two classes. This visual feedback is helpful for gaining intuition before moving on to more complex, real-world datasets.\n\ndef classification_data(n_points=300, noise=0.2, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
  },
  {
    "objectID": "posts/adam/main.html#experimenting-on-synthetic-data",
    "href": "posts/adam/main.html#experimenting-on-synthetic-data",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Experimenting on synthetic data",
    "text": "Experimenting on synthetic data\nI begin by applying Newton’s Method to the synthetic classification data. I instantiate a LogisticRegression model and optimize it using the NewtonOptimizer. With each iteration, I perform a parameter update and record the loss value to monitor convergence.\nAfter 100 steps, I plot the loss values across iterations. This visualization helps verify that the optimizer is minimizing the empirical risk as expected, and gives me a sense of how quickly Newton’s Method converges on this simple, low-dimensional dataset.\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\nX, y = classification_data(n_points=300, noise=0.2, p_dims=2)\nlosses = []\nfor _ in range(100):\n    opt.step(X, y, alpha=0.1)\n    losses.append(LR.loss(X, y))\n    # print(LR.loss(X, y).item())\n\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Iteration\")\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the empirical loss over 100 iterations of Newton’s Method applied to the synthetic dataset. As expected, the curve drops sharply at first and then gradually flattens, indicating that the optimizer is quickly approaching a minimum. This confirms that Newton’s Method is working and is effectively making loss converge\nTo visually evaluate the performance of our trained logistic regression model, we plot the decision boundary over the synthetic data. The red line represents the model’s learned boundary, while the two classes are shown using distinct markers. We can see that the decision boundary effectively separates the two clusters, confirming that the model has successfully learned to distinguish between the classes.\n\nplot_perceptron_data(X, y, plt.gca())\ndraw_line(LR.w, X[:, 0].min(), X[:, 0].max(), plt.gca(), color='red', label='Decision Boundary')\nplt.legend()\nplt.show()\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_87374/247323338.py:20: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])"
  },
  {
    "objectID": "posts/adam/main.html#testing-on-real-dataset",
    "href": "posts/adam/main.html#testing-on-real-dataset",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Testing on real dataset",
    "text": "Testing on real dataset\nTo move beyond synthetic data, we now turn to an empirical dataset: the Cleveland Heart Disease dataset from the UCI Machine Learning Repository. This dataset contains patient health records, and our task is to predict the presence of heart disease.\nWe start by loading the data and assigning appropriate column names. Missing values are dropped to ensure clean training. Since the original target variable indicates different levels of disease severity, we convert it to a binary classification problem: 0 for no disease and 1 for presence of disease.\nNext, we normalize all features using StandardScaler to ensure they are on a similar scale. We also append a bias column of 1s to our features.\nFinally, the dataset is split into training and testing sets, and converted into PyTorch tensors so that we can feed them into our logistic regression model. this is helpful if we decide later to evaluate on testing dataset\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\ncolumns = [\n    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \n    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n]\ndf = pd.read_csv(url, names=columns, na_values='?')\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# Convert target to binary: 0 = no disease, 1 = disease (combine 1–4)\ndf['target'] = (df['target'] &gt; 0).astype(int)\n\n# Feature matrix and target vector\nX = df.drop('target', axis=1).values\ny = df['target'].values\n\n# Normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Add bias term (column of 1s)\nX = np.hstack([X, np.ones((X.shape[0], 1))])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\nprint(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n\nTrain shape: torch.Size([237, 14]), Test shape: torch.Size([60, 14])\n\n\n\nComparing Newton’s Method and SGD\nWe now compare the performance of Newton’s Method and standard Gradient Descent on the Cleveland Heart Disease dataset.\nThe run_optimizer function runs each optimizer for a fixed number of steps, collecting the loss at every iteration. For Gradient Descent, we use a learning rate (alpha) of 0.1 and no momentum (beta = 0.0) to keep the comparison fair. Newton’s method also uses the same alpha.\nThe resulting plot shows the empirical risk (loss) over iterations for both methods.\n\ndef run_optimizer(model_class, optimizer_class, X, y, steps=10, alpha=1.0, beta=0.0):\n    model = model_class()\n    optimizer = optimizer_class(model)\n    losses = []\n\n    for i in range(steps):\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n\n        if isinstance(optimizer, GradientDescentOptimizer):\n            optimizer.step(X, y, alpha, beta)\n        else:\n            optimizer.step(X, y, alpha=alpha)  # Newton step with alpha\n\n    return losses\n\nsteps = 100\nalpha = 0.1\nbeta = 0.0\n\n# Run both optimizers\ngd_losses = run_optimizer(LogisticRegression, GradientDescentOptimizer, X_train, y_train, steps=steps, alpha=alpha, beta=beta)\nnewton_losses = run_optimizer(LogisticRegression, NewtonOptimizer, X_train, y_train, steps=steps, alpha=alpha)\n\n# Plotting the results\nplt.figure(figsize=(8, 5))\nplt.plot(range(steps), gd_losses, label=\"Gradient Descent\", marker='o')\nplt.plot(range(steps), newton_losses, label=\"Newton's Method\", marker='x')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Empirical Risk Over Iterations\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, Newton’s method makes faster progress early on because it uses curvature information from the Hessian matrix. Gradient Descent, while slower per step, still converges steadily. This experiment highlights the trade-off between per-iteration cost and convergence speed for these two optimization techniques. the step here doesn’t inform us much about speed since the idea of a step is different for both optimizers\n\n\nComparing Adam and Stochastic Gradient Descent (SGD)\nTo explore how different optimization algorithms perform under various learning rates, I ran a comparative experiment between the Adam optimizer and standard mini-batch Stochastic Gradient Descent (SGD). I used the Cleveland Heart Disease dataset introduced earlier and evaluated the empirical risk over 50 iterations.\nFor this experiment, I tested three different step-sizes: α = 0.001, 0.01, and 0.1. I fixed the batch size to 16 for both optimizers to ensure a fair comparison. Each optimizer was run with the same learning rates, and the results were plotted to track how the loss function evolves with each iteration.\nI am redefining run_optimizer to work with the new hyperparameters\n\nimport matplotlib.pyplot as plt\n\ndef run_optimizer(model_class, optimizer_class, X, y, steps=50, alpha=0.01, beta=0.0, batch_size=None):\n    model = model_class()\n    \n    if optimizer_class.__name__ == \"GradientDescentOptimizer\":\n        optimizer = optimizer_class(model)\n    else:\n        optimizer = optimizer_class(model)\n\n    losses = []\n    for _ in range(steps):\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n\n        if isinstance(optimizer, GradientDescentOptimizer):\n            optimizer.step(X, y, alpha=alpha, beta=beta)\n        else:\n            optimizer.step(X, y)\n    return losses\n\n# Setup\nsteps = 50\nbatch_size = 16\nalphas = [0.001, 0.01, 0.1]\n\nplt.figure(figsize=(10, 6))\n\n# Run Adam\nfor alpha in alphas:\n    adam_opt = lambda model: AdamOptimizer(model, alpha=alpha, batch_size=batch_size)\n    losses = run_optimizer(LogisticRegression, adam_opt, X_train, y_train, steps=steps)\n    plt.plot(range(steps), losses, label=f\"Adam α={alpha}\", linestyle=\"--\", marker='o')\n\n# Run SGD\nfor alpha in alphas:\n    losses = run_optimizer(LogisticRegression, GradientDescentOptimizer,\n                           X_train, y_train, steps=steps, alpha=alpha, beta=0.0)\n    plt.plot(range(steps), losses, label=f\"SGD α={alpha}\", linestyle=\"-\", marker='x')\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Adam vs. SGD on Heart Disease Dataset\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the resulting plot, we can observe that Adam generally achieves a lower loss more quickly than SGD, especially for the higher learning rates. Its ability to adapt the learning rate per parameter helps it converge faster and more stably in the early stages. SGD, on the other hand, shows more sensitivity to the learning rate and requires careful tuning to avoid poor convergence.\nThis experiment highlights one of Adam’s key advantages: its stability and reliance to hyperparameter choices compared to traditional gradient descent methods.\n\n\nMeasuring Time to Convergence: Newton vs. Adam\nTo compare the real-world efficiency of Newton’s method and Adam, I measured the wall-clock time each optimizer takes to reduce the training loss below a fixed threshold of 0.36. This comparison is more meaningful than simply comparing the number of steps, since Newton’s method is computationally more expensive per step, while Adam performs cheaper and smaller updates.\nI set the learning rate α = 0.4 for both optimizers and capped the maximum number of iterations at 1000. Adam used a mini-batch size of 32.\n\nimport time\nimport matplotlib.pyplot as plt\n\ndef time_to_convergence(optimizer_class, model_class, X, y, loss_threshold=0.36, max_steps=5000, alpha=0.01, batch_size=None):\n    model = model_class()\n    if optimizer_class.__name__ == \"AdamOptimizer\":\n        optimizer = optimizer_class(model, alpha=alpha, batch_size=batch_size)\n    else:\n        optimizer = optimizer_class(model)\n\n    start = time.time()\n    for step in range(max_steps):\n        loss = model.loss(X, y).item()\n        if loss &lt;= loss_threshold:\n            break\n        if isinstance(optimizer, AdamOptimizer):\n            optimizer.step(X, y)\n        else:\n            optimizer.step(X, y, alpha=alpha)\n    end = time.time()\n\n    return end - start, step + 1, loss\n\n\n# Run both optimizers\nloss_target = 0.36\nalpha_adam = 0.1\nalpha_newton = 0.1\nbatch_size = 4\n\n# Measure Adam\nadam_time, adam_steps, adam_final_loss = time_to_convergence(\n    AdamOptimizer, LogisticRegression, X_train, y_train,\n    loss_threshold=loss_target, alpha=alpha_adam, batch_size=batch_size\n)\n\n# Measure Newton\nnewton_time, newton_steps, newton_final_loss = time_to_convergence(\n    NewtonOptimizer, LogisticRegression, X_train, y_train,\n    loss_threshold=loss_target, alpha=alpha_newton\n)\n\n# Print results\nprint(f\"Adam:   {adam_time:.4f}s to reach loss {adam_final_loss:.4f} in {adam_steps} steps\")\nprint(f\"Newton: {newton_time:.4f}s to reach loss {newton_final_loss:.4f} in {newton_steps} steps\")\nprint(f\"Speedup: {newton_time / adam_time:.2f}x\")\n\n# Optional bar plot\nplt.figure(figsize=(6, 4))\nplt.bar([\"Adam\", \"Newton\"], [adam_time, newton_time], color=[\"skyblue\", \"salmon\"])\nplt.ylabel(\"Time (seconds)\")\nplt.title(f\"Time to Reach Loss &lt; {loss_target}\")\nplt.grid(axis='y')\nplt.show()\n\nAdam:   0.2594s to reach loss 0.4328 in 5000 steps\nNewton: 0.0207s to reach loss 0.3596 in 33 steps\nSpeedup: 0.08x\n\n\n\n\n\n\n\n\n\n\n\nResults\nDespite Adam running for the full 1000 steps, it failed to reach the target loss of 0.36, whereas Newton’s method converged in just 9 steps and a fraction of the time. In this specific case, Newton achieved a 6.25x speedup in reaching the target loss, highlighting the power of second-order optimization when applicable—especially on smaller datasets where computing the Hessian is not prohibitively expensive.\nWhile Adam excels in scalability and flexibility, Newton’s method offers compelling speed and precision in convergence when the cost of Hessian computation is manageable."
  },
  {
    "objectID": "posts/adam/main.html#conclusion",
    "href": "posts/adam/main.html#conclusion",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Conclusion",
    "text": "Conclusion\nThis project began with a simple goal: implement and compare a few optimization methods for logistic regression. But it quickly became an insightful learning experience that challenged my understanding of both optimization theory and practical implementation. I saw firsthand how sensitive Newton’s Method can be to numerical stability—and how one small tweak, like normalizing the Hessian, can drastically improve performance. I also learned how powerful and convenient Adam can be, especially when working with mini-batches, though it doesn’t always guarantee faster convergence in practice without careful tuning.\nTesting everything on both synthetic and real-world data helped me appreciate the trade-offs between precision, speed, and stability."
  },
  {
    "objectID": "posts/Double Descent/main.html",
    "href": "posts/Double Descent/main.html",
    "title": "Overparameterization and Double Descent",
    "section": "",
    "text": "In this project, we explore the phenomenon of double descent in overparameterized linear regression models. Using a custom feature mapping approach inspired by kernel methods, we transform 1-dimensional data into a higher-dimensional space using random feature projections. This overparameterization allows the model to capture complex, nonlinear patterns but also introduces the risk of overfitting when the number of features exceeds the number of data points. We observe that the test error initially decreases as the model gains capacity, spikes sharply around the interpolation threshold, and then decreases again as the model becomes significantly overparameterized. This counterintuitive behavior highlights the unique generalization properties of modern machine learning models and provides insights into why deep networks can perform well despite their extreme parameter counts. You can see my implementation of the closed-form linear regression at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Double Descent/main.html#abstract",
    "href": "posts/Double Descent/main.html#abstract",
    "title": "Overparameterization and Double Descent",
    "section": "",
    "text": "In this project, we explore the phenomenon of double descent in overparameterized linear regression models. Using a custom feature mapping approach inspired by kernel methods, we transform 1-dimensional data into a higher-dimensional space using random feature projections. This overparameterization allows the model to capture complex, nonlinear patterns but also introduces the risk of overfitting when the number of features exceeds the number of data points. We observe that the test error initially decreases as the model gains capacity, spikes sharply around the interpolation threshold, and then decreases again as the model becomes significantly overparameterized. This counterintuitive behavior highlights the unique generalization properties of modern machine learning models and provides insights into why deep networks can perform well despite their extreme parameter counts. You can see my implementation of the closed-form linear regression at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Double Descent/main.html#setup",
    "href": "posts/Double Descent/main.html#setup",
    "title": "Overparameterization and Double Descent",
    "section": "Setup",
    "text": "Setup\nThis cell defines the RandomFeatures class for generating random nonlinear feature maps, which is adapted from Professor Chodrow’s notes. The RandomFeatures class constructs high-dimensional representations of input data using random weight vectors and bias terms, with customizable activation functions. This is a crucial step in implementing overparameterized linear regression, allowing the model to capture complex, nonlinear relationships in the data. The default activation function is the logistic sigmoid, but it can be replaced with other functions, such as the square function, for experimentation with different feature mappings.\n\nimport torch\nimport torch.nn as nn\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\nThe Breakdown of the Normal Equation When (p &gt; n)\nWhen optimizing a linear regression model to minimize the mean-squared error loss, we often use the closed-form solution for the optimal weights:\n\\[\n\\hat{w} = (X^\\top X)^{-1} X^\\top y\n\\]\nThe issue lies in the matrix \\(X\\), particularly its invertibility. When the number of features \\(p\\) exceeds the number of data points \\(n\\), the matrix \\(X\\) must have linearly dependent columns. As a result, the operation \\(X^\\top X\\) becomes undefined in the context of matrix inversion because \\(X^\\top X\\) is not invertible. Therefore, the normal equation cannot be used when \\(p &gt; n\\). We will solve this issue by introducing the pseudoinverse. The pseudoinverse, specifically the Moore-Penrose pseudoinverse, extends the concept of matrix inversion to singular or non-square matrices, providing a stable solution for least-squares problems when p &gt; n.\n\n\nTesting Linear Regression on Random Features\nTo test the linear regression I implemented in linear.py, I will first generate my data. my data points are sampled from the function \\(f(x) = x^4\\) with normal noise added to it\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\nNow, let’s fit our linear regression model on 150 random features generated from our 2D data. This setup intentionally overparameterizes the model, with the number of features p = 150 exceeding the number of data points n = 100. This allows us to observe the effects of overparameterization and potential overfitting.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1) Create a feature map (try p &gt; n to see overparameterization)\nn_features = 100\nphi = RandomFeatures(n_features=n_features, activation=square)\nphi.fit(X)                               \nX_feat = phi.transform(X)                \n\n# 2) Instantiate your regression model and optimizer\nmodel = MyLinearRegression()\nopt   = OverParameterizedLinearRegressionOptimizer(model)\n\n# 3) Fit closed‑form and predict\nopt.fit(X_feat, y.squeeze())             # y has shape (100, 1) so we squeeze to (100,)\ny_pred = model.predict(X_feat)           # continuous predictions\n\n# 4) Plot data vs. learned curve\nplt.scatter(X.numpy(), y.numpy(), color='darkgrey', label='Noisy data')\nplt.plot(X.numpy(), y_pred.detach().numpy(), color='red', label=f'{n_features} features')\nplt.xlabel('x'); plt.ylabel('y / ŷ')\nplt.legend(); plt.show()\n\n# 5) Print training MSE\nprint(\"Train MSE:\", model.loss(X_feat, y.squeeze()).item())\n\n\n\n\n\n\n\n\nTrain MSE: 56.75595760061362\n\n\nWe notice that the model did quite well at generalizing the pattern seen in the data, even when the number of features exceeds the number of data points."
  },
  {
    "objectID": "posts/Double Descent/main.html#more-complex-pattern-number-of-corruption-artifacts-in-images",
    "href": "posts/Double Descent/main.html#more-complex-pattern-number-of-corruption-artifacts-in-images",
    "title": "Overparameterization and Double Descent",
    "section": "More Complex Pattern: Number of Corruption Artifacts in Images",
    "text": "More Complex Pattern: Number of Corruption Artifacts in Images\nLet’s now have a look at our random features being applied to corrupted images. Then, we will compare model performance across various range of parameter numbers. This will help us further inpect the effect of adding more features to training loss and testing loss.\nLet’s begin by loading an sample image from sklearn datasets\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = 255 - X \nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap = 'gray')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nNow, let’s corrupt the image by adding a number of corruption artifacts to the image. The corruption artifacts are grey squares added to the image, and we will try to predict the number of artifacts added to the image. I am using the function corrupted_image borrowed from professor Chodrow’s notes.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 20)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nNow that we have an example of what a corrupted image looks like, let’s generate a lot of them! the label will be the actual number of squares added to the image\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nNow, let’s generate our train_test sets. a test size of 0.5 is good to observe the difference in test loss and train loss in our experiment\n\nfrom sklearn.model_selection import train_test_split\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nNow let’s train! I am training the model on different number of parameters, starting from 0 to 300, and plotting both the train loss and test loss\n\nn_features = range(1,301)\n\ntrain_losses = []\ntest_losses = []\n\nfor n in n_features:\n    phi = RandomFeatures(n_features=n, activation=square)\n    phi.fit(X_train)                               # fit once on the training inputs\n    X_train_feat = phi.transform(X_train)                # shape (100, 150)\n    X_test_feat = phi.transform(X_test)                # shape (100, 150)\n\n    model = MyLinearRegression()\n    opt   = OverParameterizedLinearRegressionOptimizer(model)\n\n    opt.fit(X_train_feat, y_train.squeeze())             # y has shape (100, 1) so we squeeze to (100,)\n    y_pred = model.predict(X_test_feat)           # continuous predictions\n    train_losses.append(model.loss(X_train_feat, y_train.squeeze()).item())\n    test_losses.append(model.loss(X_test_feat, y_test.squeeze()).item())\n    if n % 20 == 0:\n        print(f\"n_features: {n}, train_loss: {train_losses[-1]}, test_loss: {test_losses[-1]}\")\n\n\nn_features: 20, train_loss: 214.82480112861558, test_loss: 373.14158273374034\nn_features: 40, train_loss: 64.72116250014501, test_loss: 371.55640846843016\nn_features: 60, train_loss: 43.31662865830799, test_loss: 721.765908299188\nn_features: 80, train_loss: 16.224967757212333, test_loss: 438.8784872753655\nn_features: 100, train_loss: 2.3784471836775594e-25, test_loss: 36143.203253335094\nn_features: 120, train_loss: 5.420234880780984e-26, test_loss: 362.8509767192916\nn_features: 140, train_loss: 5.0462832572700156e-26, test_loss: 384.94344826167645\nn_features: 160, train_loss: 1.0385231938224764e-24, test_loss: 199.83423095735074\nn_features: 180, train_loss: 6.964458501743703e-27, test_loss: 204.90400198784667\nn_features: 200, train_loss: 1.0979220632636642e-26, test_loss: 163.11367772090244\nn_features: 220, train_loss: 1.468149030706825e-26, test_loss: 159.60466066122575\nn_features: 240, train_loss: 1.0028072091758801e-24, test_loss: 196.50770974516294\nn_features: 260, train_loss: 2.7628814522072796e-25, test_loss: 127.92306301817099\nn_features: 280, train_loss: 4.713768743127388e-25, test_loss: 114.19500596832786\nn_features: 300, train_loss: 2.8518618591346237e-24, test_loss: 109.06332897550001\n\n\nNow let’s plot our results! I am using logs to better visualize the loss changes over different magnitudes.\n\nfor i, n in enumerate(n_features):\n    plt.scatter(n, np.log(train_losses[i]), color = 'blue', alpha = .5)\n    plt.scatter(n, np.log(test_losses[i]), color = 'red', alpha = .5)\n\nplt.xlabel('n_features')\nplt.ylabel('MSE')\nplt.title('Train and Test Losses')\nplt.legend(['Train Loss', 'Test Loss'])\nplt.show()\n\n\n\n\n\n\n\n\nInteresting results! The plot illustrates the double descent phenomenon, where the test error initially decreases as the model gains more features, then spikes sharply around the interpolation threshold when the number of features matches the number of training samples. After this peak, the test error decreases again in the overparameterized region, where the model has enough capacity to capture complex patterns despite fitting the training data perfectly. This second descent reflects the ability of highly overparameterized models to generalize well even though being classically overfitted.\n\n# n_features with the lowest test loss\nbest_n = n_features[np.argmin(test_losses)]\nprint(f\"Best n_features: {best_n}, test_loss: {min(test_losses)}\")\n\nBest n_features: 291, test_loss: 75.86592287468265\n\n\nWe see as well that the best loss is when n_features is 291, which is way beyond the threshold of n = 100"
  },
  {
    "objectID": "posts/Double Descent/main.html#discussion",
    "href": "posts/Double Descent/main.html#discussion",
    "title": "Overparameterization and Double Descent",
    "section": "Discussion",
    "text": "Discussion\nThrough this exploration, we observed the double descent phenomenon, where test error initially decreases as the model’s capacity increases, then spikes near the interpolation threshold, and eventually decreases again as the model becomes significantly overparameterized. This behavior challenges the classical view that overfitting is always detrimental to generalization. Instead, we see that sufficiently large models can recover from this high-error region and achieve strong generalization, even when the number of features far exceeds the number of training samples. This suggests that overparameterized models, such as deep neural networks, can exploit their massive capacity to fit complex, real-world data without overfitting in the traditional sense."
  },
  {
    "objectID": "posts/Exploring-Perceptrom/index.html",
    "href": "posts/Exploring-Perceptrom/index.html",
    "title": "Exploring and Implementing the Perceptron",
    "section": "",
    "text": "Abstract\nThis blog post explores the implementation and behavior of the perceptron algorithm, a foundational model in machine learning for binary classification. I begin by building the perceptron from scratch using PyTorch, including custom classes for the model, loss computation, and optimization steps. Through a series of visual experiments, I demonstrate that the perceptron converges when the data is linearly separable, and fails to do so otherwise. I also extend the analysis to higher-dimensional data, using PCA to visualize the decision boundary. This post highlights both the capabilities and limitations of the perceptron, setting the stage for more advanced linear models.\nYou can find my implementation of the perceptron at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nData Generation and Visualization\nFirst, let’s generate and visualize 2D synthetic data for testing the perceptron algorithm. The perceptron_data() function creates a binary classification dataset with Gaussian noise and appends a bias term to each data point. The plot_perceptron_data() function displays the two classes using distinct markers and colors.\nFunctions provided by Professor Phil Chodrow as part of CSCI 0451 course materials.\nThis setup provides a visual way to assess whether the perceptron correctly learns to separate the two classes in later experiments.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data(n_points =  250, noise = 0.2)\nplot_perceptron_data(X, y, ax)\n\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n\nTraining the Perceptron\nNow, let’s define and run the train_perceptron() function, which trains a perceptron model on the synthetic dataset using the Perceptron update rule. The function samples one random data point per iteration, checks if it’s misclassified, and updates the model if necessary. The training continues either until convergence (zero loss) or until the specified maximum number of iterations is reached.\nIf visualize=True, the function also displays a sequence of plots showing how the decision boundary evolves as the model learns. Dashed lines represent the previous boundary before an update, and solid lines show the updated one. The logic is also adapted from Professor Chodrow’s work.\n\ndef train_perceptron(X, y, max_iter=1_000_000, visualize=False, max_axes=6, k = 1 , alpha = 0.1):\n    \"\"\"\n    Train a perceptron on the given data.\n\n    Parameters:\n        X (torch.Tensor): Input features of shape (n_samples, 3)\n        y (torch.Tensor): Binary labels (0 or 1) of shape (n_samples,)\n        max_iter (int): Maximum number of iterations to run (-1 means run until convergence)\n        visualize (bool): If True, visualize decision boundary evolution\n        max_axes (int): Number of subplots to show if visualizing\n\n    Returns:\n        p (Perceptron): Trained perceptron model\n        opt (PerceptronOptimizer): Optimizer used for training\n        loss_vec (list): Loss values over training steps\n    \"\"\"\n\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    n = X.size(0)\n    loss_vec = []\n\n    # Trigger initialization of p.w by calling score once\n    _ = p.score(X)\n\n    loss = 1.0\n    iteration = 0\n    current_ax = 0\n\n    if visualize:\n        fig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)\n        plt.rcParams[\"figure.figsize\"] = (7, 5)\n        markers = [\"o\", \",\"]\n        marker_map = {-1: 0, 1: 1}\n\n    while loss &gt; 0 and (max_iter == -1 or iteration &lt; max_iter):\n        old_w = torch.clone(p.w)\n\n        # Sample one data point\n        i = torch.randint(n, size=(1,))\n        x_i = X[[i], :]\n        y_i = y[i]\n        local_loss = p.loss(x_i, y_i).item()\n\n        # If misclassified, update and optionally visualize\n        if local_loss &gt; 0:\n            opt.step(x_i, y_i, k=k, alpha=alpha)\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n\n            if visualize and current_ax &lt; max_axes:\n                ax = axarr.ravel()[current_ax]\n                plot_perceptron_data(X, y, ax)\n                draw_line(old_w, x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n                draw_line(p.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n                ax.scatter(\n                    X[i, 0],\n                    X[i, 1],\n                    color=\"black\",\n                    facecolors=\"none\",\n                    edgecolors=\"black\",\n                    marker=markers[marker_map[2 * y[i].item() - 1]],\n                )\n                ax.set_title(f\"loss = {loss:.3f}\")\n                ax.set(xlim=(-1, 2), ylim=(-1, 2))\n                current_ax += 1\n\n        iteration += 1\n\n    if visualize:\n        plt.tight_layout()\n        plt.show()\n\n    return p, opt, loss_vec\n\np, opt, loss_vec = train_perceptron(X, y, visualize=True, max_iter= 1000000)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n\nResults: Convergence of the Perceptron Algorithm\nThe plots above show the progression of the perceptron’s decision boundary during training on linearly separable 2D data. Each subplot corresponds to a misclassified point that triggered an update. The dashed line represents the previous decision boundary, while the solid line shows the updated one after a single perceptron step.\nAs training proceeds, the loss steadily decreases until it reaches zero, indicating perfect classification on the dataset. This confirms that the perceptron algorithm converges when the data is linearly separable, as expected from the theory.\nlet’s make sure that we indeed do get 0 loss at the end\n\nloss_vec[-1]\n\n0.0\n\n\nLet us now visualize the loss vector to see how it updates over time!\n\n# Plotting the evolution of loss during training\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec, linewidth=2)\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Perceptron Training Loss Over Time\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe loss indeed converges to zero with some spikes along the way!\n\n\nIntroducing a Misclassified Point\nnow, I will modify the dataset to make it linearly inseparable by manually injecting a single mislabeled data point. This new point at (0, 0.5) is assigned to class 1, but is positioned among points from class 0. As a result, no straight line can perfectly separate the two classes.\nThis visualization sets up the next experiment to test how the perceptron behaves when perfect classification is impossible.\n\n#injecting one data point to make the data linearly inseparable\n\nX_ , y_ = X, y\nX_ = torch.cat((X_, torch.tensor([[0, 0.4, 1.0]])), dim=0)\ny_ = torch.cat((y_, torch.tensor([True])), dim=0)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X_, y_, ax)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\nLet’s retrain on the linearly inseparable data and get it to run for 10000 iterations\n\np, opt, loss_vec = train_perceptron(X_, y_, visualize=True, max_iter= 10000)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\nloss_vec[-1]\n\n0.0039840638637542725\n\n\nThe final loss after training is approximately 0.0066, indicating that while the perceptron was able to correctly classify most data points, it could not achieve perfect accuracy due to the presence of the injected outlier. This aligns with theoretical expectations: the perceptron algorithm cannot converge to zero loss on linearly inseparable data.\n\n\nPerceptron in Higher Dimensions\nTo explore how the perceptron performs in higher dimensions, I generated a 5-dimensional dataset using the same perceptron_data() function. The model was then trained for a maximum of 10,000 iterations. The resulting loss plot shows that the perceptron consistently reduced the misclassification rate over time, suggesting that the data is likely linearly separable in this higher-dimensional space.\nSince visualizing 5D data directly is not possible, I used Principal Component Analysis (PCA) to project the data into 2D for visualization. Additionally, I approximated the perceptron’s decision boundary (a 4D hyperplane) by sampling points that satisfy the hyperplane equation and projecting them into the same 2D space using PCA.\nThe resulting plot shows a clear separation between the two classes in the projected space, with the projected hyperplane aligning well between them. This demonstrates that the perceptron is capable of finding meaningful separating boundaries even in higher-dimensional feature spaces.\n\nfrom sklearn.decomposition import PCA\n\n# Step 1: Generate data\nX_hd, y_hd = perceptron_data(n_points=300, noise=0.3, p_dims=5)\n\n# Step 2: Train perceptron\np_hd, opt_hd, loss_vec_hd = train_perceptron(X_hd, y_hd, max_iter=10000, visualize=False)\n\n# Step 3: Plot loss over time\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec_hd)\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Misclassification Rate\")\nplt.title(\"Loss Over Time (5D Perceptron Training)\")\nplt.grid(True)\nplt.show()\n\n# Step 4: PCA fit (exclude bias column)\nX_hd_np = X_hd[:, :-1].numpy()\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_hd_np)\n\n# Step 5: Project a sample of points on the decision hyperplane\n# We pick random 5D points x such that w·x + b ≈ 0\nw = p_hd.w[:-1]\nb = p_hd.w[-1]\n\n# Sample points along the hyperplane by solving w·x + b = 0\n# Fix all but one dimension and solve for the last\nx_vals = []\nfor i in range(-5, 6):  # varying x0 from -5 to 5\n    x_sample = torch.zeros(5)\n    x_sample[0] = i\n    # solve for x1 such that the hyperplane condition is met\n    # w0*x0 + w1*x1 + ... + w4*x4 + b = 0\n    # → x1 = -(b + w0*x0 + w2*x2 + ...)/w1\n    # we'll keep x2, x3, x4 = 0 for simplicity\n    if w[1] != 0:\n        x_sample[1] = -(b + w[0]*x_sample[0]) / w[1]\n        x_vals.append(x_sample.numpy())\n\n# Convert to numpy and apply PCA\nx_vals = np.stack(x_vals)\nx_vals_pca = pca.transform(x_vals)\n\n# Step 6: Plot PCA projection of data and separating line\nplt.figure(figsize=(6, 5))\nplt.scatter(X_pca[y_hd==0][:, 0], X_pca[y_hd==0][:, 1], label=\"Class 0\", alpha=0.5)\nplt.scatter(X_pca[y_hd==1][:, 0], X_pca[y_hd==1][:, 1], label=\"Class 1\", alpha=0.5)\nplt.plot(x_vals_pca[:, 0], x_vals_pca[:, 1], color='black', label=\"Projected Hyperplane\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.title(\"PCA Projection with Projected Hyperplane\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Perceptron on mini-batches\nIn my implementation of the perceptron, I have made recent modifications that allows me to calculate the gradient on batches of points and then averaging. In this section, I will experiment using multiple alpha values and k (the batch size)\nLet’s repeat the experiment. We will start with k = 1. this will theoretically get us something similar to the regular perceptron\n\n# when k = 1, the perceptron is equivalent to the standard perceptron. here is an example\nX, y = perceptron_data(n_points = 400, noise = 0.2)\nplot_perceptron_data(X, y, ax)\np, opt, loss_vec = train_perceptron(X, y, visualize=True, max_iter= 1000000, k = 1, alpha = 0.5)\nloss_vec[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0\n\n\nThis is the same results we got in the beginning. Now, let’s introduce higher number of k. starting with k = 10\n\n# now, let's try k = 10\np10, opt10, loss_vec10 = train_perceptron(X, y, visualize=True, max_iter= 1000000, k = 10, alpha = 0.5)\nloss_vec10[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0\n\n\n\n# now let's make the data linearly inseparable\nX_, y_ = X, y\nX_ = torch.cat((X_, torch.tensor([[0, 0.4, 1.0]])), dim=0)\ny_ = torch.cat((y_, torch.tensor([True])), dim=0)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X_, y_, ax)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n# now let's try with k = size of the data to see if it can converge\np_max, opt_max, loss_vec_max = train_perceptron(X_, y_, visualize=True, max_iter= 100, k = X_.shape[0], alpha = 0.5)\nloss_vec_max[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0024937656708061695\n\n\n\n# let's now plot all the loss curves together\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec, label=\"k=1\")\nplt.plot(loss_vec10, label=\"k=10\")\nplt.plot(loss_vec_max, label=\"k=400\")\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time for Different k Values\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe observe that the loss indeed converges when we average the gradient over all points in our dataset. We also notice that, when the learning rate is tuned, we can achieve perfect separability faster using the mini-batch approach\n\n\nConclusion\nIn this notebook, I implemented the perceptron algorithm from scratch and explored its behavior through a series of experiments. I confirmed that the perceptron converges on linearly separable data and fails to fully converge when the data is not perfectly separable. Visualizations of the decision boundary and loss trajectory provided insight into how the model evolves with each update. I also extended the analysis to higher-dimensional data and used PCA to visualize the model’s decision hyperplane, showing that the perceptron remains effective in separating classes beyond two dimensions. Finally, I experimented with the mini-batch algorithm of averaging the grad over a subset of points before making a step, which has the benefit of faster convergence, and convergence when data is not linearly separable. Overall, these experiments reinforce both the strengths and limitations of the perceptron algorithm, providing a solid foundation for understanding more advanced linear classifiers."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html",
    "href": "posts/understanding-feature-separability/index.html",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#abstract",
    "href": "posts/understanding-feature-separability/index.html#abstract",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#discussion",
    "href": "posts/understanding-feature-separability/index.html#discussion",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "Discussion",
    "text": "Discussion\nThrough this analysis, I gained insights into the importance of feature selection in classification tasks. My IoU-based approach helped quantify feature separability, leading to better-informed choices. Visualization techniques, such as pair plots and decision boundaries, provided valuable interpretability into the model’s behavior. The performance comparison between Logistic Regression, Decision Trees, and Random Forest demonstrated that Decision Trees models generalize best, minimizing overfitting while maintaining high accuracy. One key takeaway is that some species are inherently harder to classify due to overlapping feature distributions, reinforcing the need for careful feature engineering and model selection. Future improvements could involve non-linear models like SVMs and better feature selection to further enhance classification accuracy. This project shows how data-driven feature selection and model evaluation can lead to meaningful and accurate species classification."
  },
  {
    "objectID": "posts/Auditing-bias/index.html",
    "href": "posts/Auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, BasicProblem\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression"
  },
  {
    "objectID": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "href": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "title": "Auditing Bias",
    "section": "patterns of disparity",
    "text": "patterns of disparity\nTo explore deeper patterns of disparity, we now perform an intersectional analysis by examining the proportion of individuals earning over $50K across combined gender and race subgroups. We add the RAC1P column (race) from the original ACS dataset to our DataFrame and map it to readable race labels using race_map.\nWe then create a new column that combines gender and race into a single intersectional group (e.g., “M & White”, “F & Black”). By grouping on this combined variable and computing the mean of the label, we obtain the proportion of high-income individuals within each intersectional group.\nFinally, we visualize these proportions with a bar chart to better observe the disparities across different gender-race combinations. This helps highlight how overlapping identities can influence economic outcomes and inform fairness analysis in the model.\n\ndf_intersection = df.copy()\ndf_intersection[\"RAC1P\"] = acs_data.loc[df.index, \"RAC1P\"]\n\n# Map RAC1P to human-readable race labels.\ndf_intersection[\"Race\"] = df_intersection[\"RAC1P\"].map(race_map)\n\n# Create an intersectional grouping variable using both Gender and Race.\ndf_intersection[\"Gender_Race\"] = df_intersection[\"Gender\"] + \" & \" + df_intersection[\"Race\"]\n\n# Compute the proportion of positive labels for each intersectional group.\nintersectional_stats = df_intersection.groupby(\"Gender_Race\")[\"label\"].mean()\nprint(\"\\nProportion of positive labels by Gender & Race group:\")\nprint(intersectional_stats)\n\n# Visualize the intersectional trends with a bar chart.\nplt.figure(figsize=(12, 6))\nsns.barplot(x=intersectional_stats.index, y=intersectional_stats.values)\nplt.xlabel(\"Gender & Race Group\")\nplt.ylabel(\"Proportion with label == 1\")\nplt.title(\"Intersectional Proportions by Gender and Race\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nProportion of positive labels by Gender & Race group:\nGender_Race\nF & Black                               0.143632\nF & Native                              0.137795\nF & Native Hawaiian/Pacific Islander    0.150943\nF & Other                               0.125604\nF & Two or More                         0.000000\nF & White                               0.136007\nM & Black                               0.256418\nM & Native                              0.240816\nM & Native Hawaiian/Pacific Islander    0.204545\nM & Other                               0.287402\nM & Two or More                         0.250000\nM & White                               0.263107\nName: label, dtype: float64\n\n\n\n\n\n\n\n\n\nThe results of the intersectional analysis show consistent disparities in high-income proportions across gender and race combinations. For females, the proportion of individuals earning over $50K ranges from 12.56% to 15.09% across most racial groups, with White females at 13.60% and Black females at 14.36%. Notably, females identifying as Two or More races have a value of 0%, likely due to a very small sample size.\nFor males, the rates are significantly higher across the board. For example, White males are at 26.31%, Black males at 25.64%, and Other males at 28.74%. These findings reinforce the earlier observed gender gap and also highlight how racial identity further compounds disparities. This intersectional breakdown is crucial for understanding how multiple identity factors can interact to affect economic outcomes.\nThis is why choosing gender might be a good thing to explore bias through in this blog post.\nHere we train and tune a RandomForestClassifier by performing a hyperparameter search over the max_depth parameter. We loop through a range of depths in steps of 2 and use 5-fold cross-validation to evaluate model performance at each depth.\nFor each candidate max_depth, we compute the average cross-validation accuracy and track it in a dictionary. To avoid unnecessary computation, we stop the search early if the performance begins to drop.\nAt the end, we identify the best-performing max_depth based on the highest mean cross-validation accuracy. This tuned depth will be used to train our final model.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# Define the candidate max_depth values.\nstep, max_val = 2, 20\n\nmax_depth_values = [int(i * step) for i in range(1, max_val + 1)]\nresults = {}\n\nfor depth in max_depth_values:\n    model = RandomForestClassifier(max_depth=depth, random_state=42)\n    \n    # Perform 5-fold cross-validation on the training data.\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    mean_score = np.mean(cv_scores)\n    results[depth] = mean_score\n    \n    print(f\"Max Depth: {depth} - Mean CV Accuracy: {mean_score:.3f}\")\n    scores = list(results.values())\n    \n    #we're finding the minimum number of max_depth values to test\n    if len(scores) &gt; 2 and scores[-1] &lt; scores[-2]:\n        break\n\n\nbest_depth = max(results, key=results.get)\nprint(\"\\nBest max_depth:\", best_depth, \"with a Mean CV Accuracy of:\", results[best_depth])\n\nMax Depth: 2 - Mean CV Accuracy: 0.801\nMax Depth: 4 - Mean CV Accuracy: 0.842\nMax Depth: 6 - Mean CV Accuracy: 0.843\nMax Depth: 8 - Mean CV Accuracy: 0.843\nMax Depth: 10 - Mean CV Accuracy: 0.844\nMax Depth: 12 - Mean CV Accuracy: 0.845\nMax Depth: 14 - Mean CV Accuracy: 0.844\n\nBest max_depth: 12 with a Mean CV Accuracy of: 0.8447350223172189\n\n\nUsing the optimal max_depth found from cross-validation, we now train a final RandomForestClassifier on the full training data. After fitting the model, we evaluate its performance on the test set.\nWe calculate several key performance metrics: - Accuracy: the proportion of correct predictions. - Positive Predictive Value (PPV): the precision, or the proportion of predicted positives that are actually positive. - False Negative Rate (FNR): the proportion of actual positives that were misclassified as negatives. - False Positive Rate (FPR): the proportion of actual negatives that were misclassified as positives.\nThese metrics provide a comprehensive view of the model’s overall performance.\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n# start and fit a new model using the best max_depth from tuning\nbest_model = RandomForestClassifier(max_depth=best_depth, random_state=42)\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_hat = best_model.predict(X_test)\n\n# Calculate overall accuracy\naccuracy = accuracy_score(y_test, y_hat)\n\n# Compute confusion matrix: returns [[TN, FP], [FN, TP]]\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n\n# Calculate positive predictive value (PPV) i.e. precision\nppv = precision_score(y_test, y_hat)\n\n# Calculate false negative rate (FNR): FN / (FN + TP) while avoiding dividing by zero\nfnr = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n\n# Calculate false positive rate (FPR): FP / (FP + TN)\nfpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\n\nprint(\"Overall Accuracy:\", accuracy)\nprint(\"Positive Predictive Value (PPV):\", ppv)\nprint(\"False Negative Rate (FNR):\", fnr)\nprint(\"False Positive Rate (FPR):\", fpr)\n\nOverall Accuracy: 0.8486220076443372\nPositive Predictive Value (PPV): 0.6697517879680269\nFalse Negative Rate (FNR): 0.5829185223997904\nFalse Positive Rate (FPR): 0.04885790751229228\n\n\nThe model achieves an overall accuracy of 84.86%, indicating strong performance in correctly classifying income levels. The positive predictive value (PPV) is 66.98%, meaning that when the model predicts someone earns over $50K, it’s correct about two-thirds of the time.\nHowever, the false negative rate (FNR) is relatively high at 58.29%, suggesting that the model frequently fails to identify individuals who do earn more than $50K. The false positive rate (FPR) is much lower at 4.89%, meaning the model rarely misclassifies low-income individuals as high earners.\nThis trade-off indicates that while the model is cautious about predicting high income (low FPR), it may be overly conservative, leading to many missed positives (high FNR)."
  },
  {
    "objectID": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "href": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "title": "Auditing Bias",
    "section": "Model Fairness across gender groups",
    "text": "Model Fairness across gender groups\nTo evaluate the model’s fairness across gender groups, we compute key performance metrics separately for males and females. For each subgroup, we calculate:\n\nAccuracy: overall correctness within the group.\nPPV (Precision): how often predicted high-income individuals are actually high-income.\nFNR (False Negative Rate): how often actual high-income individuals are missed.\nFPR (False Positive Rate): how often low-income individuals are incorrectly classified as high-income.\n\nThese metrics are stored in a dictionary and then converted into a DataFrame for easy viewing. We also extract the FNR, FPR, and PPV for each group (male = 1, female = 2) for use in later fairness visualizations and audits.\n\n\n# Dictionary to hold metrics keyed by group\nmetrics_dict = {}\n\n# Get the unique groups from your test set\nunique_groups = np.unique(group_test)\n\n# Loop over each subgroup in group_test\nfor grp in unique_groups:\n    # Create a mask for the current group\n    mask = (group_test == grp)\n    \n    # Subset the true labels and predictions for this group\n    y_true_grp = y_test[mask]\n    y_pred_grp = y_hat[mask]\n    \n    # Calculate accuracy for the subgroup\n    grp_accuracy = accuracy_score(y_true_grp, y_pred_grp)\n    \n    # Calculate the confusion matrix: [[TN, FP], [FN, TP]]\n    tn, fp, fn, tp = confusion_matrix(y_true_grp, y_pred_grp).ravel()\n    \n    # Calculate PPV (precision) for the subgroup\n    grp_ppv = precision_score(y_true_grp, y_pred_grp)\n    \n    # Calculate False Negative Rate (FNR): FN / (FN + TP)\n    grp_fnr = fn / (fn + tp)\n    \n    # Calculate False Positive Rate (FPR): FP / (FP + tn)\n    grp_fpr = fp / (fp + tn)\n    \n    # Store the results in a dictionary keyed by group label (e.g., 1 or 2)\n    metrics_dict[grp] = {\n        \"Accuracy\": grp_accuracy,\n        \"PPV\": grp_ppv,\n        \"FNR\": grp_fnr,\n        \"FPR\": grp_fpr\n    }\n\n# Convert the dictionary to a DataFrame for display\ndf_group_metrics = pd.DataFrame.from_dict(metrics_dict, orient=\"index\")\ndf_group_metrics.index.name = \"Group\"\ndf_group_metrics.reset_index(inplace=True)\n\nprint(df_group_metrics)\n\n# Optionally, you can directly pull out metrics for each group:\nfnr_m = metrics_dict[1][\"FNR\"] if 1 in metrics_dict else None\nfpr_m = metrics_dict[1][\"FPR\"] if 1 in metrics_dict else None\nppv_m = metrics_dict[1][\"PPV\"] if 1 in metrics_dict else None\n\nfnr_f = metrics_dict[2][\"FNR\"] if 2 in metrics_dict else None\nfpr_f = metrics_dict[2][\"FPR\"] if 2 in metrics_dict else None\nppv_f = metrics_dict[2][\"PPV\"] if 2 in metrics_dict else None\n\nprint(\"\\nMale FNR:\", fnr_m, \" FPR:\", fpr_m, \" PPV:\", ppv_m)\nprint(\"Female FNR:\", fnr_f, \" FPR:\", fpr_f, \" PPV:\", ppv_f)\n\n   Group  Accuracy       PPV       FNR       FPR\n0      1  0.817064  0.774603  0.608504  0.038629\n1      2  0.879570  0.551477  0.534743  0.057487\n\nMale FNR: 0.6085038106698757  FPR: 0.03862894450489663  PPV: 0.7746031746031746\nFemale FNR: 0.5347432024169184  FPR: 0.05748709122203098  PPV: 0.5514771709937332\n\n\nThe subgroup performance metrics reveal meaningful disparities between males and females:\n\nAccuracy is higher for females (87.96%) than for males (81.71%).\nPPV (Precision) is significantly higher for males (77.46%) than for females (55.15%), indicating that when the model predicts high income, it is more correct for males.\nFNR (False Negative Rate) is worse for males (60.85%) than for females (53.47%), meaning the model misses more high-income males.\nFPR (False Positive Rate) is slightly better for males (3.86%) compared to females (5.75%), suggesting the model more often incorrectly labels females as high-income.\n\nThese results suggest a gender-based imbalance in prediction quality especially in precision which may reflect or amplify real-world income disparities and requires careful consideration when interpreting model fairness.\nHere we calculate the prevalence of high income (i.e. the proportion of individuals earning over $50K) separately for males and females in the test set. This is done by taking the mean of the binary target (y_test) within each gender group. These values will be used in the fairness analysis to determine feasible combinations of false positive and false negative rates under fixed PPV, as described in Chouldechova (2017).\n\np_m = (y_test[group_test == 1]).mean()  # Prevalence for Males\np_f = (y_test[group_test == 2]).mean()  # Prevalence for Females\np_m, p_f\n\n(0.2532249873031996, 0.13188564598067537)\n\n\nwe fix the positive predictive value (PPV) across groups by setting it to the lower of the two observed PPVs. This ensures a consistent standard of predictive parity when plotting feasible combinations of false negative and false positive rates. The common_ppv will be used to generate the fairness trade-off lines for each group.\n\ncommon_ppv = min(ppv_m, ppv_f)\nprint(\"Using common PPV =\", common_ppv)\n\nUsing common PPV = 0.5514771709937332\n\n\nTo visualize the fairness trade-offs described in Chouldechova (2017), we plot feasible combinations of false negative rate (FNR) and false positive rate (FPR) for each gender group under a fixed PPV (set to the lower of the two observed PPVs).\nWe define a function based on Equation (2.6) from the paper to compute FPR as a function of FNR, prevalence, and PPV. Using this, we generate lines for males and females by sweeping FNR values from 0 to 1.\nWe then plot: - The feasible FNR–FPR line for each group. - The observed FNR and FPR as points on the plot.\nThis visualization illustrates the trade-off between FNR and FPR under predictive parity constraints. For example, to equalize FPR between groups, one might need to significantly increase the FNR in one group, which highlights the inherent tension between different fairness criteria.\n\ndef feasible_fpr(fnr_array, p, ppv):\n    \"\"\"\n    Given an array of FNR values in [0, 1],\n    returns the corresponding FPR values from Chouldechova (2017), eq. (2.6).\n    FPR = [p * (1 - FNR) * (1 - PPV)] / [1 - p]\n    \"\"\"\n    return (p * (1 - fnr_array) * (1 - ppv)) / (1 - p)\n\n# We'll sweep FNR from 0 to 1 for plotting\nfnr_grid = np.linspace(0, 1, 200)\n\n# Compute the feasible lines for each group, \n# using the *common* PPV for both\nfpr_line_m = feasible_fpr(fnr_grid, p_m, common_ppv)\nfpr_line_f = feasible_fpr(fnr_grid, p_f, common_ppv)\n\n# Plot the feasible lines\nplt.figure(figsize=(7,5))\n\nplt.plot(fnr_grid, fpr_line_m, label=\"Feasible line (Males)\", color=\"blue\")\nplt.plot(fnr_grid, fpr_line_f, label=\"Feasible line (Females)\", color=\"orange\")\n\n# Plot the observed points for each group\nplt.scatter(fnr_m, fpr_m, color=\"blue\", marker=\"o\", s=80, \n            label=\"Observed (Males)\")\nplt.scatter(fnr_f, fpr_f, color=\"orange\", marker=\"o\", s=80, \n            label=\"Observed (Females)\")\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) Combinations under Fixed PPV = {:.3f}\".format(common_ppv))\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe observed FPR for females is higher than for males, which suggests that the model is more likely to incorrectly classify low-income females as high-income.\nThe FNR for males is higher than for females, meaning the model is missing more actual high-income males.\nIf we wanted to equalize FPR between groups, we would have to increase the FNR for males, moving it further along the blue line. This trade-off shows the conflict or tension between different fairness goals.\n\nThis visualization illustrates that achieving equalized error rates across groups requires making trade-offs that may disproportionately impact different subgroups.\n\n\nConcluding Discussion\nThe ability to predict income levels has several potential applications in both commercial and governmental settings. Companies in finance, such as banks and credit card providers, could use this model to assess credit-worthiness, loan eligibility, or target specific financial products. Marketing agencies might use similar predictions to segment consumers for advertising high-end products or services. Government agencies could employ such models for economic policy analysis, workforce development, or social welfare program distribution.\nHowever, deploying this model at a large scale carries significant risks, particularly concerning fairness and bias. Our bias audit revealed disparities in predictive performance across gender groups. Notably, the model has a higher false positive rate (FPR) for females, meaning it more often misclassifies lower-income women as high-income. Conversely, it has a higher false negative rate (FNR) for males, meaning it more frequently fails to recognize high-income males. If deployed in real-world scenarios such as hiring or loan approvals, this could systematically disadvantage certain groups, reinforcing existing economic inequalities.\nExamining different types of bias, our model does not satisfy error rate balance, as FNR and FPR differ between genders. Additionally, the calibration of the model is problematic—males have a higher precision (PPV) than females, meaning the model is more confident in its positive predictions for men than for women. This suggests potential bias in how income is modeled, reflecting either societal disparities or weaknesses in the dataset itself.\nBeyond bias, there are other concerns with deploying such a model. One key issue is data representativeness—the ACS dataset might not fully capture income distributions across different racial or socioeconomic groups. Additionally, there’s a risk of automation bias, where decision-makers might overly rely on model predictions without questioning their validity. Finally, privacy concerns arise when using sensitive demographic data for predictions, as such models could be exploited for discriminatory profiling.\nTo mitigate these issues, several steps could be taken: - Fairness constraints: Adjusting the decision threshold per group to balance FPR and FNR. - Re-weighting techniques: Ensuring training data better reflects underrepresented groups. - Explainability measures: Making the model’s predictions more interpretable to reduce blind reliance. - Human oversight: Keeping final decision-making in human hands rather than automating high-stakes outcomes.\nfinally, while predictive models can be powerful tools, deploying them responsibly requires continuous auditing, transparency, and fairness interventions to prevent unintended harm."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html",
    "href": "posts/when-numbers-lie/index.html",
    "title": "When Numbers Lie",
    "section": "",
    "text": "I came to machine learning because I believed in its elegance—the way a few lines of code could make predictions, optimize decisions, and maybe even eliminate human bias. Studying philosophy alongside computer science, I have always wondered if utilizing the machine, supported with some rigorous math, can eliminate some tendencies in some humans to perpetuate injustice and unfairness. But somewhere along the way, this class made me ask: Whose data is this model trained on? What decisions does it reinforce? And who gets left behind when we define “fairness” with math?\nThese questions became even more urgent as I read Arvind Narayanan’s 2022 James Baldwin Lecture, The Limits of the Quantitative Approach to Discrimination (Narayanan 2022). Narayanan argues that current quantitative methods often serve to justify the status quo more than they challenge it. That’s a bold statement—especially for a field that prides itself on objectivity and precision. But as I sat with this claim, I realized: it resonates. At a time when the U.S. government is investing over $500 billion into machine learning (Reuters Staff 2025)—much of it flowing into public systems that affect millions of lives—it’s critical to examine what ethical foundation, if any, these models are built on. Especially during a time where the government is displaying tendencies to practice radical, and often problematic, interpretations of justice, using means that I consider unethical and are against what states can do. Is machine learning another tool for them to justify their actions?\nThis isn’t a theoretical concern. The U.S. government has reportedly begun employing AI-powered agents to monitor the social media of international students, scanning for signs of “support” for terrorist organizations, but also any signs of criticizing the complicity of the government in war crimes happening right now in the Middle East (Fried 2025; Weiss and Parker 2025). This process—untransparent, unauditable, and unchallengeable—has led to the flagging and deportation of students based on ambiguous indicators and culturally uncontextualized speech. What’s happening here is exactly what Narayanan warns about: using mathematical tools to assess fairness based on the same logics that have historically led to unfairness. These models are embedded with political and cultural assumptions, yet shielded from scrutiny by a veil of statistical legitimacy.\n\n\nNarayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves.\n\n\n\nTo be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist.\n\n\n\nA more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art.\n\n\n\nTo me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me.\n\n\n\nSo, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "href": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "title": "When Numbers Lie",
    "section": "",
    "text": "Narayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "href": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "href": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "title": "When Numbers Lie",
    "section": "",
    "text": "A more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#redefining-fairness",
    "href": "posts/when-numbers-lie/index.html#redefining-fairness",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "href": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "title": "When Numbers Lie",
    "section": "",
    "text": "So, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/Implementing-logistic-regression/SGD.html",
    "href": "posts/Implementing-logistic-regression/SGD.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nIn this project, I explore different optimization strategies for logistic regression, including vanilla gradient descent, momentum-based gradient descent, and an evolutionary algorithm. By applying these methods to both synthetic and real-world datasets, I analyze their convergence behavior, generalization performance, and decision boundary characteristics. Visualizations such as PCA projections and loss curves provide a deeper understanding of each method’s dynamics. The study highlights the strengths and trade-offs of traditional and evolutionary approaches, offering insight into how different optimizers shape model learning in high-dimensional settings. You can find my implementation of Logistic Regression at this GitHub link.\n\n\n\nSetup\nLet’s load essential components for the project. Let’s start to import my implementations of LogisticRegression, GradientDescentOptimizer, and EvolutionOptimizer (which will be explored later), along with core libraries like torch, numpy, and time.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, EvolutionOptimizer\nimport torch\nimport numpy as np\nimport time\nimport random\n\n\n\nGenerating Synthetic Data\nI use a data generation function borrowed from Professor Chodrow’s notes to create a simple binary classification dataset. It produces two noisy Gaussian clusters with a bias term appended to the features. Here, I generate 300 points with a noise level of 0.5.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nTraining with Gradient Descent and Momentum\nI initialized a LogisticRegression model and trained it using gradient descent with momentum (β = 0.9) for 100 iterations. After each step, I recorded the loss to track convergence. This setup allowed me to observe how the model’s performance evolved over time.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nlosses = []\nfor _ in range(100):\n    opt.step(X, y, alpha=0.1, beta=0.9)\n    losses.append(LR.loss(X, y).item())\n\n\nLet’s plot them and see what we have!\n\nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over time with Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nVisualizing Loss Convergence\nI plotted the loss over 100 iterations to visualize how the model trained with momentum converges. As expected, the loss decreases steadily and smoothly, showing that gradient descent with momentum effectively accelerates convergence. It would be interesting to compare the rate of convergence to a vanilla optimizer (no momentum)\nI will also borrow a couple of functions from professor Chodrow’s notes that will help me visualize the logistic regression model and the decision boundary\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\nVanilla Gradient Descent and Decision Boundary\nTo test the baseline performance of logistic regression, I trained the model using vanilla gradient descent (β = 0) on a low-noise dataset so that I can make sure they’re linearly separable (even tho the model will converge any way). I ran the optimization for 1000 iterations and plotted the loss curve, which decreases steadily. I also visualized the final decision boundary, which cleanly separates the two classes, indicating that the model has successfully learned a good linear separator.\n\nX, y = classification_data(p_dims=2, noise=0.2)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nlosses = []\nfor _ in range(1000):\n    opt.step(X, y, alpha=0.1, beta=0.0)\n    losses.append(LR.loss(X, y).item())\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time (Vanilla GD, β = 0)\")\nplt.grid(True)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\ndraw_line(LR.w, X[:,0].min(), X[:,0].max(), ax, color=\"black\", label=\"Decision Boundary\")\nax.legend()\nax.set_title(\"Decision Boundary (Vanilla GD, β = 0)\")\nplt.show()\n\n\n\n\n\n\n\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_28797/1220661661.py:13: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\nTraining with vanilla gradient descent (β = 0) over 1000 iterations produced a steadily decreasing loss, as shown in the first plot. The loss curve reflects stable convergence toward a minimum without oscillations or instability. This is expected as our loss function is convex and GD should be able to get us to the minimum without fluctuations.\nIn the second plot, I visualized the decision boundary learned by the model. It cleanly separates the two classes, indicating that even without momentum, gradient descent is capable of finding a good linear separator given enough iterations and low noise.\n\n\nComparing Vanilla Gradient Descent vs Momentum\nTo explore the benefits of momentum, I trained two logistic regression models on the same dataset: one with vanilla gradient descent (β = 0) and the other with momentum (β = 0.9). As shown in the plot, the model trained with momentum converged faster — reaching a lower loss in fewer iterations. This illustrates how momentum can help the optimizer move more effectively through flatter regions of the loss surface.\n\nX, y = classification_data(p_dims=2, noise=0.2)\n\n# Vanilla GD (β = 0)\nLR_vanilla = LogisticRegression()\nopt_vanilla = GradientDescentOptimizer(LR_vanilla)\n\nlosses_vanilla = []\nstart_time = time.time()\nfor _ in range(100):\n    opt_vanilla.step(X, y, alpha=0.1, beta=0.0)\n    losses_vanilla.append(LR_vanilla.loss(X, y).item())\nprint(\"Vanilla GD Time:\", time.time() - start_time)\n# Momentum GD (β = 0.9)\nLR_momentum = LogisticRegression()\nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\nlosses_momentum = []\nstart_time = time.time()\nfor _ in range(100):\n    opt_momentum.step(X, y, alpha=0.1, beta=0.9)\n    losses_momentum.append(LR_momentum.loss(X, y).item())\nprint(\"Momentum GD Time:\", time.time() - start_time)\n# Plot both losses\nplt.figure(figsize=(10, 5))\nplt.plot(losses_vanilla, label=\"Vanilla GD (β = 0)\")\nplt.plot(losses_momentum, label=\"Momentum GD (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time: Vanilla vs Momentum\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nVanilla GD Time: 0.005932807922363281\nMomentum GD Time: 0.005321979522705078\n\n\n\n\n\n\n\n\n\n\n\nMomentum Accelerates Convergence\nThis plot compares the loss over time for two models: one trained using vanilla gradient descent (β = 0) and the other using momentum (β = 0.9). Both models were trained for 100 iterations on the same dataset. The momentum-based optimizer clearly converges more quickly, achieving a lower loss in fewer steps. This demonstrates the effectiveness of momentum in accelerating optimization by dampening oscillations and pushing consistently in productive directions. Also, not only that Momentum made the model converge faster, it was also faster!\nSince we will be using accuracy as a metric, or potentially as an early stopping criteria, let’s define a simple function that calculates it based on the model, data, and labels\n\ndef accuracy(model, X, y):\n    y_pred = model.predict(X)\n    return (y_pred == y).float().mean().item()\n\n\n\nOverfitting in High Dimensions\nTo explore model behavior in a high-dimensional setting, I generated a training and testing dataset with 100 features each. I trained a logistic regression model using vanilla gradient descent (β = 0) until it achieved 100% accuracy on the training set. The loss at convergence confirms perfect separation on the training data — a clear sign of overfitting, especially given the relatively small sample size compared to the feature count.\n\nX_train, y_train = classification_data(p_dims=100,n_points=50, noise=0.2)\nX_test, y_test = classification_data(p_dims=100,n_points=50, noise=0.2)\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\ntrain_accuracy = 0\nwhile train_accuracy != 1:\n    opt.step(X_train, y_train, alpha=0.01, beta=0)\n    train_accuracy = accuracy(LR, X_train, y_train)\n\nprint(\"Training Loss: \", LR.loss(X_train, y_train).item())\n\nTraining Loss:  0.1373424082994461\n\n\n\ntrain_accuracy\n\n1.0\n\n\nThe model successfully achieved 100% training accuracy, with a final loss of approximately 0.15. This confirms that the model has completely fit the training data. While impressive on the surface, this is a classic sign of overfitting — especially in a setting with far more features (100) than samples (50).\n\n\nGeneralization Check\nAfter achieving perfect accuracy on the training set, I evaluated the model on a separate test set drawn from the same distribution. The model achieved a test accuracy of 87.99%, suggesting that while there is some risk of overfitting due to the high dimensionality, the model still generalizes well in this case — likely due to the low noise and relatively separable data.\n\ntest_accuracy = accuracy(LR, X_test, y_test)\ntest_accuracy\n\n0.8799999952316284\n\n\n\n\nReal-World Dataset: Banknote Authentication\nTo evaluate my model on real-world data, I used the Banknote Authentication dataset from the UCI Machine Learning Repository. This dataset contains four numerical features extracted from images of banknotes, along with a binary class label indicating whether a note is genuine or forged.\nI standardized the features using StandardScaler and added a bias term manually. The dataset was then split into training (60%), validation (20%), and test (20%) sets to allow for model tuning and evaluation.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", header=None)\ndf.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n\nX = df.drop(\"class\", axis=1).values\ny = df[\"class\"].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\nX = torch.cat([X, torch.ones(X.size(0), 1)], dim=1)\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nTraining Function with Optional Momentum\nTo streamline experimentation, I created a train_model function that trains a logistic regression model using gradient descent, with an optional momentum parameter. The function tracks both training and validation loss over time, allowing for easy comparison of optimization strategies. It accepts learning rate, momentum factor (β), and number of epochs as configurable parameters.\n\ndef train_model(X_train, y_train, X_val, y_val, use_momentum=False, lr=0.01, beta=0.9, epochs=300):\n    model = LogisticRegression()\n    opt = GradientDescentOptimizer(model)\n\n    train_losses, val_losses = [], []\n\n    for i in range(epochs):\n        opt.step(X_train, y_train, alpha=lr, beta=beta if use_momentum else 0.0)\n        train_losses.append(model.loss(X_train, y_train).item())\n        val_losses.append(model.loss(X_val, y_val).item())\n\n    return model, train_losses, val_losses\n\n\n\nMomentum vs No Momentum on Banknote Authentication\nI trained two logistic regression models on the Banknote Authentication dataset — one with momentum and one without. Both models were trained for 300 epochs, and I tracked training and validation loss throughout.\n\nmodel_m, train_m, val_m = train_model(X_train, y_train, X_val, y_val, use_momentum=True)\nmodel_nm, train_nm, val_nm = train_model(X_train, y_train, X_val, y_val, use_momentum=False)\n\nplt.plot(train_m, label=\"Train w/ Momentum\")\nplt.plot(val_m, label=\"Val w/ Momentum\")\nplt.plot(train_nm, '--', label=\"Train no Momentum\")\nplt.plot(val_nm, '--', label=\"Val no Momentum\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs shown in the plot, both models converge, but the one trained with momentum reaches lower training and validation loss more quickly. The gap is especially noticeable early in training, highlighting momentum’s ability to accelerate convergence without harming generalization.\n\nprint(\"With Momentum:\")\nprint(\"Test Loss:\", model_m.loss(X_test, y_test).item())\nprint(\"Test Accuracy:\", accuracy(model_m, X_test, y_test))\n\nprint(\"\\nWithout Momentum:\")\nprint(\"Test Loss:\", model_nm.loss(X_test, y_test).item())\nprint(\"Test Accuracy:\", accuracy(model_nm, X_test, y_test))\n\nWith Momentum:\nTest Loss: 0.17036399245262146\nTest Accuracy: 0.9599999785423279\n\nWithout Momentum:\nTest Loss: 0.6167036294937134\nTest Accuracy: 0.6218181848526001\n\n\n\n\nFinal Evaluation on Test Set\nAfter training, I evaluated both models on the test set to assess generalization above\nThe difference is significant — the model trained with momentum generalizes much better. This reinforces the effectiveness of momentum not just in speeding up training, but also in helping the optimizer escape poor local minima and converge to flatter, more generalizable solutions.\n\n\nEvolutionary Optimization vs Gradient Descent\nTo push the boundaries of optimization strategies, I compared my EvolutionOptimizer with traditional gradient descent on a high-dimensional, low-noise synthetic dataset (200 features, 1000 points).\nI configured the evolutionary algorithm with a high mutation rate (0.5) and a population size of 100. Both optimizers ran until they achieved 100% training accuracy. I recorded the loss at each step and the total time taken for convergence.\nAs shown in the plot, the two approaches followed different optimization paths. Gradient descent (with momentum) was faster and more stable, while the evolutionary strategy took longer but still reached perfect accuracy. This experiment highlights the potential of evolutionary algorithms in settings where gradient information is unavailable or unreliable.\n\nX, y = classification_data(p_dims=200, noise=0.8, n_points=1000)\nLR = LogisticRegression()\nopt = EvolutionOptimizer(LR)\nopt.set_mutation(0.5)\nopt.set_population_size(100)\nlosses = []\nstart = time.time()\nwhile accuracy(LR, X, y) &lt; 1:\n    opt.step(X, y)\n    losses.append(LR.loss(X, y).item())\nprint(\"Time taken: \", time.time() - start)\n\n\nLR2 = LogisticRegression()\nopt2 = GradientDescentOptimizer(LR2)\nlosses2 = []\nstart = time.time()\nwhile accuracy(LR2, X, y) &lt; 1:\n    opt2.step(X, y, alpha=0.1, beta=0)\n    losses2.append(LR2.loss(X, y).item())\nprint(\"Time taken: \", time.time() - start)\nplt.plot(losses, label=\"Evolution Optimizer Loss\")\nplt.plot(losses2, label=\"Gradient Descent Loss\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Comparison of Optimizers: Evolution vs Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n    \n\nTime taken:  2.2356069087982178\nTime taken:  0.22991013526916504\n\n\n\n\n\n\n\n\n\n\n\nOptimization Results\nBoth the evolutionary optimizer and gradient descent successfully achieved perfect training accuracy. However, gradient descent was significantly faster, converging in approximately 0.009 seconds compared to 0.365 seconds for the evolutionary approach.\nThe loss curves also reveal key differences: gradient descent steadily and smoothly reduces loss, while the evolutionary optimizer exhibits a more jagged trajectory with slower overall convergence. This is expected as we are only relying on randomness to introduce “genes” with better performance. Despite this, the evolutionary method still managed to find a good solution, demonstrating its viability as an alternative strategy in high-dimensional settings.\n\n\nVisualizing Decision Boundaries from the Population\nTo better understand the diversity within the population of evolved models, I randomly sampled four individuals from the final population of the EvolutionOptimizer. Since the data lives in a 200-dimensional space, I used PCA to project both the data and model weights into 2D for visualization.\nEach subplot shows a decision boundary generated from one sampled individual’s projected weights, alongside the full dataset projected into the same PCA space. The axes are fixed between -10 and 10 to provide a consistent frame of reference across plots. This gives a glimpse into the variety of linear separators present in the population, some of which may still be viable despite not being the single “best” solution.\n\nfrom sklearn.decomposition import PCA\nsamples = random.sample(LR.population, 4)\n\npca = PCA(n_components=2)\nX_2D = pca.fit_transform(X[:, :-1]) \n\ndef project_weights(w, pca):\n    w_no_bias = w[:-1].detach().numpy() \n    bias = w[-1].item()\n    w_pca = pca.components_ @ w_no_bias\n    return torch.tensor([*w_pca, bias], dtype=torch.float32)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor i, (w, ax) in enumerate(zip(samples, axes)):\n    w_proj = project_weights(w, pca)\n\n    ax.scatter(X_2D[:, 0], X_2D[:, 1], c=y, cmap=\"bwr\", alpha=0.5, edgecolor='k')\n\n    x_vals = torch.linspace(-10, 10, 200)\n    y_vals = -(w_proj[0] * x_vals + w_proj[2]) / w_proj[1]\n\n    ax.plot(x_vals, y_vals, color='black')\n    ax.set_title(f\"Sample {i+1} from Population\")\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n    ax.set_xlabel(\"PCA Component 1\")\n    ax.set_ylabel(\"PCA Component 2\")\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDiversity in Evolved Decision Boundaries (PCA Projection)\nThis visualization shows four randomly selected individuals from the final population of the EvolutionOptimizer, each plotted as a linear decision boundary in 2D PCA space.\nDespite the original feature space having 200 dimensions, PCA allows us to reduce the data to its two most informative components for visualization. Each black line represents a different individual’s projected weight vector and bias term, while the red and blue points show the two data classes.\nInterestingly, while the overall structure of the population seems fairly consistent, small variations in angle and offset reflect the natural diversity produced by evolutionary mutation and crossover. This illustrates how evolutionary algorithms maintain a pool of viable—but slightly different—solutions.\n\n\nConclusion\nThrough this exploration, I implemented and compared multiple optimization strategies for logistic regression, including vanilla gradient descent, gradient descent with momentum, and an evolutionary algorithm. Each method brought its own strengths: momentum accelerated convergence and improved generalization, while the evolutionary approach offered robustness and solution diversity, even in high-dimensional spaces.\nWorking with both synthetic and real-world datasets allowed me to understand not just how these optimizers behave in theory, but how they perform in practice under noise, overfitting risks, and varying feature dimensions. Visualizations like PCA projections and loss curves provided intuitive insights into what the evolutionary optimizer was doing under the hood.\nUltimately, this project deepened my appreciation for optimization in machine learning—not just as a mathematical procedure, but as a creative design space full of trade-offs and gains."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Machine Learning Blog",
    "section": "",
    "text": "Overparameterization and Double Descent\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 10, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Newton and Adam Optimizers\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nExploring and Implementing the Perceptron\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMar 23, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Numbers Lie\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\nMachine Learning\n\n\nBias\n\n\nFairness\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Feature Separability in Penguin Classification\n\n\n\n\n\n\nMachine Learning\n\n\nProjects\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\nNo matching items"
  }
]