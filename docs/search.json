[
  {
    "objectID": "music.html",
    "href": "music.html",
    "title": "Yahya's Blog",
    "section": "",
    "text": "import pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\ndf[\"genre\"].unique()\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy', \"genre\"]      \ndf_ef = df[engineered_features]\ndf_lyrics = df[['lyrics', 'genre']]\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfrom sklearn.preprocessing import StandardScaler\n\ndef prepare_df(df):\n    y = df[\"genre\"]\n    y = le.fit_transform(y)\n    X = df.drop(columns=[\"genre\"])\n\n    # Use StandardScaler for more stable training\n    scaler = StandardScaler()\n    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n    \n    return X, y\n\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n# Select only the engineered features\nengineered_features = ['dating', 'violence', 'world/life', 'night/time', 'shake the audience', 'family/gospel', \n                       'romantic', 'communication', 'obscene', 'music', 'movement/places', \n                       'light/visual perceptions', 'family/spiritual', 'like/girls', 'sadness', \n                       'feelings', 'danceability', 'loudness', 'acousticness', 'instrumentalness', \n                       'valence', 'energy', 'genre']\ndf_ef = df[engineered_features]\n\n# Encode the target labels\nle = LabelEncoder()\ndf_ef.loc[:, \"genre\"] = le.fit_transform(df_ef[\"genre\"])\n# Separate features and labels\nX = df_ef.drop(columns=[\"genre\"]).values\ny = df_ef[\"genre\"].values\n\n# Use MinMax scaling for more consistent gradients\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n# Oversample the minority classes\nsmote = SMOTE(random_state=42)\nX, y = smote.fit_resample(X, y)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Create DataLoader for training and validation sets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n\n# Define the DNN model with more aggressive regularization\nclass DeepDNN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(DeepDNN, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n# Initialize the model, loss function, and optimizer\ninput_size = X_train.shape[1]\nnum_classes = len(le.classes_)\nmodel = DeepDNN(input_size, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)\n\n# Use a more aggressive learning rate scheduler\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n\n# Training loop with early stopping\nnum_epochs = 100\nbest_val_acc = 0\npatience = 10\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n\n    val_acc = 100 * correct / total\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n          f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_acc:.2f}%\")\n\n    # Early stopping\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        early_stop_counter = 0\n        print(f\"New best validation accuracy: {val_acc:.2f}%\")\n    else:\n        early_stop_counter += 1\n\n    if early_stop_counter &gt;= patience:\n        print(\"Early stopping triggered.\")\n        break\n\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[1], line 35\n     33 # Oversample the minority classes\n     34 smote = SMOTE(random_state=42)\n---&gt; 35 X, y = smote.fit_resample(X, y)\n     37 # Train-test split\n     38 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nFile ~/miniforge3/envs/pytorch/lib/python3.12/site-packages/imblearn/base.py:202, in BaseSampler.fit_resample(self, X, y, **params)\n    181 def fit_resample(self, X, y, **params):\n    182     \"\"\"Resample the dataset.\n    183 \n    184     Parameters\n   (...)\n    200         The corresponding label of `X_resampled`.\n    201     \"\"\"\n--&gt; 202     return super().fit_resample(X, y, **params)\n\nFile ~/miniforge3/envs/pytorch/lib/python3.12/site-packages/sklearn/base.py:1389, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1382     estimator._validate_params()\n   1384 with config_context(\n   1385     skip_parameter_validation=(\n   1386         prefer_skip_nested_validation or global_skip_validation\n   1387     )\n   1388 ):\n-&gt; 1389     return fit_method(estimator, *args, **kwargs)\n\nFile ~/miniforge3/envs/pytorch/lib/python3.12/site-packages/imblearn/base.py:97, in SamplerMixin.fit_resample(self, X, y, **params)\n     72 @_fit_context(prefer_skip_nested_validation=True)\n     73 def fit_resample(self, X, y, **params):\n     74     \"\"\"Resample the dataset.\n     75 \n     76     Parameters\n   (...)\n     95         The corresponding label of `X_resampled`.\n     96     \"\"\"\n---&gt; 97     check_classification_targets(y)\n     98     arrays_transformer = ArraysTransformer(X, y)\n     99     X, y, binarize_y = self._check_X_y(X, y)\n\nFile ~/miniforge3/envs/pytorch/lib/python3.12/site-packages/sklearn/utils/multiclass.py:222, in check_classification_targets(y)\n    214 y_type = type_of_target(y, input_name=\"y\")\n    215 if y_type not in [\n    216     \"binary\",\n    217     \"multiclass\",\n   (...)\n    220     \"multilabel-sequences\",\n    221 ]:\n--&gt; 222     raise ValueError(\n    223         f\"Unknown label type: {y_type}. Maybe you are trying to fit a \"\n    224         \"classifier, which expects discrete classes on a \"\n    225         \"regression target with continuous values.\"\n    226     )\n\nValueError: Unknown label type: unknown. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n\n\n\n\nnum_classes = len(le.classes_)\nprint(\"Number of classes:\", num_classes)\n\n\nprint(pd.Series(y_train).value_counts())\n\n\n!pip install imblearn\n\nRequirement already satisfied: imblearn in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imblearn) (0.13.0)\nRequirement already satisfied: numpy&lt;3,&gt;=1.24.3 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (2.2.4)\nRequirement already satisfied: scipy&lt;2,&gt;=1.10.1 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (1.15.2)\nRequirement already satisfied: scikit-learn&lt;2,&gt;=1.3.2 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (1.6.1)\nRequirement already satisfied: sklearn-compat&lt;1,&gt;=0.1 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (0.1.3)\nRequirement already satisfied: joblib&lt;2,&gt;=1.1.1 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (1.4.2)\nRequirement already satisfied: threadpoolctl&lt;4,&gt;=2.0.0 in /Users/yahyarahhawi/miniforge3/envs/pytorch/lib/python3.12/site-packages (from imbalanced-learn-&gt;imblearn) (3.6.0)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Machine Learning Blog",
    "section": "",
    "text": "Evolution Based Weight Vector Optimization\n\n\n\n\n\nImplementation of Evolution Based Weight Vector Optimization\n\n\n\n\n\nJul 5, 2026\n\n\nJames Cummings, Jiffy Lesica, Yahya Rahhawi, Lukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nOverparameterization and Double Descent\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 10, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Newton and Adam Optimizers\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nImplementation\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nExploring and Implementing the Perceptron\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMar 23, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Numbers Lie\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\nMachine Learning\n\n\nBias\n\n\nFairness\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Feature Separability in Penguin Classification\n\n\n\n\n\n\nMachine Learning\n\n\nProjects\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Implementing-logistic-regression/SGD.html",
    "href": "posts/Implementing-logistic-regression/SGD.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nIn this project, I explore different optimization strategies for logistic regression, including vanilla gradient descent, momentum-based gradient descent, and an evolutionary algorithm. By applying these methods to both synthetic and real-world datasets, I analyze their convergence behavior, generalization performance, and decision boundary characteristics. Visualizations such as PCA projections and loss curves provide a deeper understanding of each method’s dynamics. The study highlights the strengths and trade-offs of traditional and evolutionary approaches, offering insight into how different optimizers shape model learning in high-dimensional settings. You can find my implementation of Logistic Regression at this GitHub link.\n\n\n\nSetup\nLet’s load essential components for the project. Let’s start to import my implementations of LogisticRegression, GradientDescentOptimizer, and EvolutionOptimizer (which will be explored later), along with core libraries like torch, numpy, and time.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, EvolutionOptimizer\nimport torch\nimport numpy as np\nimport time\nimport random\n\n\n\nGenerating Synthetic Data\nI use a data generation function borrowed from Professor Chodrow’s notes to create a simple binary classification dataset. It produces two noisy Gaussian clusters with a bias term appended to the features. Here, I generate 300 points with a noise level of 0.5.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nTraining with Gradient Descent and Momentum\nI initialized a LogisticRegression model and trained it using gradient descent with momentum (β = 0.9) for 100 iterations. After each step, I recorded the loss to track convergence. This setup allowed me to observe how the model’s performance evolved over time.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nlosses = []\nfor _ in range(100):\n    opt.step(X, y, alpha=0.1, beta=0.9)\n    losses.append(LR.loss(X, y).item())\n\n\nLet’s plot them and see what we have!\n\nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over time with Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nVisualizing Loss Convergence\nI plotted the loss over 100 iterations to visualize how the model trained with momentum converges. As expected, the loss decreases steadily and smoothly, showing that gradient descent with momentum effectively accelerates convergence. It would be interesting to compare the rate of convergence to a vanilla optimizer (no momentum)\nI will also borrow a couple of functions from professor Chodrow’s notes that will help me visualize the logistic regression model and the decision boundary\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\nVanilla Gradient Descent and Decision Boundary\nTo test the baseline performance of logistic regression, I trained the model using vanilla gradient descent (β = 0) on a low-noise dataset so that I can make sure they’re linearly separable (even tho the model will converge any way). I ran the optimization for 1000 iterations and plotted the loss curve, which decreases steadily. I also visualized the final decision boundary, which cleanly separates the two classes, indicating that the model has successfully learned a good linear separator.\n\nX, y = classification_data(p_dims=2, noise=0.2)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nlosses = []\nfor _ in range(1000):\n    opt.step(X, y, alpha=0.1, beta=0.0)\n    losses.append(LR.loss(X, y).item())\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time (Vanilla GD, β = 0)\")\nplt.grid(True)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\ndraw_line(LR.w, X[:,0].min(), X[:,0].max(), ax, color=\"black\", label=\"Decision Boundary\")\nax.legend()\nax.set_title(\"Decision Boundary (Vanilla GD, β = 0)\")\nplt.show()\n\n\n\n\n\n\n\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_28797/1220661661.py:13: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\nTraining with vanilla gradient descent (β = 0) over 1000 iterations produced a steadily decreasing loss, as shown in the first plot. The loss curve reflects stable convergence toward a minimum without oscillations or instability. This is expected as our loss function is convex and GD should be able to get us to the minimum without fluctuations.\nIn the second plot, I visualized the decision boundary learned by the model. It cleanly separates the two classes, indicating that even without momentum, gradient descent is capable of finding a good linear separator given enough iterations and low noise.\n\n\nComparing Vanilla Gradient Descent vs Momentum\nTo explore the benefits of momentum, I trained two logistic regression models on the same dataset: one with vanilla gradient descent (β = 0) and the other with momentum (β = 0.9). As shown in the plot, the model trained with momentum converged faster — reaching a lower loss in fewer iterations. This illustrates how momentum can help the optimizer move more effectively through flatter regions of the loss surface.\n\nX, y = classification_data(p_dims=2, noise=0.2)\n\n# Vanilla GD (β = 0)\nLR_vanilla = LogisticRegression()\nopt_vanilla = GradientDescentOptimizer(LR_vanilla)\n\nlosses_vanilla = []\nstart_time = time.time()\nfor _ in range(100):\n    opt_vanilla.step(X, y, alpha=0.1, beta=0.0)\n    losses_vanilla.append(LR_vanilla.loss(X, y).item())\nprint(\"Vanilla GD Time:\", time.time() - start_time)\n# Momentum GD (β = 0.9)\nLR_momentum = LogisticRegression()\nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\nlosses_momentum = []\nstart_time = time.time()\nfor _ in range(100):\n    opt_momentum.step(X, y, alpha=0.1, beta=0.9)\n    losses_momentum.append(LR_momentum.loss(X, y).item())\nprint(\"Momentum GD Time:\", time.time() - start_time)\n# Plot both losses\nplt.figure(figsize=(10, 5))\nplt.plot(losses_vanilla, label=\"Vanilla GD (β = 0)\")\nplt.plot(losses_momentum, label=\"Momentum GD (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time: Vanilla vs Momentum\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nVanilla GD Time: 0.005932807922363281\nMomentum GD Time: 0.005321979522705078\n\n\n\n\n\n\n\n\n\n\n\nMomentum Accelerates Convergence\nThis plot compares the loss over time for two models: one trained using vanilla gradient descent (β = 0) and the other using momentum (β = 0.9). Both models were trained for 100 iterations on the same dataset. The momentum-based optimizer clearly converges more quickly, achieving a lower loss in fewer steps. This demonstrates the effectiveness of momentum in accelerating optimization by dampening oscillations and pushing consistently in productive directions. Also, not only that Momentum made the model converge faster, it was also faster!\nSince we will be using accuracy as a metric, or potentially as an early stopping criteria, let’s define a simple function that calculates it based on the model, data, and labels\n\ndef accuracy(model, X, y):\n    y_pred = model.predict(X)\n    return (y_pred == y).float().mean().item()\n\n\n\nOverfitting in High Dimensions\nTo explore model behavior in a high-dimensional setting, I generated a training and testing dataset with 100 features each. I trained a logistic regression model using vanilla gradient descent (β = 0) until it achieved 100% accuracy on the training set. The loss at convergence confirms perfect separation on the training data — a clear sign of overfitting, especially given the relatively small sample size compared to the feature count.\n\nX_train, y_train = classification_data(p_dims=100,n_points=50, noise=0.2)\nX_test, y_test = classification_data(p_dims=100,n_points=50, noise=0.2)\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\ntrain_accuracy = 0\nwhile train_accuracy != 1:\n    opt.step(X_train, y_train, alpha=0.01, beta=0)\n    train_accuracy = accuracy(LR, X_train, y_train)\n\nprint(\"Training Loss: \", LR.loss(X_train, y_train).item())\n\nTraining Loss:  0.1373424082994461\n\n\n\ntrain_accuracy\n\n1.0\n\n\nThe model successfully achieved 100% training accuracy, with a final loss of approximately 0.15. This confirms that the model has completely fit the training data. While impressive on the surface, this is a classic sign of overfitting — especially in a setting with far more features (100) than samples (50).\n\n\nGeneralization Check\nAfter achieving perfect accuracy on the training set, I evaluated the model on a separate test set drawn from the same distribution. The model achieved a test accuracy of 87.99%, suggesting that while there is some risk of overfitting due to the high dimensionality, the model still generalizes well in this case — likely due to the low noise and relatively separable data.\n\ntest_accuracy = accuracy(LR, X_test, y_test)\ntest_accuracy\n\n0.8799999952316284\n\n\n\n\nReal-World Dataset: Banknote Authentication\nTo evaluate my model on real-world data, I used the Banknote Authentication dataset from the UCI Machine Learning Repository. This dataset contains four numerical features extracted from images of banknotes, along with a binary class label indicating whether a note is genuine or forged.\nI standardized the features using StandardScaler and added a bias term manually. The dataset was then split into training (60%), validation (20%), and test (20%) sets to allow for model tuning and evaluation.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", header=None)\ndf.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n\nX = df.drop(\"class\", axis=1).values\ny = df[\"class\"].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\nX = torch.cat([X, torch.ones(X.size(0), 1)], dim=1)\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nTraining Function with Optional Momentum\nTo streamline experimentation, I created a train_model function that trains a logistic regression model using gradient descent, with an optional momentum parameter. The function tracks both training and validation loss over time, allowing for easy comparison of optimization strategies. It accepts learning rate, momentum factor (β), and number of epochs as configurable parameters.\n\ndef train_model(X_train, y_train, X_val, y_val, use_momentum=False, lr=0.01, beta=0.9, epochs=300):\n    model = LogisticRegression()\n    opt = GradientDescentOptimizer(model)\n\n    train_losses, val_losses = [], []\n\n    for i in range(epochs):\n        opt.step(X_train, y_train, alpha=lr, beta=beta if use_momentum else 0.0)\n        train_losses.append(model.loss(X_train, y_train).item())\n        val_losses.append(model.loss(X_val, y_val).item())\n\n    return model, train_losses, val_losses\n\n\n\nMomentum vs No Momentum on Banknote Authentication\nI trained two logistic regression models on the Banknote Authentication dataset — one with momentum and one without. Both models were trained for 300 epochs, and I tracked training and validation loss throughout.\n\nmodel_m, train_m, val_m = train_model(X_train, y_train, X_val, y_val, use_momentum=True)\nmodel_nm, train_nm, val_nm = train_model(X_train, y_train, X_val, y_val, use_momentum=False)\n\nplt.plot(train_m, label=\"Train w/ Momentum\")\nplt.plot(val_m, label=\"Val w/ Momentum\")\nplt.plot(train_nm, '--', label=\"Train no Momentum\")\nplt.plot(val_nm, '--', label=\"Val no Momentum\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs shown in the plot, both models converge, but the one trained with momentum reaches lower training and validation loss more quickly. The gap is especially noticeable early in training, highlighting momentum’s ability to accelerate convergence without harming generalization.\n\nprint(\"With Momentum:\")\nprint(\"Test Loss:\", model_m.loss(X_test, y_test).item())\nprint(\"Test Accuracy:\", accuracy(model_m, X_test, y_test))\n\nprint(\"\\nWithout Momentum:\")\nprint(\"Test Loss:\", model_nm.loss(X_test, y_test).item())\nprint(\"Test Accuracy:\", accuracy(model_nm, X_test, y_test))\n\nWith Momentum:\nTest Loss: 0.17036399245262146\nTest Accuracy: 0.9599999785423279\n\nWithout Momentum:\nTest Loss: 0.6167036294937134\nTest Accuracy: 0.6218181848526001\n\n\n\n\nFinal Evaluation on Test Set\nAfter training, I evaluated both models on the test set to assess generalization above\nThe difference is significant — the model trained with momentum generalizes much better. This reinforces the effectiveness of momentum not just in speeding up training, but also in helping the optimizer escape poor local minima and converge to flatter, more generalizable solutions.\n\n\nEvolutionary Optimization vs Gradient Descent\nTo push the boundaries of optimization strategies, I compared my EvolutionOptimizer with traditional gradient descent on a high-dimensional, low-noise synthetic dataset (200 features, 1000 points).\nI configured the evolutionary algorithm with a high mutation rate (0.5) and a population size of 100. Both optimizers ran until they achieved 100% training accuracy. I recorded the loss at each step and the total time taken for convergence.\nAs shown in the plot, the two approaches followed different optimization paths. Gradient descent (with momentum) was faster and more stable, while the evolutionary strategy took longer but still reached perfect accuracy. This experiment highlights the potential of evolutionary algorithms in settings where gradient information is unavailable or unreliable.\n\nX, y = classification_data(p_dims=200, noise=0.8, n_points=1000)\nLR = LogisticRegression()\nopt = EvolutionOptimizer(LR)\nopt.set_mutation(0.5)\nopt.set_population_size(100)\nlosses = []\nstart = time.time()\nwhile accuracy(LR, X, y) &lt; 1:\n    opt.step(X, y)\n    losses.append(LR.loss(X, y).item())\nprint(\"Time taken: \", time.time() - start)\n\n\nLR2 = LogisticRegression()\nopt2 = GradientDescentOptimizer(LR2)\nlosses2 = []\nstart = time.time()\nwhile accuracy(LR2, X, y) &lt; 1:\n    opt2.step(X, y, alpha=0.1, beta=0)\n    losses2.append(LR2.loss(X, y).item())\nprint(\"Time taken: \", time.time() - start)\nplt.plot(losses, label=\"Evolution Optimizer Loss\")\nplt.plot(losses2, label=\"Gradient Descent Loss\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Comparison of Optimizers: Evolution vs Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n    \n\nTime taken:  2.2356069087982178\nTime taken:  0.22991013526916504\n\n\n\n\n\n\n\n\n\n\n\nOptimization Results\nBoth the evolutionary optimizer and gradient descent successfully achieved perfect training accuracy. However, gradient descent was significantly faster, converging in approximately 0.009 seconds compared to 0.365 seconds for the evolutionary approach.\nThe loss curves also reveal key differences: gradient descent steadily and smoothly reduces loss, while the evolutionary optimizer exhibits a more jagged trajectory with slower overall convergence. This is expected as we are only relying on randomness to introduce “genes” with better performance. Despite this, the evolutionary method still managed to find a good solution, demonstrating its viability as an alternative strategy in high-dimensional settings.\n\n\nVisualizing Decision Boundaries from the Population\nTo better understand the diversity within the population of evolved models, I randomly sampled four individuals from the final population of the EvolutionOptimizer. Since the data lives in a 200-dimensional space, I used PCA to project both the data and model weights into 2D for visualization.\nEach subplot shows a decision boundary generated from one sampled individual’s projected weights, alongside the full dataset projected into the same PCA space. The axes are fixed between -10 and 10 to provide a consistent frame of reference across plots. This gives a glimpse into the variety of linear separators present in the population, some of which may still be viable despite not being the single “best” solution.\n\nfrom sklearn.decomposition import PCA\nsamples = random.sample(LR.population, 4)\n\npca = PCA(n_components=2)\nX_2D = pca.fit_transform(X[:, :-1]) \n\ndef project_weights(w, pca):\n    w_no_bias = w[:-1].detach().numpy() \n    bias = w[-1].item()\n    w_pca = pca.components_ @ w_no_bias\n    return torch.tensor([*w_pca, bias], dtype=torch.float32)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten()\n\nfor i, (w, ax) in enumerate(zip(samples, axes)):\n    w_proj = project_weights(w, pca)\n\n    ax.scatter(X_2D[:, 0], X_2D[:, 1], c=y, cmap=\"bwr\", alpha=0.5, edgecolor='k')\n\n    x_vals = torch.linspace(-10, 10, 200)\n    y_vals = -(w_proj[0] * x_vals + w_proj[2]) / w_proj[1]\n\n    ax.plot(x_vals, y_vals, color='black')\n    ax.set_title(f\"Sample {i+1} from Population\")\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n    ax.set_xlabel(\"PCA Component 1\")\n    ax.set_ylabel(\"PCA Component 2\")\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDiversity in Evolved Decision Boundaries (PCA Projection)\nThis visualization shows four randomly selected individuals from the final population of the EvolutionOptimizer, each plotted as a linear decision boundary in 2D PCA space.\nDespite the original feature space having 200 dimensions, PCA allows us to reduce the data to its two most informative components for visualization. Each black line represents a different individual’s projected weight vector and bias term, while the red and blue points show the two data classes.\nInterestingly, while the overall structure of the population seems fairly consistent, small variations in angle and offset reflect the natural diversity produced by evolutionary mutation and crossover. This illustrates how evolutionary algorithms maintain a pool of viable—but slightly different—solutions.\n\n\nConclusion\nThrough this exploration, I implemented and compared multiple optimization strategies for logistic regression, including vanilla gradient descent, gradient descent with momentum, and an evolutionary algorithm. Each method brought its own strengths: momentum accelerated convergence and improved generalization, while the evolutionary approach offered robustness and solution diversity, even in high-dimensional spaces.\nWorking with both synthetic and real-world datasets allowed me to understand not just how these optimizers behave in theory, but how they perform in practice under noise, overfitting risks, and varying feature dimensions. Visualizations like PCA projections and loss curves provided intuitive insights into what the evolutionary optimizer was doing under the hood.\nUltimately, this project deepened my appreciation for optimization in machine learning—not just as a mathematical procedure, but as a creative design space full of trade-offs and gains."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html",
    "href": "posts/when-numbers-lie/index.html",
    "title": "When Numbers Lie",
    "section": "",
    "text": "I came to machine learning because I believed in its elegance—the way a few lines of code could make predictions, optimize decisions, and maybe even eliminate human bias. Studying philosophy alongside computer science, I have always wondered if utilizing the machine, supported with some rigorous math, can eliminate some tendencies in some humans to perpetuate injustice and unfairness. But somewhere along the way, this class made me ask: Whose data is this model trained on? What decisions does it reinforce? And who gets left behind when we define “fairness” with math?\nThese questions became even more urgent as I read Arvind Narayanan’s 2022 James Baldwin Lecture, The Limits of the Quantitative Approach to Discrimination (Narayanan 2022). Narayanan argues that current quantitative methods often serve to justify the status quo more than they challenge it. That’s a bold statement—especially for a field that prides itself on objectivity and precision. But as I sat with this claim, I realized: it resonates. At a time when the U.S. government is investing over $500 billion into machine learning (Reuters Staff 2025)—much of it flowing into public systems that affect millions of lives—it’s critical to examine what ethical foundation, if any, these models are built on. Especially during a time where the government is displaying tendencies to practice radical, and often problematic, interpretations of justice, using means that I consider unethical and are against what states can do. Is machine learning another tool for them to justify their actions?\nThis isn’t a theoretical concern. The U.S. government has reportedly begun employing AI-powered agents to monitor the social media of international students, scanning for signs of “support” for terrorist organizations, but also any signs of criticizing the complicity of the government in war crimes happening right now in the Middle East (Fried 2025; Weiss and Parker 2025). This process—untransparent, unauditable, and unchallengeable—has led to the flagging and deportation of students based on ambiguous indicators and culturally uncontextualized speech. What’s happening here is exactly what Narayanan warns about: using mathematical tools to assess fairness based on the same logics that have historically led to unfairness. These models are embedded with political and cultural assumptions, yet shielded from scrutiny by a veil of statistical legitimacy.\n\n\nNarayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves.\n\n\n\nTo be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist.\n\n\n\nA more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art.\n\n\n\nTo me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me.\n\n\n\nSo, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "href": "posts/when-numbers-lie/index.html#the-illusion-of-objectivity",
    "title": "When Numbers Lie",
    "section": "",
    "text": "Narayanan’s core critique is this: quantitative methods, especially when wielded without critical awareness, give us the illusion of objectivity while silently reproducing injustice (Narayanan 2022). Researchers make countless subjective decisions when designing models—what features to include, what fairness metric to optimize, what dataset to trust. These choices are often framed as neutral, yet they reflect particular worldviews and assumptions.\nWhat’s worse is that these methods rarely question the framing of the problem. Narayanan argues that if we assess reality with the same lens that led it to become unfair, we can’t expect much change to happen. This means that there’s a structural trap in the way we approach fairness: instead of shifting the paradigm, we tweak parameters.\nFor example, if a hiring algorithm discriminates against people with non-white-sounding names, we might try to retrain it with “blind” features or adjust the threshold. But the question remains: why does the system prioritize certain qualifications or communication styles in the first place? What histories of exclusion built the resume formats we consider “professional”? These deeper layers are not captured by confusion matrices or calibration curves."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "href": "posts/when-numbers-lie/index.html#when-quantitative-methods-work-and-dont",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To be clear, quantitative methods aren’t useless. In Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan describe how algorithms can expose patterns of inequality at a scale that human intuition cannot. They point out that data-driven systems can, in some cases, be more transparent than human decision-making—because at least we can audit the code and track the outcomes.\nOne powerful example is the use of audit studies in hiring discrimination. Bertrand and Mullainathan’s seminal resume experiment, where identical resumes were sent out with “white-sounding” and “Black-sounding” names, revealed stark differences in callback rates. Technically, this study was probing the fairness notion of demographic parity or equal opportunity, depending on how you interpret the outcome variable. Morally, it revealed a clear case of allocative harm: job opportunities were being withheld purely on the basis of racial signifiers.\nThe strength of this study lies in its simplicity and clarity. It shows that discrimination exists—not as an abstract theory but as a measurable, reproducible reality. It forced a conversation in policy and public discourse, and rightly so. Fairness doesn’t simply arise when we get rid of human actors in making the decisions directly, and replacing them with machines designed to observe our perspective (through data) of the status quo, in fact, this is the last thing we want to do when we try to reimagine our society to be more fair and radically different from the unfair version of what already exists.\nBut even this kind of analysis has limits. As Fairness and Machine Learning points out, the use of names as proxies for race is itself fraught. What if employers are responding not to perceived race, but to assumptions about class, region, or education? The interpretation is never clean. And that’s part of Narayanan’s argument: the idea that we can simply “measure” fairness assumes a tidy, apolitical world that does not exist."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "href": "posts/when-numbers-lie/index.html#when-algorithms-backfire",
    "title": "When Numbers Lie",
    "section": "",
    "text": "A more disturbing example of quantitative methods going wrong is found in the use of risk assessment tools in criminal justice. Tools like COMPAS assign scores to defendants predicting their likelihood of reoffending, often based on data from law enforcement records. These tools are “calibrated,” meaning that for each risk score, the actual rate of recidivism is about the same across racial groups.\nTechnically, calibration sounds like a fairness win. But morally? It’s a disaster. As ProPublica and researchers like Julia Angwin and Virginia Eubanks have shown, the data itself is biased: arrest records reflect over-policing in Black neighborhoods, not an intrinsic difference in behavior. So even if the algorithm is mathematically “fair,” its predictions reinforce biased policing and sentencing.\nThis is what Data Feminism by D’Ignazio and Klein (2023) calls the “privilege hazard” (D’Ignazio and Klein 2023): the people designing these systems often cannot see the assumptions baked into their models because they’ve never had to. Their lives aren’t algorithmically surveilled, flagged, or punished. And so they optimize for clean metrics rather than complex realities.\nTheir framework emphasizes that fairness is not just about inputs and outputs—it’s about power. Who collects the data? Who defines the labels? Who decides which outcomes matter? Data Feminism argues that without answering these questions, we are not doing fairness—we are doing statistical performance art."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#redefining-fairness",
    "href": "posts/when-numbers-lie/index.html#redefining-fairness",
    "title": "When Numbers Lie",
    "section": "",
    "text": "To me, fairness goes far beyond meritocracy—the belief that the most “qualified” should always win. In practice, meritocracy often just repackages privilege. Fairness isn’t about pretending we all start at the same line; it’s about acknowledging that we don’t—and building systems that reflect that truth.\nFairness also goes beyond what an algorithm can or can’t do. It’s a social commitment: a way of seeing others as equals, of including their experiences and voices in shaping the systems they live under. We can’t fix injustice with math alone. We need historical awareness, community input, qualitative insights, and above all, humility.\nRight now, too many people put too much trust in numbers without understanding what those numbers mean—or what they erase. In a world where “data-driven” is a synonym for “truth,” we have to ask: whose truths are missing from the dataset?\nThis is especially urgent for international students like myself. The idea that an AI agent could monitor my posts—stripping words from context, translating emotion into probability scores, and potentially flagging me for deportation—isn’t just dystopian. It’s happening. It’s real. It forces me to ask whether the systems we’re building even have a place for someone like me."
  },
  {
    "objectID": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "href": "posts/when-numbers-lie/index.html#a-qualified-agreement",
    "title": "When Numbers Lie",
    "section": "",
    "text": "So, where do I stand on Narayanan’s claim that quantitative methods “do more harm than good”? I agree—but with qualifications.\nYes, these tools often uphold the status quo. Yes, they obscure rather than reveal injustice. Yes, they can even be dangerous when placed in the hands of people who lack understanding of the cultural, political, and historical narratives that shaped the data.\nBut that’s not a reason to give up on the tools. It’s a reason to change who uses them—and how.\nAs Barocas et al. suggest, quantitative methods can serve transparency, accountability, and insight—if wielded with care (Barocas, Hardt, and Narayanan 2023). But that care has to be built into every step of the process: from data collection to metric choice to outcome interpretation. It requires interdisciplinary work, community engagement, and ongoing critique. It requires treating fairness not as a mathematical constraint, but as a moral imperative.\nI still believe in machine learning. But I no longer believe that fairness can be computed. Fairness has to be created—with intention, with reflection, and with people, not just models, at the center."
  },
  {
    "objectID": "posts/blog-post/main.html",
    "href": "posts/blog-post/main.html",
    "title": "Evolution Based Weight Vector Optimization",
    "section": "",
    "text": "Abstract\nThis blog post explores the application of the principals of evolution to weight vector optimization problems. A comprehensive evolutionary optimizer class, with hyperparameters allowing for control over selection, inheritance, diversity, mutations rates, and mutation styles, was created. Exploratory experiments were then run to attempt to understand the strengths and limitations of the various values for each of these hyperparameters. Experiments were performed on generated data as well as the MNIST dataset. Due to computation limitations, a complete optimization loop on the MNIST dataset was out of scope for this project. However, a final accuracy of 82% was achieved.\n\n\nIntroduction\nThis blog post explores how features of evolution in nature can inspire solutions to overcoming the shortcomings of gradient descent. Gradient Descent only works on differentiable loss functions, meaning it can become stuck in local loss minima when attempting to model non-convex loss functions. In other words, gradient descent cannot explore the entire solution space on nondifferentiable loss functions. This limitation can be overcome by harnessing the characteristics of evolution and natural selection in nature. Evolution has a wide variety of applications concerning Machine Learning, but this project focuses on its applications to weight vector optimization Telikani et al. (2021).\nLewontin identifies 3 key population characteristics for evolution: phenotypic variation in a population, differential fitness, and fitness must be heritable Lewontin (1970). With these 3 characteristics, evolution then occurs as ‘fitter’ individuals are better able to pass on their traits to future generations, while less fit individuals are not. At the individual level, evolution requires a blueprint, self-replication, mutation, and selection. By applying these principles to machine learning models, this blog post explores the strengths and limitations of evolutionary principles when applied to weight vector optimization in machine learning. To satisfy the requirement of phenotypic variation, each evolutionary optimizer has an entire population of weight vectors storing different weights. The different weights result in different losses, which in combination with selection pressures regarding the resulting different losses, satisfy the differential fitness requirement. With weight vectors serving as our genetic blueprint, those weight vectors can be duplicated to create or refill the population of weight vectors. Slight random adjustments to those weight vectors during replication serve as the mutations, ensuring the continuation of phenotypic variation. A variety of methods can be used to eliminate population vectors during an iteration, including loss and diversity, which function as selection. Eliminating high-loss weight vectors allows only vectors with high accuracy to pass on their characteristics, while eliminating low diversity can ensure that the solution space is adequately explored. Through the implementation of hyperparameters, many variations of evolutionary machine learning algorithms are explored to better understand their strengths and weaknesses.\nThe many hyperparameters are then tested on both generated and real data from the MNIST dataset to develop initial hypotheses regarding the optimal parameterization for evolutionary weight vector optimization to succeed.\n\n\nValues Statement\nThe potential users of the evolutionary-based weight vector optimization class are researchers, data scientists, and developers, especially those who work on non-differentiable problems with which gradient descent-based solutions struggle. Our class provides both a potential solution to overcoming the limitations of gradient descent on non-differentiable classification problems and serves as a potential benchmark against which other algorithms can be compared.\nOne major potential impact of the widespread use of our algorithm, or similar ones, is the increase in computational power required to run them. Because each epoch of an evolutionary algorithm requires the computation of an entire population of new weight vectors, the computational power required for an epoch is higher than most algorithms. This has potential positive implications for the manufacturers of computational chips and the owners of servers. On the other hand, the potential negative effects of increased energy and material consumption to perform these computations cannot be overlooked either.\nBecause the majority of our work was focused on the creation of a class, and not the optimization of a specific algorithm, the potential for positive and negative impacts of our class depends on who gains access to the class and what they decide to do with it.\n\n\nMaterials and Methods\nProject dependencies: - torch - numpy - pandas - scikit-learn - matplotlib\nOur evolutionary optimizer translates biological principles into a deep neural network optimiser. Biological evolution, and thus our algorithmic approach, rely on four core attributes: blueprint, self-replication, mutation, and selection. Our “blueprints,” genes or DNA in the natural world, are our weight vectors for the parameters of the neural network. We begin with an initial “population” of \\(N\\) such vectors that are sampled uniformly at random. In each generation, every individual is evaluated on a mini-batch of examples, combining cross-entropy loss (exploitation) with an optional diversity penalty (exploration). The lowest‐loss individuals (and occasionally a small “sneaker” fraction of high-loss outliers) serve as parents for the next generation. Some elite low-loss survivors carry forward unchanged. New offspring are created via uniform crossover, where each weight entry, or gene, is inherited from \\(k\\) randomly chosen parents, then mutated by adding small Gaussian or Laplacian noise with some probability. Optionally, each child can receive a single gradient‐descent step to fine-tune its accuracy. Initially, we relied on synthetic binary-classification data generated using torch.rand to train and validate our evolutionary approach. This allowed us to develop a proof of concept that evolution could, in fact, solve problems and that our selection, crossover, mutation, etc., behaved as expected before we moved on to real-world inputs.\n\n\nExample of MNIST Digits\n\nWe decided to employ our evolutionary approach to the MNIST handwritten-digit dataset LeCun, Cortes, and Burges (2010). This dataset is made up of 70,000 gray-scale images of size 28×28 pixels, labeled 0–9. We accessed the dataset through torch datasets. In smaller experiments with the MNIST dataset, we opted to draw a random subset of anywhere from 1,000 to 20,000 digits to improve computational efficiency. Although the smaller subsets enabled rapid prototyping, they may have overrepresented certain rarer handwriting styles and potentially skewed accuracy.\n\n\nHyperparameters\n\nProof of concept/vanilla evolution:\n\n\nSelection Processes:\nIn nature, genetic traits are passed from one generation to the next by individuals that survive and successfully reproduce. These survivors make up the gene pool of their generation, while those that fail to reproduce are effectively excluded from the evolutionary process. In our implementation, we emulate this principle by defining fitness based on standard cross-entropy loss or diversity-augmented loss, depending on the user. At each generation, we sort the population in a minheap based on loss, then the top 50% (the half with the lowest loss) are selected to form the gene pool. The other half does not have the chance to reproduce. In the next section, we will dive into how we handle creating the next generation from the gene pool.\n\n\nIllustration of Gene Pool Selection\n\nMirroring the random nature of evolution, we incorporate some chance in the makeup of our gene pool. A small number of lower-performing individuals (10% by default) are included in the gene pool with low probability. These individuals, whom we call sneakers, introduce genetic variation that helps maintain a diversified population and prevents premature convergence.\n\n\nIllustration of Sneaker Population\n\nFinally, we employ an elitist strategy to preserve our high-performing solutions. Each generation, a percentage of the top performers based purely on cross-entropy loss are included in the gene pool and also survive unchanged and unmutated to the next generation. This preserves the integrity of the best solutions by keeping a lineage of high-performing individuals.\n\n\nOverview of New Generation Gene Makeup\n\n\n\nInheritance and Parent Quantity:\nAt each iteration of our evolutionary optimization, following the creation of a ‘gene pool’ in the selection stage, the population must be replenished with new individuals. There are three ways that this can be accomplished. 1: All new individuals are new randomized weight vectors with no input from the gene pool. 2: Each new individual has a single parent randomly selected from the gene pool from which its weights are inherited with random mutations. 3: Each individual has n parents randomly selected from the gene pool. Each feature weight is then inherited from the corresponding feature weight of a random one of its parents.\nThe first scenario, with no inherited weight vectors, is a baseline against which our true evolutionary models can be tested. This is not truly evolution, as it does not include any heritability of fitness for new individuals in the population Lewontin (1970).\nThe second Scenario, includes heritability of fitness, but with only a single parent for each child individual, the diversity can be expected to be more limited.\n\n\nDiagram of Inheritance when num_parents = 1\n\nThe Third Scenario, allows for a slightly reduced heritability of fitness, with the addition of diverse new individuals produced with each generation. The diversity rate is specifically limitted by the mutation_rate and mutation_intensity hyperparameters.\n\n\nDiagram of Inheritance when num_parents = 2\n\nFunctionally, this process occurs after selection has occured, and an overall gene pool of parents has been created. A random sampling with replacement is then performed on that gene pool, in which each new child is assigned n, the value of the num_parents hyperparameter passed to the function, parents from the gene pool. For each weight in each child’s weight vector, a random one of that child’s n parents is then chosen from which it inherits that specific weight. If num_parents = 0, then every child recieves a completely random weight vector. Once the weights have been assigned, the child weight vector is then mutated.\nAs discussed in the results section, the choice of the number of parents can have a significant impact on loss, accuracy, and diversity.\n\n\nHybridizing evolution with gradient descent:\nOur approach to evolutionary optimization incorporates a gradient-based refinement step that allows individuals to local optimize their performance (slightly) after being created. In essence, this hybrid evolutionary-gradient approach combines the global search strengths of evolutionary algorithms with the precise, local updates enabled by backpropagation. For each new individual generated during the evolutionary step, we apply a single gradient update to refine its weights. This is accomplished using a method implemented within the model that performs a forward pass, calculates cross-entropy, and uses PyTorch’s automatic differentiation to compute gradients. Weights are then updated according to the direction of the negative gradient, and scaled by a learning rate set to 0.5.\nThe backpropagation step is called once per individual in the evolutionary loop, immediately after crossover and mutation have produced a new weight vector candidate. The updated individual is then re-inserted into the population. By integrating this light update of gradient descent, the optimizer benefits from enhancing convergence rates - while still being able to prioritize diversity - with fewer generations of evolution.\n\n\nComputing Diversity and Diversity-Based Loss:\nOur evolutionary optimization implementation includes a mechanism for encouraging population diversity by directly incorporating a diversity term into the model’s loss function. Diversity is measured over the entire population of weight vectors, with four distinct methods implemented to quantify it. These include Euclidean distance, cosine dissimilarity, standard deviation, and variance. The Euclidean distance metric calculates the mean spatial difference between every pair of individuals in the population. Cosine dissimilarity measures the angular dissimilarity of weight vectors by computing one mines the cosine similarity between weight vectors. The standard deviation and variance metrics, on the other hand, operate across the whole population of weight vectors by computing the average distribution/variance of all weight vectors within a generation.\nOnce computed, the diversity score is used to modify the model’s loss. Specifically, during each evaluation of an individual weight vector in the population, the standard cross-entropy loss is calculated and then a diversity term is subtracted from it. This diversity term equals the above mentioned diversity value scaled by a user-set diversity coefficient. The effect of this subtraction is that models with higher diversity scores receive a lower total loss, incentivizing the optimizer to explore a broader range of solutions. This diversity-aware loss is only applied when explicitly enabled through a boolean flag in the model, giving flexibility for experiments that compare/evaluate the performance of diversity-based and non-diversity based evolutionary optimization.\n\n\nAdjustment from binary to multi-class classification:\nBecause our target task is classification on the MNIST dataset - which involves 10 possible output classes (digits 0 through 9), we implemented multiclass classification using Pytorch’s CrossEntropyLoss function. Unlike binary cross-entropy, which assumes a binary classification problem and compares scalar outputs to binary labels, cross-entropy loss compares a vector of probabilities (logits) against a single target value label. This function internally applies a softmax operation which evaluates the likelihood of each logit being the right output class.\nIn our implementation, the CrossEntropyLoss function is used in both the model’s forward loss evaluation and backpropagation step. This ensures that each prediction is treated as a multiclass decision and that the model can properly learn to distinguish between all 10 classes in the MNIST dataset.\n\n\nMutation Methods:\nOne thing that we created in our vanilla EVO optimizer was a random mutation mechanism. This mutation mechanism let’s us assign a small probability that each of the weight’s entries’ values can be nudged by a certain amount positively or negatively. This nudge and its intensity is modeled by a normal distribution around the current value, and what we call “Mutation Intensity” is the standard deviation of that normal distribution. This ensures that we are constantly updating our weights randomly, and that there is a chance for weights to get better. What we noticed is that only using normal distribution might not be sufficient in achieving fast convergence. Because the normal distribution’s tails flatten with the X-axis quickly, it does not give the slightest opportunity for the model to get an aggressive nudge.\n\nThis led us to explore different distributions that also share the characteristic that ensures the nudge is usually not too aggressive, but also allows ever so rarely for it to change the weight’s entry significantly. This distribution that we introduced is the Laplacian distribution.\n\nThis became another hyperparameter that allows us to see how different mutation methods affect different models that our EVO optimizer tries to solve.\n\n\n\nResults\n\nChoices in Selection Processes:\nBy default, we select the best 50% of the population to enter the gene pool, however, this is a hyperparameter that users can play with. We conducted some experiments on a small subset of 1000 digits from the MNIST dataset to examine how different gene pool sizes (10%–90% of the population) would affect our accuracy, loss, and diversity over 500 generations.\n\n\nGene Pool Size vs Accuracy\n\n\n\nGene Pool Size vs Loss\n\n\n\nGene Pool Size vs Diversity\n\nThere are several interesting things to note about these figures. Focusing on the extremes first, only picking 10% of the best individuals is advantageous if we look purely at accuracy. However, this came at the cost of significantly reducing diversity, with such a small portion of the population passing through at each generation. Having too homogeneous a population can lead to getting stuck in local minima without exploring the wider loss landscape. On the other hand, having too many members of the population reproduce increases exploration of the loss landscape, but reduces selection pressure, as individuals with suboptimal solutions continue to reproduce. We can see this illustrated above as the accuracy lags far behind all of the other gene pool sizes. Keeping the best half performed is a strong middle ground with comparatively great accuracy, second only to keeping the top 10%, while remaining diverse.\nWe also investigated the effects of varying the probability that “sneakers”—individuals from the bottom 10% of the population—could enter the gene pool. We tested probabilities from 0–45%.\n\n\nSneaker Probability vs Accuracy\n\n\n\nSneaker Probability vs Loss\n\n\n\nSneaker Probability vs Diversity\n\nInterestingly, across a range of sneaker probabilities, we didn’t observe much variation in loss or diversity. So it doesn’t impact our learning dynamics to a noticeable degree. However, having a 45% sneaker probability performed quite well, accuracy-wise. This may be a reflection of random variation of our dataset or starting genepool, but it may also suggest that a degree of genetic noise can occasionally help guide the population out of local minima. In future experiments, it would be insightful to set the hyperparameter to be above 50% and see the results.\nFinally, we explored the impacts of elitism by varying the percentage of top-performing individuals who we carried unchanged to the next generation.\n\n\nElitist Population Size vs Accuracy\n\n\n\nElitist Population Size vs Loss\n\n\n\nElitist Population Size vs Diversity\n\nWhen we have too many elites, we slow down evolutionary convergence. We aren’t introducing enough change from generation to generation to explore the landscape and improve our solution. We can see evidence of this in our stunted accuracy, low diversity, and higher loss when we increase the size of our elite population. However, when we eliminate elites or keep only 5%, we see noticeable improvements. Our loss is converging faster, we maintain a diverse population, and our accuracies after 500 generations are the highest. Keeping the elite population helps our accuracy, outperforming the population without elites by over 5% over 500 generations. Overall, we observed that on MNIST, modest elitism provides a valuable balance between preserving high-quality solutions and allowing diversity within the population.\n\n\nInheritance and Parent Quantity:\nTwo experiments were performed to explore the limitations and strengths of different inheritance methods, specifically the adjustment in the number of parents from which each child weight vector receives it’s own weight vector values.\nFor both experiments below hyperparameters that were held constant were:\n-Survivor Ratio: 0.1\n-Fitness Ratio: 0.5\n-Sneaker Probability: 0.01\n-Sneakers ratio: 0.1\n-mutation rate: 0.05\n-mutation intensity: 0.05\n\nGenerated Data Experiment:\nThis generated data experiment explores the performance of our model when varying the num_parents hyperparameter. A multi parent classification experiment was run on generated 2 dimensional data with 0.2 noise and 300 points. The accuracy, loss, and Euclidean diversity was tracked across 300 iterations. The experiment was run with the hyperparameter num_parents set to 0, 1, 2, 3, 5, and 10.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on Generated data using 0, 1, 2, 3, 5, and 10 parents over 300 iterations\n\nAs seen in the above visualization, Loss and Accuracy were comparable across all quantities of parents, while diversity varied significantly. In particular, with num_parents set to 0 and to a lesser extent 1, diversity was much lower than all other quantities of parents. The accuracy of the 0 parent model also performed worse than the other models over more iterations. With 0 parents evolution is conceptually replaced by random chance. The heritability, defined as a requirement for evolution by Lewontin (1970), is eliminated from the process.\nWhile this had a much smaller impact on this relatively simple experiment of generated data, the implications on a much more complex classification problem, such as MNIST, could be significant.\n\n\nMNIST Experiment:\nA similar, more complex experiment performed on a subset 1000 images from the MNIST dataset tested the accuracy, loss, and diversity of num_parents = 0, 1, 2, 5, 10 over 1000 iterations. As a significantly more complex classification problem, the strengths of including more parents become much clearer.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on a Subset of the MNIST dataset using 0, 1, 2, 5, and 10 parents over 1,000 iterations\n\nThe benefits of inheritance are clear, as the zero parent model has a significantly higher loss and lower accuracy throughout the 1000 iterations compared to all other models.\nStarting with loss, we can see that the loss for all test groups are relatively similar with the exception of num_parents = 0.\nWith regards to accuracy, we see a more nuanced picture. 0 parents performs poorly throughout the experiment, never reaching 21% accuracy. 1 parent has logarithmic like improvement in accuracy at around 35%. All higher quantities of parents follow a similar trajectory, but with major jumps in accuracy breaking the logarithmic like trend. This can be better understood by looking at diversity levels.\nDue to the random nature of the weight vectors for the 0-parent group, the diversity is constant and extremely high at 65.124. For all other groups, it is clearly shown that more parents results in maintained diversity. As selection occurs, and the population is replenished with weight vectors inherited from the gene pool, diversity decreases overall. But by allowing for more varied combinations from that gene pool, some level of diversity is preserved. This appears to have diminishing benefits as demonstrated by the similar diversity for both 5 and 10 parents.\nThis becomes a problem of optimizing the heritability of fitness and the phenotypic variation mentioned by Lewontin (1970). fewer parents means more pure inheritance of fitness, as the child will more closely resemble its parents. It also means less phenotypic diversity, as completely new weight vectors are less likely to emerge. The opposite with regards to both phenotypic variation and fitness heritability applies. The benefits of multi-parent inheritance are demonstrated by the declining improvement in accuracy when num_parents = 1. The lower diversity compared to the other models, and the importance of diversity in evolutionary algorithms in allowing for the exploration of the solution space, results in poorer performance of the single parent model. A single parent allows for the inheritance of fitness,leading to better performance compared to the 0 parent model Lewontin (1970). However, it does not allow for large enough variation in fitness. With lower diversity, the 1 parent model is less likely to find a global minimum compared to the 2+ parent models. While it does find some form of a local minimum, the lack of diversity results in a drop off in improvement at around 600 iterations, while the models with 2, 5, and 10 parents continue to have spikes in improvement.\nIn the context of classification of the MNIST dataset, evolutionary models benefit from the added diversity resulting from the use of larger quantities of parents contributing weights to each new child in the subsequent generation. While more computing power, and more iterations are required to truly optimize this hyperparameter, these experiments clearly demonstrate the benefits of multi-parent inheritance.\n\n\n\nQuantifying Diversity and Diversity-Based Loss:\nThis section evaluates the effect of diversity-aware loss functions in evolutionary training of a neural network classifier on a subset of the MNIST handwritten dataset. We experimented with four diversity metrics - Euclidean Distance, Cosine Dissimilarity, Standard Deviation (STD) and variance - and measured their influence on test accuracy, cross-entropy loss, and diversity levels in our weight population over 200 generations. Additional hyperparameter tuning was performed for the Cosine diversity metric to explore how mutation rate, mutation intensity, population size, and diversity coefficient influence outcomes.\nThe first experiment (figure 1) compared the test accuracy, loss, and normalized diversity across all four diversity metrics under a fixed training setup. All metrics enabled the model to reach between 75%-81% accuracy over 200 generations, with all other hyperparameters held constant. euclidean distance and STD slightly outperformed others in final diversity. All methods reduced loss substantially within 60 generations. When it came to Normalized diversity, all metrics except for, interestingly, cosine dissimilarity between weight vectors increased/maintained high diversity over time. Cosine dissimilarity diversity rapidly decayed to near-zero within 100 generations, while STD, variance and euclidean distance maintained high diversity levels, suggesting that cosine may be more prone to premature convergence or intrinsically mediates the impact of diverse weight populations.\n\nFigure 1: Comparing test accuracy, loss, and normalized diversity values for all 4 diversity metrics.\nTo better understand the behavior of the cosine dissimilarity metric, we ran additional training with varied diversity coefficients, population sizes, and mutation hyperparameters. The default hyperparameters used were population size 50, mutation rate 0.4, mutation intensity 0.5 and diversity coefficient 0.1. Increasing the diversity coefficient to 0.3 (figure 4) significantly improved diversity values - up to 0.2 - over each generation, confirming that the penalty term has a regulating effect on population diversity. When the diversity coefficient was set to 0.0 (figure 3), the model still trained to reasonable accuracy but showed completely flat diversity values, indicating the diversity term is implemented correctly to at least affect our metric value. Increasing population size to 100 (figure 5) improved diversity over each generation, especially in the first 100 generations, but did not substantially improve test accuracy. This suggests diminishing returns from larger populations in this setting. Raising mutation rate to 0.7 and intensity to 0.8 (figure 6) had a negligible to slightly positive impact on accuracy while maintaining diversity at moderate levels. Accuracy did experience more noisiness under these conditions, but ultimately achieved reasonable levels.\n\nFigure 2: Baseline experiment outputs to provide reference test accuracy, loss, and diversity values for cosine driven loss.\n\nFigure 3: Confirming working implementation of diversity coefficient’s effect on diversity based loss by setting diversity coefficient to 0.0\n\nFigure 4: Results showing the effects of increased diversity coefficient of 0.3 - i.e. higher effect of diversity punishment/reward on loss - on test accuracy, loss, and diversity values.\n\nFigure 5: Results for increased population size of 100 weight vectors on test accuracy, loss, and diversity values.\n\nFigure 6: Results for impact of high mutation rate and mutation intensity on test accuracy, loss, and diversity values.\nIn summary, all four diversity metrics led to successful convergence and comparable final test accuracies, with euclidean distance and STD slightly ahead. Cosine dissimilarity driven diversity tends to descend quickly, requiring further parameter tuning to explore what it takes to keep diversity high. Enabling the diversity penalty to loss had a clear and measurable effect on both training behavior and final diversity levels, validating its implementation. Mutation and population hyperparameters affected convergence stability and final accuracy but had less influence than the choice of diversity metric.\nThis study was constrained by computational limitations, which restricted the breadth of hyperparameter combinations we could explore. In particular, both the population size and the number of generations were limited in order to keep training time feasible. Larger populations and longer training schedules could potentially yield more robust insights into the effects of diversity-aware loss function. Further investigation into the behavior of cosine dissimilarity is warranted. Across multiple experiments we observed a consistent decline in diversity when using this metric. One possible explanation for this is that cosine dissimilarity only measures angular differences between vectors, ignoring their magnitudes. As a result, the population may converge to a set of similarly oriented but differently scaled vectors, which could be interpreted as low diversity by this metric. This limitation could implicitly constrain the optimizer’s ability to maintain variation during training, and future work could test this hypothesis more directly or explore hybrid metrics that include both angular and magnitude components. Additionally, we were limited in the size of training and test batches, which may influence generalization performance. It would be valuable to evaluate how increasing batch size or dataset subset size impact both diversity value and resulting model accuracy. Please note, all of these experiementes were run on a hybridized version of the evolution optimized DNNs which included, for every step of training one gradient descent step on each weight vector. This was done in hopes to reduce runtimes without straying too far from pure evolution. Pure evolution, we speculated, would have needed to require high data inputs, generation numbers, and population sizes to produce valuable results, which did not fit the computational capacities of our computers, nor our time constraints.\n\n\nFinal MNIST Results:\nAfter combining all of our implementations together, we trained a deep neural network with layers [764,32,10] to classify our MNIST dataset. We settled on the following hyperparameters:\nmodel.diversity_coeff = 0.2\noptimizer = EvolutionOptimizer(model)\noptimizer.set_population_size(200)\noptimizer.use_backprop = False\noptimizer.set_survivors_ratio(0.1)\noptimizer.set_fitness_ratio(0.5)\noptimizer.set_sneaker_prob(0)\noptimizer.set_mutation_intensity(0.05)\noptimizer.mutation_rate = 0.05\n\nThe plot above shows that the accuracy rapidly increases during the early generations, indicating that the evolutionary algorithm quickly identifies promising weight vectors, significantly reducing the initial error. This is likely because the selection criteria is too strict so it rapidly eliminates poorly performing members of the population. The curve starts to smooth out, reflecting a deceleration in accuracy improvement as the optimizer converges on better solutions. After around 2000 generations, the accuracy curve stabilizes, indicating that the optimizer has reached a near-optimal solution for the given problem. The final accuracy appears to stabilize around 82%, suggesting that the current hyperparameter settings and genetic operators are effective but may have room for further optimization, possibly through adjustments to mutation rates, diversity coefficients, or parent selection mechanisms.\n\n\n\nConcluding Discussion:\nAs we combined all of our implementations of various evolutionary components, and created our unified optimizer that has significant flexibility in deciding the environment where our weights can evolve to solve the ML problems over time, we were ready to test this optimizer at a famous problem for deep neural networks: Classifying handwritten digits. Tweaking many of our hyperparameters lead to significantly different converging speeds, diversity metrics, and overall performance. This flexibility can be helpful in tailoring our algorithm to work in different context and on different models.\nOur intention from the beginning was to simulate how living creatures solve the problem of survival: Evolution. What encouraged us to explore this algorithm is the beauty of how living beings have evolved to solve the same problems very differently. This diversity that exist in nature is what got us thinking about ways we could achieve this concept in optimizing machine learning models on different datasets. If we can create a population of weights that can reproduce over time, and spread their genes and cross it with one another, what can we notice about the diversity of their solutions? This took us on a journey of simulating this natural process, abstracting it into simpler components, and specifying it to our context.\nOur project worked in many ways: our EVO optimizer managed to get the population to converge through random mutation, and maintain diversity by adjusting the diversity coefficient hyperparameter. We did however see a natural decay in diversity as the exploration phase ends and the exploitation phase begins where the population begin to converge around good solutions it found in the initial phase. Beyond the scope of the optimizer, our project worked in a sense that it provided us with the opportunity to investigate, design, and implement a complex environment for evolution. This has been the project that taught me at least the most about object oriented programming, and has taught us a lot about how to write legible code that will be built on by others.\nOur results are comparable to other evolutionary optimization algorithms in terms of convergence speed and diversity emphasis, however, different implementations have allowed for even more complex design decisions of the environment, more complex selection criteria (like tournament style), adaptive mutation rate, limitations on the mating process. These added complexity unlocks many different combinations of hyperparameters that outperform our simple-er implementation.\nIf we had more time, we would definitely work on improving speed. Currently, our code does not fully utilize GPU. There are a few python for-loops when popping our populations based on total loss and cross entropy loss. These are operations that, when vectorized, could speed up the training process significantly. In addition, we can add more design options for more complex evolutionary algorithms. Also, we would perform a grid search to find the best hyperparameters that would optimize for a deep neural network for handwritten MNIST dataset in terms of accuracy and diversity. Finally, we would implement an inference mechanism that would classify data using majority voting, assuming that the diversity in the population allows for a broader knowledge base to solve the problem, i.e, it would be interesting to see if the phenomenon of the wisdom of the crowd emerges under our current evolutionary algorithm within a population.\n\n\nGroup Contributions:\n\nLukka:\nAs a unit, the whole team contributed to the conceptualization and the early stages of building a working prototype. Lukka worked mainly on implementing, refining, and exploring the selection mechanisms in our evolutionary model. he also helped integrate the Laplacian mutation distribution. Lukka also helped include and streamline my work and the work of others into a central working file. This was work that helped build the base of how we would handle our object-oriented programming approach and handle tuning hyperparameters. He also spent considerable effort and time getting MNIST to run correctly on the Middlebury cluster to facilitate larger-scale experimentation. In all the team meetings, we all spent time digging into one another’s code, learning and helping on implementation, debugging, and developing conceptual frameworks.\n\n\nJiffy:\nFor this project, Jiffy contributed to both the conceptual development and the technical implementation of our evolutionary optimization framework. Early in the project, je created a demo notebook (evolution_demo_warmup.ipynb) that introduced the basic principles of evolutionary algorithms using synthetic data, aiming to outline a clear conceptual framework of evolution’s purpose and potential in our project. Jiffy was primarily responsible for implementing and optimizing the diversity-aware loss framework, including vectorized versions of the Euclidean distance and cosine dissimilarity metrics, as well as additional metrics based on standard deviation and variance. He added support for toggling these metrics and integrating them into the final loss calculation. Jiffy also extended our codebase to support multiclass classification, enabling us to apply our models to the MNIST dataset. Much of Jiffy’s experimentation involved running classification trials with varying diversity metrics and hyperparameters - mutation rate, intensity, and diversity coefficient - which he documented in a jupyter notebook (ExploringDiversity.ipynb). Jiffy wrote an initial training logic and data loading code for MNIST, and developed visualization tools using matplotlib to track accuracy, loss, and diversity across generations. He also implemented the hybrid optimization step, which combines evolution with gradient descent via backpropagation. For the final blog post, Jiffy focused on writing detailed technical explanations of the algorithmic components I implemented, along with reporting and analyzing the results of my experiments. This includes the materials/methods section on hybridizing evolution with gradient descent, computing diversity and how diversity-based loss was implemented, and the transition from binary to multiclass classification. It also includes the results writeup for ‘Quantifying Diversity and Diversity-based Loss’.\n\n\nJames:\nJames’ main contribution to this project was the creation, implementation, and experimentation on the benefits and limitations of adjusting the inheritance process, in terms of how many parents each child’s weight vector has. This included identifying scholarly sources which provided a framework for understanding how each change to our hyperparameters, and in the case of inheritance specifically, in the number of parents influences the forces of evolution Lewontin (1970). The majority his work is found in the multi-parent and multi-parent2 folders, although the important changes to the evo class were eventually merged with the blog-post EVO class. While the most important contributions can be found in /multi-parent2/MultiParent.ipynb, James spent considerable time working to overcome computational limitations of my computer and then working to have my code run on ADA. James wrote the abstract, the introduction, the values statement, and the Inheritance and Parent Quantity subsections of the Hyperparameter and results sections.\n\n\nYahya:\nFor this project, I contributed to both the conceptual foundation and technical implementation of our evolutionary optimization framework. Early in the project, I helped conceptualize and explain various mathematical approaches to designing the optimizer, providing the initial direction for our implementations. I also supplied resources to support my teammates in understanding the theoretical aspects of evolutionary algorithms. I implemented a proof-of-concept version of the optimizer, integrating basic mutation, crossover, and selection mechanisms, which served as the foundation for our more complex final implementation. This included designing and implementing the core structure of the optimizer, which became the base for further development. I also investigated different diversity metrics and incorporated them as terms in the loss function to maintain diverse populations and reduce the risk of premature convergence. I designed and refined the OOP structure throughout the project. Additionally, I designed the mutation mechanism and introduced a Laplacian distribution as an alternative to Gaussian mutation, allowing for a more varied exploration of the solution space. To address computational bottlenecks, I collaborated with Professor Vaccari to set up scripts for running Jupyter servers on Middlebury’s GPU nodes, resolving runtime issues for the team. Finally, I helped assemble the final version of the EVO optimizer and experimented with different hyperparameter combinations to fine-tune the model and achieve the results presented in our report.\n\n\n\n\n\n\n\n\n\nPersonal Reflection:\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010. “MNIST Handwritten Digit Database.” ATT Labs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2.\n\n\nLewontin, R. C. 1970. “The Units of Selection.” Annual Review of Ecology and Systematics 1: 1–18. http://www.jstor.org/stable/2096764.\n\n\nTelikani, Akbar, Amirhessam Tahmassebi, Wolfgang Banzhaf, and Amir H. Gandomi. 2021. “Evolutionary Machine Learning: A Survey.” ACM Comput. Surv. 54 (8). https://doi.org/10.1145/3467477."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html",
    "href": "posts/understanding-feature-separability/index.html",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#abstract",
    "href": "posts/understanding-feature-separability/index.html#abstract",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.984375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/understanding-feature-separability/index.html#discussion",
    "href": "posts/understanding-feature-separability/index.html#discussion",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "Discussion",
    "text": "Discussion\nThrough this analysis, I gained insights into the importance of feature selection in classification tasks. My IoU-based approach helped quantify feature separability, leading to better-informed choices. Visualization techniques, such as pair plots and decision boundaries, provided valuable interpretability into the model’s behavior. The performance comparison between Logistic Regression, Decision Trees, and Random Forest demonstrated that Decision Trees models generalize best, minimizing overfitting while maintaining high accuracy. One key takeaway is that some species are inherently harder to classify due to overlapping feature distributions, reinforcing the need for careful feature engineering and model selection. Future improvements could involve non-linear models like SVMs and better feature selection to further enhance classification accuracy. This project shows how data-driven feature selection and model evaluation can lead to meaningful and accurate species classification."
  },
  {
    "objectID": "posts/Auditing-bias/index.html",
    "href": "posts/Auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, BasicProblem\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression"
  },
  {
    "objectID": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "href": "posts/Auditing-bias/index.html#patterns-of-disparity",
    "title": "Auditing Bias",
    "section": "patterns of disparity",
    "text": "patterns of disparity\nTo explore deeper patterns of disparity, we now perform an intersectional analysis by examining the proportion of individuals earning over $50K across combined gender and race subgroups. We add the RAC1P column (race) from the original ACS dataset to our DataFrame and map it to readable race labels using race_map.\nWe then create a new column that combines gender and race into a single intersectional group (e.g., “M & White”, “F & Black”). By grouping on this combined variable and computing the mean of the label, we obtain the proportion of high-income individuals within each intersectional group.\nFinally, we visualize these proportions with a bar chart to better observe the disparities across different gender-race combinations. This helps highlight how overlapping identities can influence economic outcomes and inform fairness analysis in the model.\n\ndf_intersection = df.copy()\ndf_intersection[\"RAC1P\"] = acs_data.loc[df.index, \"RAC1P\"]\n\n# Map RAC1P to human-readable race labels.\ndf_intersection[\"Race\"] = df_intersection[\"RAC1P\"].map(race_map)\n\n# Create an intersectional grouping variable using both Gender and Race.\ndf_intersection[\"Gender_Race\"] = df_intersection[\"Gender\"] + \" & \" + df_intersection[\"Race\"]\n\n# Compute the proportion of positive labels for each intersectional group.\nintersectional_stats = df_intersection.groupby(\"Gender_Race\")[\"label\"].mean()\nprint(\"\\nProportion of positive labels by Gender & Race group:\")\nprint(intersectional_stats)\n\n# Visualize the intersectional trends with a bar chart.\nplt.figure(figsize=(12, 6))\nsns.barplot(x=intersectional_stats.index, y=intersectional_stats.values)\nplt.xlabel(\"Gender & Race Group\")\nplt.ylabel(\"Proportion with label == 1\")\nplt.title(\"Intersectional Proportions by Gender and Race\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nProportion of positive labels by Gender & Race group:\nGender_Race\nF & Black                               0.143632\nF & Native                              0.137795\nF & Native Hawaiian/Pacific Islander    0.150943\nF & Other                               0.125604\nF & Two or More                         0.000000\nF & White                               0.136007\nM & Black                               0.256418\nM & Native                              0.240816\nM & Native Hawaiian/Pacific Islander    0.204545\nM & Other                               0.287402\nM & Two or More                         0.250000\nM & White                               0.263107\nName: label, dtype: float64\n\n\n\n\n\n\n\n\n\nThe results of the intersectional analysis show consistent disparities in high-income proportions across gender and race combinations. For females, the proportion of individuals earning over $50K ranges from 12.56% to 15.09% across most racial groups, with White females at 13.60% and Black females at 14.36%. Notably, females identifying as Two or More races have a value of 0%, likely due to a very small sample size.\nFor males, the rates are significantly higher across the board. For example, White males are at 26.31%, Black males at 25.64%, and Other males at 28.74%. These findings reinforce the earlier observed gender gap and also highlight how racial identity further compounds disparities. This intersectional breakdown is crucial for understanding how multiple identity factors can interact to affect economic outcomes.\nThis is why choosing gender might be a good thing to explore bias through in this blog post.\nHere we train and tune a RandomForestClassifier by performing a hyperparameter search over the max_depth parameter. We loop through a range of depths in steps of 2 and use 5-fold cross-validation to evaluate model performance at each depth.\nFor each candidate max_depth, we compute the average cross-validation accuracy and track it in a dictionary. To avoid unnecessary computation, we stop the search early if the performance begins to drop.\nAt the end, we identify the best-performing max_depth based on the highest mean cross-validation accuracy. This tuned depth will be used to train our final model.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# Define the candidate max_depth values.\nstep, max_val = 2, 20\n\nmax_depth_values = [int(i * step) for i in range(1, max_val + 1)]\nresults = {}\n\nfor depth in max_depth_values:\n    model = RandomForestClassifier(max_depth=depth, random_state=42)\n    \n    # Perform 5-fold cross-validation on the training data.\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    mean_score = np.mean(cv_scores)\n    results[depth] = mean_score\n    \n    print(f\"Max Depth: {depth} - Mean CV Accuracy: {mean_score:.3f}\")\n    scores = list(results.values())\n    \n    #we're finding the minimum number of max_depth values to test\n    if len(scores) &gt; 2 and scores[-1] &lt; scores[-2]:\n        break\n\n\nbest_depth = max(results, key=results.get)\nprint(\"\\nBest max_depth:\", best_depth, \"with a Mean CV Accuracy of:\", results[best_depth])\n\nMax Depth: 2 - Mean CV Accuracy: 0.801\nMax Depth: 4 - Mean CV Accuracy: 0.842\nMax Depth: 6 - Mean CV Accuracy: 0.843\nMax Depth: 8 - Mean CV Accuracy: 0.843\nMax Depth: 10 - Mean CV Accuracy: 0.844\nMax Depth: 12 - Mean CV Accuracy: 0.845\nMax Depth: 14 - Mean CV Accuracy: 0.844\n\nBest max_depth: 12 with a Mean CV Accuracy of: 0.8447350223172189\n\n\nUsing the optimal max_depth found from cross-validation, we now train a final RandomForestClassifier on the full training data. After fitting the model, we evaluate its performance on the test set.\nWe calculate several key performance metrics: - Accuracy: the proportion of correct predictions. - Positive Predictive Value (PPV): the precision, or the proportion of predicted positives that are actually positive. - False Negative Rate (FNR): the proportion of actual positives that were misclassified as negatives. - False Positive Rate (FPR): the proportion of actual negatives that were misclassified as positives.\nThese metrics provide a comprehensive view of the model’s overall performance.\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n# start and fit a new model using the best max_depth from tuning\nbest_model = RandomForestClassifier(max_depth=best_depth, random_state=42)\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_hat = best_model.predict(X_test)\n\n# Calculate overall accuracy\naccuracy = accuracy_score(y_test, y_hat)\n\n# Compute confusion matrix: returns [[TN, FP], [FN, TP]]\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n\n# Calculate positive predictive value (PPV) i.e. precision\nppv = precision_score(y_test, y_hat)\n\n# Calculate false negative rate (FNR): FN / (FN + TP) while avoiding dividing by zero\nfnr = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n\n# Calculate false positive rate (FPR): FP / (FP + TN)\nfpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\n\nprint(\"Overall Accuracy:\", accuracy)\nprint(\"Positive Predictive Value (PPV):\", ppv)\nprint(\"False Negative Rate (FNR):\", fnr)\nprint(\"False Positive Rate (FPR):\", fpr)\n\nOverall Accuracy: 0.8486220076443372\nPositive Predictive Value (PPV): 0.6697517879680269\nFalse Negative Rate (FNR): 0.5829185223997904\nFalse Positive Rate (FPR): 0.04885790751229228\n\n\nThe model achieves an overall accuracy of 84.86%, indicating strong performance in correctly classifying income levels. The positive predictive value (PPV) is 66.98%, meaning that when the model predicts someone earns over $50K, it’s correct about two-thirds of the time.\nHowever, the false negative rate (FNR) is relatively high at 58.29%, suggesting that the model frequently fails to identify individuals who do earn more than $50K. The false positive rate (FPR) is much lower at 4.89%, meaning the model rarely misclassifies low-income individuals as high earners.\nThis trade-off indicates that while the model is cautious about predicting high income (low FPR), it may be overly conservative, leading to many missed positives (high FNR)."
  },
  {
    "objectID": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "href": "posts/Auditing-bias/index.html#model-fairness-across-gender-groups",
    "title": "Auditing Bias",
    "section": "Model Fairness across gender groups",
    "text": "Model Fairness across gender groups\nTo evaluate the model’s fairness across gender groups, we compute key performance metrics separately for males and females. For each subgroup, we calculate:\n\nAccuracy: overall correctness within the group.\nPPV (Precision): how often predicted high-income individuals are actually high-income.\nFNR (False Negative Rate): how often actual high-income individuals are missed.\nFPR (False Positive Rate): how often low-income individuals are incorrectly classified as high-income.\n\nThese metrics are stored in a dictionary and then converted into a DataFrame for easy viewing. We also extract the FNR, FPR, and PPV for each group (male = 1, female = 2) for use in later fairness visualizations and audits.\n\n\n# Dictionary to hold metrics keyed by group\nmetrics_dict = {}\n\n# Get the unique groups from your test set\nunique_groups = np.unique(group_test)\n\n# Loop over each subgroup in group_test\nfor grp in unique_groups:\n    # Create a mask for the current group\n    mask = (group_test == grp)\n    \n    # Subset the true labels and predictions for this group\n    y_true_grp = y_test[mask]\n    y_pred_grp = y_hat[mask]\n    \n    # Calculate accuracy for the subgroup\n    grp_accuracy = accuracy_score(y_true_grp, y_pred_grp)\n    \n    # Calculate the confusion matrix: [[TN, FP], [FN, TP]]\n    tn, fp, fn, tp = confusion_matrix(y_true_grp, y_pred_grp).ravel()\n    \n    # Calculate PPV (precision) for the subgroup\n    grp_ppv = precision_score(y_true_grp, y_pred_grp)\n    \n    # Calculate False Negative Rate (FNR): FN / (FN + TP)\n    grp_fnr = fn / (fn + tp)\n    \n    # Calculate False Positive Rate (FPR): FP / (FP + tn)\n    grp_fpr = fp / (fp + tn)\n    \n    # Store the results in a dictionary keyed by group label (e.g., 1 or 2)\n    metrics_dict[grp] = {\n        \"Accuracy\": grp_accuracy,\n        \"PPV\": grp_ppv,\n        \"FNR\": grp_fnr,\n        \"FPR\": grp_fpr\n    }\n\n# Convert the dictionary to a DataFrame for display\ndf_group_metrics = pd.DataFrame.from_dict(metrics_dict, orient=\"index\")\ndf_group_metrics.index.name = \"Group\"\ndf_group_metrics.reset_index(inplace=True)\n\nprint(df_group_metrics)\n\n# Optionally, you can directly pull out metrics for each group:\nfnr_m = metrics_dict[1][\"FNR\"] if 1 in metrics_dict else None\nfpr_m = metrics_dict[1][\"FPR\"] if 1 in metrics_dict else None\nppv_m = metrics_dict[1][\"PPV\"] if 1 in metrics_dict else None\n\nfnr_f = metrics_dict[2][\"FNR\"] if 2 in metrics_dict else None\nfpr_f = metrics_dict[2][\"FPR\"] if 2 in metrics_dict else None\nppv_f = metrics_dict[2][\"PPV\"] if 2 in metrics_dict else None\n\nprint(\"\\nMale FNR:\", fnr_m, \" FPR:\", fpr_m, \" PPV:\", ppv_m)\nprint(\"Female FNR:\", fnr_f, \" FPR:\", fpr_f, \" PPV:\", ppv_f)\n\n   Group  Accuracy       PPV       FNR       FPR\n0      1  0.817064  0.774603  0.608504  0.038629\n1      2  0.879570  0.551477  0.534743  0.057487\n\nMale FNR: 0.6085038106698757  FPR: 0.03862894450489663  PPV: 0.7746031746031746\nFemale FNR: 0.5347432024169184  FPR: 0.05748709122203098  PPV: 0.5514771709937332\n\n\nThe subgroup performance metrics reveal meaningful disparities between males and females:\n\nAccuracy is higher for females (87.96%) than for males (81.71%).\nPPV (Precision) is significantly higher for males (77.46%) than for females (55.15%), indicating that when the model predicts high income, it is more correct for males.\nFNR (False Negative Rate) is worse for males (60.85%) than for females (53.47%), meaning the model misses more high-income males.\nFPR (False Positive Rate) is slightly better for males (3.86%) compared to females (5.75%), suggesting the model more often incorrectly labels females as high-income.\n\nThese results suggest a gender-based imbalance in prediction quality especially in precision which may reflect or amplify real-world income disparities and requires careful consideration when interpreting model fairness.\nHere we calculate the prevalence of high income (i.e. the proportion of individuals earning over $50K) separately for males and females in the test set. This is done by taking the mean of the binary target (y_test) within each gender group. These values will be used in the fairness analysis to determine feasible combinations of false positive and false negative rates under fixed PPV, as described in Chouldechova (2017).\n\np_m = (y_test[group_test == 1]).mean()  # Prevalence for Males\np_f = (y_test[group_test == 2]).mean()  # Prevalence for Females\np_m, p_f\n\n(0.2532249873031996, 0.13188564598067537)\n\n\nwe fix the positive predictive value (PPV) across groups by setting it to the lower of the two observed PPVs. This ensures a consistent standard of predictive parity when plotting feasible combinations of false negative and false positive rates. The common_ppv will be used to generate the fairness trade-off lines for each group.\n\ncommon_ppv = min(ppv_m, ppv_f)\nprint(\"Using common PPV =\", common_ppv)\n\nUsing common PPV = 0.5514771709937332\n\n\nTo visualize the fairness trade-offs described in Chouldechova (2017), we plot feasible combinations of false negative rate (FNR) and false positive rate (FPR) for each gender group under a fixed PPV (set to the lower of the two observed PPVs).\nWe define a function based on Equation (2.6) from the paper to compute FPR as a function of FNR, prevalence, and PPV. Using this, we generate lines for males and females by sweeping FNR values from 0 to 1.\nWe then plot: - The feasible FNR–FPR line for each group. - The observed FNR and FPR as points on the plot.\nThis visualization illustrates the trade-off between FNR and FPR under predictive parity constraints. For example, to equalize FPR between groups, one might need to significantly increase the FNR in one group, which highlights the inherent tension between different fairness criteria.\n\ndef feasible_fpr(fnr_array, p, ppv):\n    \"\"\"\n    Given an array of FNR values in [0, 1],\n    returns the corresponding FPR values from Chouldechova (2017), eq. (2.6).\n    FPR = [p * (1 - FNR) * (1 - PPV)] / [1 - p]\n    \"\"\"\n    return (p * (1 - fnr_array) * (1 - ppv)) / (1 - p)\n\n# We'll sweep FNR from 0 to 1 for plotting\nfnr_grid = np.linspace(0, 1, 200)\n\n# Compute the feasible lines for each group, \n# using the *common* PPV for both\nfpr_line_m = feasible_fpr(fnr_grid, p_m, common_ppv)\nfpr_line_f = feasible_fpr(fnr_grid, p_f, common_ppv)\n\n# Plot the feasible lines\nplt.figure(figsize=(7,5))\n\nplt.plot(fnr_grid, fpr_line_m, label=\"Feasible line (Males)\", color=\"blue\")\nplt.plot(fnr_grid, fpr_line_f, label=\"Feasible line (Females)\", color=\"orange\")\n\n# Plot the observed points for each group\nplt.scatter(fnr_m, fpr_m, color=\"blue\", marker=\"o\", s=80, \n            label=\"Observed (Males)\")\nplt.scatter(fnr_f, fpr_f, color=\"orange\", marker=\"o\", s=80, \n            label=\"Observed (Females)\")\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) Combinations under Fixed PPV = {:.3f}\".format(common_ppv))\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe observed FPR for females is higher than for males, which suggests that the model is more likely to incorrectly classify low-income females as high-income.\nThe FNR for males is higher than for females, meaning the model is missing more actual high-income males.\nIf we wanted to equalize FPR between groups, we would have to increase the FNR for males, moving it further along the blue line. This trade-off shows the conflict or tension between different fairness goals.\n\nThis visualization illustrates that achieving equalized error rates across groups requires making trade-offs that may disproportionately impact different subgroups.\n\n\nConcluding Discussion\nThe ability to predict income levels has several potential applications in both commercial and governmental settings. Companies in finance, such as banks and credit card providers, could use this model to assess credit-worthiness, loan eligibility, or target specific financial products. Marketing agencies might use similar predictions to segment consumers for advertising high-end products or services. Government agencies could employ such models for economic policy analysis, workforce development, or social welfare program distribution.\nHowever, deploying this model at a large scale carries significant risks, particularly concerning fairness and bias. Our bias audit revealed disparities in predictive performance across gender groups. Notably, the model has a higher false positive rate (FPR) for females, meaning it more often misclassifies lower-income women as high-income. Conversely, it has a higher false negative rate (FNR) for males, meaning it more frequently fails to recognize high-income males. If deployed in real-world scenarios such as hiring or loan approvals, this could systematically disadvantage certain groups, reinforcing existing economic inequalities.\nExamining different types of bias, our model does not satisfy error rate balance, as FNR and FPR differ between genders. Additionally, the calibration of the model is problematic—males have a higher precision (PPV) than females, meaning the model is more confident in its positive predictions for men than for women. This suggests potential bias in how income is modeled, reflecting either societal disparities or weaknesses in the dataset itself.\nBeyond bias, there are other concerns with deploying such a model. One key issue is data representativeness—the ACS dataset might not fully capture income distributions across different racial or socioeconomic groups. Additionally, there’s a risk of automation bias, where decision-makers might overly rely on model predictions without questioning their validity. Finally, privacy concerns arise when using sensitive demographic data for predictions, as such models could be exploited for discriminatory profiling.\nTo mitigate these issues, several steps could be taken: - Fairness constraints: Adjusting the decision threshold per group to balance FPR and FNR. - Re-weighting techniques: Ensuring training data better reflects underrepresented groups. - Explainability measures: Making the model’s predictions more interpretable to reduce blind reliance. - Human oversight: Keeping final decision-making in human hands rather than automating high-stakes outcomes.\nfinally, while predictive models can be powerful tools, deploying them responsibly requires continuous auditing, transparency, and fairness interventions to prevent unintended harm."
  },
  {
    "objectID": "posts/Exploring-Perceptrom/index.html",
    "href": "posts/Exploring-Perceptrom/index.html",
    "title": "Exploring and Implementing the Perceptron",
    "section": "",
    "text": "Abstract\nThis blog post explores the implementation and behavior of the perceptron algorithm, a foundational model in machine learning for binary classification. I begin by building the perceptron from scratch using PyTorch, including custom classes for the model, loss computation, and optimization steps. Through a series of visual experiments, I demonstrate that the perceptron converges when the data is linearly separable, and fails to do so otherwise. I also extend the analysis to higher-dimensional data, using PCA to visualize the decision boundary. This post highlights both the capabilities and limitations of the perceptron, setting the stage for more advanced linear models.\nYou can find my implementation of the perceptron at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nData Generation and Visualization\nFirst, let’s generate and visualize 2D synthetic data for testing the perceptron algorithm. The perceptron_data() function creates a binary classification dataset with Gaussian noise and appends a bias term to each data point. The plot_perceptron_data() function displays the two classes using distinct markers and colors.\nFunctions provided by Professor Phil Chodrow as part of CSCI 0451 course materials.\nThis setup provides a visual way to assess whether the perceptron correctly learns to separate the two classes in later experiments.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data(n_points =  250, noise = 0.2)\nplot_perceptron_data(X, y, ax)\n\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n\nTraining the Perceptron\nNow, let’s define and run the train_perceptron() function, which trains a perceptron model on the synthetic dataset using the Perceptron update rule. The function samples one random data point per iteration, checks if it’s misclassified, and updates the model if necessary. The training continues either until convergence (zero loss) or until the specified maximum number of iterations is reached.\nIf visualize=True, the function also displays a sequence of plots showing how the decision boundary evolves as the model learns. Dashed lines represent the previous boundary before an update, and solid lines show the updated one. The logic is also adapted from Professor Chodrow’s work.\n\ndef train_perceptron(X, y, max_iter=1_000_000, visualize=False, max_axes=6, k = 1 , alpha = 0.1):\n    \"\"\"\n    Train a perceptron on the given data.\n\n    Parameters:\n        X (torch.Tensor): Input features of shape (n_samples, 3)\n        y (torch.Tensor): Binary labels (0 or 1) of shape (n_samples,)\n        max_iter (int): Maximum number of iterations to run (-1 means run until convergence)\n        visualize (bool): If True, visualize decision boundary evolution\n        max_axes (int): Number of subplots to show if visualizing\n\n    Returns:\n        p (Perceptron): Trained perceptron model\n        opt (PerceptronOptimizer): Optimizer used for training\n        loss_vec (list): Loss values over training steps\n    \"\"\"\n\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    n = X.size(0)\n    loss_vec = []\n\n    # Trigger initialization of p.w by calling score once\n    _ = p.score(X)\n\n    loss = 1.0\n    iteration = 0\n    current_ax = 0\n\n    if visualize:\n        fig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)\n        plt.rcParams[\"figure.figsize\"] = (7, 5)\n        markers = [\"o\", \",\"]\n        marker_map = {-1: 0, 1: 1}\n\n    while loss &gt; 0 and (max_iter == -1 or iteration &lt; max_iter):\n        old_w = torch.clone(p.w)\n\n        # Sample one data point\n        i = torch.randint(n, size=(1,))\n        x_i = X[[i], :]\n        y_i = y[i]\n        local_loss = p.loss(x_i, y_i).item()\n\n        # If misclassified, update and optionally visualize\n        if local_loss &gt; 0:\n            opt.step(x_i, y_i, k=k, alpha=alpha)\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n\n            if visualize and current_ax &lt; max_axes:\n                ax = axarr.ravel()[current_ax]\n                plot_perceptron_data(X, y, ax)\n                draw_line(old_w, x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n                draw_line(p.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n                ax.scatter(\n                    X[i, 0],\n                    X[i, 1],\n                    color=\"black\",\n                    facecolors=\"none\",\n                    edgecolors=\"black\",\n                    marker=markers[marker_map[2 * y[i].item() - 1]],\n                )\n                ax.set_title(f\"loss = {loss:.3f}\")\n                ax.set(xlim=(-1, 2), ylim=(-1, 2))\n                current_ax += 1\n\n        iteration += 1\n\n    if visualize:\n        plt.tight_layout()\n        plt.show()\n\n    return p, opt, loss_vec\n\np, opt, loss_vec = train_perceptron(X, y, visualize=True, max_iter= 1000000)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n\nResults: Convergence of the Perceptron Algorithm\nThe plots above show the progression of the perceptron’s decision boundary during training on linearly separable 2D data. Each subplot corresponds to a misclassified point that triggered an update. The dashed line represents the previous decision boundary, while the solid line shows the updated one after a single perceptron step.\nAs training proceeds, the loss steadily decreases until it reaches zero, indicating perfect classification on the dataset. This confirms that the perceptron algorithm converges when the data is linearly separable, as expected from the theory.\nlet’s make sure that we indeed do get 0 loss at the end\n\nloss_vec[-1]\n\n0.0\n\n\nLet us now visualize the loss vector to see how it updates over time!\n\n# Plotting the evolution of loss during training\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec, linewidth=2)\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Perceptron Training Loss Over Time\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe loss indeed converges to zero with some spikes along the way!\n\n\nIntroducing a Misclassified Point\nnow, I will modify the dataset to make it linearly inseparable by manually injecting a single mislabeled data point. This new point at (0, 0.5) is assigned to class 1, but is positioned among points from class 0. As a result, no straight line can perfectly separate the two classes.\nThis visualization sets up the next experiment to test how the perceptron behaves when perfect classification is impossible.\n\n#injecting one data point to make the data linearly inseparable\n\nX_ , y_ = X, y\nX_ = torch.cat((X_, torch.tensor([[0, 0.4, 1.0]])), dim=0)\ny_ = torch.cat((y_, torch.tensor([True])), dim=0)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X_, y_, ax)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\nLet’s retrain on the linearly inseparable data and get it to run for 10000 iterations\n\np, opt, loss_vec = train_perceptron(X_, y_, visualize=True, max_iter= 10000)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\nloss_vec[-1]\n\n0.0039840638637542725\n\n\nThe final loss after training is approximately 0.0066, indicating that while the perceptron was able to correctly classify most data points, it could not achieve perfect accuracy due to the presence of the injected outlier. This aligns with theoretical expectations: the perceptron algorithm cannot converge to zero loss on linearly inseparable data.\n\n\nPerceptron in Higher Dimensions\nTo explore how the perceptron performs in higher dimensions, I generated a 5-dimensional dataset using the same perceptron_data() function. The model was then trained for a maximum of 10,000 iterations. The resulting loss plot shows that the perceptron consistently reduced the misclassification rate over time, suggesting that the data is likely linearly separable in this higher-dimensional space.\nSince visualizing 5D data directly is not possible, I used Principal Component Analysis (PCA) to project the data into 2D for visualization. Additionally, I approximated the perceptron’s decision boundary (a 4D hyperplane) by sampling points that satisfy the hyperplane equation and projecting them into the same 2D space using PCA.\nThe resulting plot shows a clear separation between the two classes in the projected space, with the projected hyperplane aligning well between them. This demonstrates that the perceptron is capable of finding meaningful separating boundaries even in higher-dimensional feature spaces.\n\nfrom sklearn.decomposition import PCA\n\n# Step 1: Generate data\nX_hd, y_hd = perceptron_data(n_points=300, noise=0.3, p_dims=5)\n\n# Step 2: Train perceptron\np_hd, opt_hd, loss_vec_hd = train_perceptron(X_hd, y_hd, max_iter=10000, visualize=False)\n\n# Step 3: Plot loss over time\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec_hd)\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Misclassification Rate\")\nplt.title(\"Loss Over Time (5D Perceptron Training)\")\nplt.grid(True)\nplt.show()\n\n# Step 4: PCA fit (exclude bias column)\nX_hd_np = X_hd[:, :-1].numpy()\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_hd_np)\n\n# Step 5: Project a sample of points on the decision hyperplane\n# We pick random 5D points x such that w·x + b ≈ 0\nw = p_hd.w[:-1]\nb = p_hd.w[-1]\n\n# Sample points along the hyperplane by solving w·x + b = 0\n# Fix all but one dimension and solve for the last\nx_vals = []\nfor i in range(-5, 6):  # varying x0 from -5 to 5\n    x_sample = torch.zeros(5)\n    x_sample[0] = i\n    # solve for x1 such that the hyperplane condition is met\n    # w0*x0 + w1*x1 + ... + w4*x4 + b = 0\n    # → x1 = -(b + w0*x0 + w2*x2 + ...)/w1\n    # we'll keep x2, x3, x4 = 0 for simplicity\n    if w[1] != 0:\n        x_sample[1] = -(b + w[0]*x_sample[0]) / w[1]\n        x_vals.append(x_sample.numpy())\n\n# Convert to numpy and apply PCA\nx_vals = np.stack(x_vals)\nx_vals_pca = pca.transform(x_vals)\n\n# Step 6: Plot PCA projection of data and separating line\nplt.figure(figsize=(6, 5))\nplt.scatter(X_pca[y_hd==0][:, 0], X_pca[y_hd==0][:, 1], label=\"Class 0\", alpha=0.5)\nplt.scatter(X_pca[y_hd==1][:, 0], X_pca[y_hd==1][:, 1], label=\"Class 1\", alpha=0.5)\nplt.plot(x_vals_pca[:, 0], x_vals_pca[:, 1], color='black', label=\"Projected Hyperplane\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.title(\"PCA Projection with Projected Hyperplane\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Perceptron on mini-batches\nIn my implementation of the perceptron, I have made recent modifications that allows me to calculate the gradient on batches of points and then averaging. In this section, I will experiment using multiple alpha values and k (the batch size)\nLet’s repeat the experiment. We will start with k = 1. this will theoretically get us something similar to the regular perceptron\n\n# when k = 1, the perceptron is equivalent to the standard perceptron. here is an example\nX, y = perceptron_data(n_points = 400, noise = 0.2)\nplot_perceptron_data(X, y, ax)\np, opt, loss_vec = train_perceptron(X, y, visualize=True, max_iter= 1000000, k = 1, alpha = 0.5)\nloss_vec[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0\n\n\nThis is the same results we got in the beginning. Now, let’s introduce higher number of k. starting with k = 10\n\n# now, let's try k = 10\np10, opt10, loss_vec10 = train_perceptron(X, y, visualize=True, max_iter= 1000000, k = 10, alpha = 0.5)\nloss_vec10[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0\n\n\n\n# now let's make the data linearly inseparable\nX_, y_ = X, y\nX_ = torch.cat((X_, torch.tensor([[0, 0.4, 1.0]])), dim=0)\ny_ = torch.cat((y_, torch.tensor([True])), dim=0)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X_, y_, ax)\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n\n# now let's try with k = size of the data to see if it can converge\np_max, opt_max, loss_vec_max = train_perceptron(X_, y_, visualize=True, max_iter= 100, k = X_.shape[0], alpha = 0.5)\nloss_vec_max[-1]\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_16978/3550169830.py:31: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n\n\n\n\n\n\n\n\n0.0024937656708061695\n\n\n\n# let's now plot all the loss curves together\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec, label=\"k=1\")\nplt.plot(loss_vec10, label=\"k=10\")\nplt.plot(loss_vec_max, label=\"k=400\")\nplt.xlabel(\"Update Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time for Different k Values\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe observe that the loss indeed converges when we average the gradient over all points in our dataset. We also notice that, when the learning rate is tuned, we can achieve perfect separability faster using the mini-batch approach\n\n\nConclusion\nIn this notebook, I implemented the perceptron algorithm from scratch and explored its behavior through a series of experiments. I confirmed that the perceptron converges on linearly separable data and fails to fully converge when the data is not perfectly separable. Visualizations of the decision boundary and loss trajectory provided insight into how the model evolves with each update. I also extended the analysis to higher-dimensional data and used PCA to visualize the model’s decision hyperplane, showing that the perceptron remains effective in separating classes beyond two dimensions. Finally, I experimented with the mini-batch algorithm of averaging the grad over a subset of points before making a step, which has the benefit of faster convergence, and convergence when data is not linearly separable. Overall, these experiments reinforce both the strengths and limitations of the perceptron algorithm, providing a solid foundation for understanding more advanced linear classifiers."
  },
  {
    "objectID": "posts/Double Descent/main.html",
    "href": "posts/Double Descent/main.html",
    "title": "Overparameterization and Double Descent",
    "section": "",
    "text": "In this project, we explore the phenomenon of double descent in overparameterized linear regression models. Using a custom feature mapping approach inspired by kernel methods, we transform 1-dimensional data into a higher-dimensional space using random feature projections. This overparameterization allows the model to capture complex, nonlinear patterns but also introduces the risk of overfitting when the number of features exceeds the number of data points. We observe that the test error initially decreases as the model gains capacity, spikes sharply around the interpolation threshold, and then decreases again as the model becomes significantly overparameterized. This counterintuitive behavior highlights the unique generalization properties of modern machine learning models and provides insights into why deep networks can perform well despite their extreme parameter counts. You can see my implementation of the closed-form linear regression at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Double Descent/main.html#abstract",
    "href": "posts/Double Descent/main.html#abstract",
    "title": "Overparameterization and Double Descent",
    "section": "",
    "text": "In this project, we explore the phenomenon of double descent in overparameterized linear regression models. Using a custom feature mapping approach inspired by kernel methods, we transform 1-dimensional data into a higher-dimensional space using random feature projections. This overparameterization allows the model to capture complex, nonlinear patterns but also introduces the risk of overfitting when the number of features exceeds the number of data points. We observe that the test error initially decreases as the model gains capacity, spikes sharply around the interpolation threshold, and then decreases again as the model becomes significantly overparameterized. This counterintuitive behavior highlights the unique generalization properties of modern machine learning models and provides insights into why deep networks can perform well despite their extreme parameter counts. You can see my implementation of the closed-form linear regression at this GitHub link.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Double Descent/main.html#setup",
    "href": "posts/Double Descent/main.html#setup",
    "title": "Overparameterization and Double Descent",
    "section": "Setup",
    "text": "Setup\nThis cell defines the RandomFeatures class for generating random nonlinear feature maps, which is adapted from Professor Chodrow’s notes. The RandomFeatures class constructs high-dimensional representations of input data using random weight vectors and bias terms, with customizable activation functions. This is a crucial step in implementing overparameterized linear regression, allowing the model to capture complex, nonlinear relationships in the data. The default activation function is the logistic sigmoid, but it can be replaced with other functions, such as the square function, for experimentation with different feature mappings.\n\nimport torch\nimport torch.nn as nn\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\nThe Breakdown of the Normal Equation When (p &gt; n)\nWhen optimizing a linear regression model to minimize the mean-squared error loss, we often use the closed-form solution for the optimal weights:\n\\[\n\\hat{w} = (X^\\top X)^{-1} X^\\top y\n\\]\nThe issue lies in the matrix \\(X\\), particularly its invertibility. When the number of features \\(p\\) exceeds the number of data points \\(n\\), the matrix \\(X\\) must have linearly dependent columns. As a result, the operation \\(X^\\top X\\) becomes undefined in the context of matrix inversion because \\(X^\\top X\\) is not invertible. Therefore, the normal equation cannot be used when \\(p &gt; n\\). We will solve this issue by introducing the pseudoinverse. The pseudoinverse, specifically the Moore-Penrose pseudoinverse, extends the concept of matrix inversion to singular or non-square matrices, providing a stable solution for least-squares problems when p &gt; n.\n\n\nTesting Linear Regression on Random Features\nTo test the linear regression I implemented in linear.py, I will first generate my data. my data points are sampled from the function \\(f(x) = x^4\\) with normal noise added to it\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\nNow, let’s fit our linear regression model on 150 random features generated from our 2D data. This setup intentionally overparameterizes the model, with the number of features p = 150 exceeding the number of data points n = 100. This allows us to observe the effects of overparameterization and potential overfitting.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1) Create a feature map (try p &gt; n to see overparameterization)\nn_features = 100\nphi = RandomFeatures(n_features=n_features, activation=square)\nphi.fit(X)                               \nX_feat = phi.transform(X)                \n\n# 2) Instantiate your regression model and optimizer\nmodel = MyLinearRegression()\nopt   = OverParameterizedLinearRegressionOptimizer(model)\n\n# 3) Fit closed‑form and predict\nopt.fit(X_feat, y.squeeze())             # y has shape (100, 1) so we squeeze to (100,)\ny_pred = model.predict(X_feat)           # continuous predictions\n\n# 4) Plot data vs. learned curve\nplt.scatter(X.numpy(), y.numpy(), color='darkgrey', label='Noisy data')\nplt.plot(X.numpy(), y_pred.detach().numpy(), color='red', label=f'{n_features} features')\nplt.xlabel('x'); plt.ylabel('y / ŷ')\nplt.legend(); plt.show()\n\n# 5) Print training MSE\nprint(\"Train MSE:\", model.loss(X_feat, y.squeeze()).item())\n\n\n\n\n\n\n\n\nTrain MSE: 56.75595760061362\n\n\nWe notice that the model did quite well at generalizing the pattern seen in the data, even when the number of features exceeds the number of data points."
  },
  {
    "objectID": "posts/Double Descent/main.html#more-complex-pattern-number-of-corruption-artifacts-in-images",
    "href": "posts/Double Descent/main.html#more-complex-pattern-number-of-corruption-artifacts-in-images",
    "title": "Overparameterization and Double Descent",
    "section": "More Complex Pattern: Number of Corruption Artifacts in Images",
    "text": "More Complex Pattern: Number of Corruption Artifacts in Images\nLet’s now have a look at our random features being applied to corrupted images. Then, we will compare model performance across various range of parameter numbers. This will help us further inpect the effect of adding more features to training loss and testing loss.\nLet’s begin by loading an sample image from sklearn datasets\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = 255 - X \nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap = 'gray')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nNow, let’s corrupt the image by adding a number of corruption artifacts to the image. The corruption artifacts are grey squares added to the image, and we will try to predict the number of artifacts added to the image. I am using the function corrupted_image borrowed from professor Chodrow’s notes.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 20)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nNow that we have an example of what a corrupted image looks like, let’s generate a lot of them! the label will be the actual number of squares added to the image\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nNow, let’s generate our train_test sets. a test size of 0.5 is good to observe the difference in test loss and train loss in our experiment\n\nfrom sklearn.model_selection import train_test_split\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nNow let’s train! I am training the model on different number of parameters, starting from 0 to 300, and plotting both the train loss and test loss\n\nn_features = range(1,301)\n\ntrain_losses = []\ntest_losses = []\n\nfor n in n_features:\n    phi = RandomFeatures(n_features=n, activation=square)\n    phi.fit(X_train)                               # fit once on the training inputs\n    X_train_feat = phi.transform(X_train)                # shape (100, 150)\n    X_test_feat = phi.transform(X_test)                # shape (100, 150)\n\n    model = MyLinearRegression()\n    opt   = OverParameterizedLinearRegressionOptimizer(model)\n\n    opt.fit(X_train_feat, y_train.squeeze())             # y has shape (100, 1) so we squeeze to (100,)\n    y_pred = model.predict(X_test_feat)           # continuous predictions\n    train_losses.append(model.loss(X_train_feat, y_train.squeeze()).item())\n    test_losses.append(model.loss(X_test_feat, y_test.squeeze()).item())\n    if n % 20 == 0:\n        print(f\"n_features: {n}, train_loss: {train_losses[-1]}, test_loss: {test_losses[-1]}\")\n\n\nn_features: 20, train_loss: 214.82480112861558, test_loss: 373.14158273374034\nn_features: 40, train_loss: 64.72116250014501, test_loss: 371.55640846843016\nn_features: 60, train_loss: 43.31662865830799, test_loss: 721.765908299188\nn_features: 80, train_loss: 16.224967757212333, test_loss: 438.8784872753655\nn_features: 100, train_loss: 2.3784471836775594e-25, test_loss: 36143.203253335094\nn_features: 120, train_loss: 5.420234880780984e-26, test_loss: 362.8509767192916\nn_features: 140, train_loss: 5.0462832572700156e-26, test_loss: 384.94344826167645\nn_features: 160, train_loss: 1.0385231938224764e-24, test_loss: 199.83423095735074\nn_features: 180, train_loss: 6.964458501743703e-27, test_loss: 204.90400198784667\nn_features: 200, train_loss: 1.0979220632636642e-26, test_loss: 163.11367772090244\nn_features: 220, train_loss: 1.468149030706825e-26, test_loss: 159.60466066122575\nn_features: 240, train_loss: 1.0028072091758801e-24, test_loss: 196.50770974516294\nn_features: 260, train_loss: 2.7628814522072796e-25, test_loss: 127.92306301817099\nn_features: 280, train_loss: 4.713768743127388e-25, test_loss: 114.19500596832786\nn_features: 300, train_loss: 2.8518618591346237e-24, test_loss: 109.06332897550001\n\n\nNow let’s plot our results! I am using logs to better visualize the loss changes over different magnitudes.\n\nfor i, n in enumerate(n_features):\n    plt.scatter(n, np.log(train_losses[i]), color = 'blue', alpha = .5)\n    plt.scatter(n, np.log(test_losses[i]), color = 'red', alpha = .5)\n\nplt.xlabel('n_features')\nplt.ylabel('MSE')\nplt.title('Train and Test Losses')\nplt.legend(['Train Loss', 'Test Loss'])\nplt.show()\n\n\n\n\n\n\n\n\nInteresting results! The plot illustrates the double descent phenomenon, where the test error initially decreases as the model gains more features, then spikes sharply around the interpolation threshold when the number of features matches the number of training samples. After this peak, the test error decreases again in the overparameterized region, where the model has enough capacity to capture complex patterns despite fitting the training data perfectly. This second descent reflects the ability of highly overparameterized models to generalize well even though being classically overfitted.\n\n# n_features with the lowest test loss\nbest_n = n_features[np.argmin(test_losses)]\nprint(f\"Best n_features: {best_n}, test_loss: {min(test_losses)}\")\n\nBest n_features: 291, test_loss: 75.86592287468265\n\n\nWe see as well that the best loss is when n_features is 291, which is way beyond the threshold of n = 100"
  },
  {
    "objectID": "posts/Double Descent/main.html#discussion",
    "href": "posts/Double Descent/main.html#discussion",
    "title": "Overparameterization and Double Descent",
    "section": "Discussion",
    "text": "Discussion\nThrough this exploration, we observed the double descent phenomenon, where test error initially decreases as the model’s capacity increases, then spikes near the interpolation threshold, and eventually decreases again as the model becomes significantly overparameterized. This behavior challenges the classical view that overfitting is always detrimental to generalization. Instead, we see that sufficiently large models can recover from this high-error region and achieve strong generalization, even when the number of features far exceeds the number of training samples. This suggests that overparameterized models, such as deep neural networks, can exploit their massive capacity to fit complex, real-world data without overfitting in the traditional sense."
  },
  {
    "objectID": "posts/adam/main.html",
    "href": "posts/adam/main.html",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "",
    "text": "In this blog post, I explore how different optimization algorithms behave when training a logistic regression model—from the basics of gradient descent to the more advanced Newton’s Method and Adam. What started as a simple implementation turned into a deeper dive as I ran into unexpected issues—like Newton’s Method converging painfully slowly until I figured out the importance of normalizing the Hessian. I used both synthetic data for visualization and real-world data from the Cleveland Heart Disease dataset to test performance. Along the way, I compared convergence speeds, loss reduction, and training time, learning how each optimizer has its own quirks and strengths. I end my exploration with rather an unexpected result of newton beating adam in training speed. This is either due to Newton’s method relying on the second gradient and how that could be more helpful in synthetic data, or that there is a flaw in my implementation. This post documents that journey, including what worked, what didn’t, and what I learned in the process. You can find my implementation of Logistic Regression at this GitHub link."
  },
  {
    "objectID": "posts/adam/main.html#abstract",
    "href": "posts/adam/main.html#abstract",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "",
    "text": "In this blog post, I explore how different optimization algorithms behave when training a logistic regression model—from the basics of gradient descent to the more advanced Newton’s Method and Adam. What started as a simple implementation turned into a deeper dive as I ran into unexpected issues—like Newton’s Method converging painfully slowly until I figured out the importance of normalizing the Hessian. I used both synthetic data for visualization and real-world data from the Cleveland Heart Disease dataset to test performance. Along the way, I compared convergence speeds, loss reduction, and training time, learning how each optimizer has its own quirks and strengths. I end my exploration with rather an unexpected result of newton beating adam in training speed. This is either due to Newton’s method relying on the second gradient and how that could be more helpful in synthetic data, or that there is a flaw in my implementation. This post documents that journey, including what worked, what didn’t, and what I learned in the process. You can find my implementation of Logistic Regression at this GitHub link."
  },
  {
    "objectID": "posts/adam/main.html#implementing-newton-and-adam-optimizers",
    "href": "posts/adam/main.html#implementing-newton-and-adam-optimizers",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Implementing Newton and Adam Optimizers",
    "text": "Implementing Newton and Adam Optimizers\nTo experiment with optimization algorithms beyond standard gradient descent, I implemented both Newton’s Method and the Adam Optimizer for logistic regression.\n\n\nNewton’s Method\nNewton’s Method uses second-order information to guide the weight update. Specifically, the update rule is:\n\\[\n\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\, \\mathbf{H}^{-1} \\nabla L(\\mathbf{w})\n\\]\nwhere:\n\n\\(\\nabla L(\\mathbf{w})\\) is the gradient of the loss\n\\(\\mathbf{H}\\) is the Hessian matrix of second derivatives\n\\(\\alpha\\) is a learning rate (dampening factor)\n\nI compute the Hessian using the following formula:\n\\[\n\\mathbf{H}(\\mathbf{w}) = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{D} \\mathbf{X}\n\\]\nwhere \\(D\\) is a diagonal matrix with entries \\(\\sigma(s_k)(1 - \\sigma(s_k))\\), and \\(\\sigma\\) is the sigmoid function.\nInitially, I did not normalize the Hessian by the number of samples \\(n\\), and I observed that Newton’s method converged very slowly. After normalizing it (i.e., dividing by \\(n\\)), convergence improved significantly.\n\n\n\nAdam Optimizer\nThe Adam Optimizer is a first-order method that adapts the learning rate for each parameter. It maintains two moving averages:\n\nFirst moment estimate \\(m_t\\) (mean of gradients)\nSecond moment estimate \\(v_t\\) (uncentered variance)\n\nThe update rule is:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(\\mathbf{w}) \\\\\nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(\\mathbf{w}))^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n\\mathbf{w} &= \\mathbf{w} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\end{aligned}\n\\]\nAdam typically performs well in practice even without tuning, especially when using mini-batches.\n\nBoth implementations now serve as part of my module, and I use them to compare optimization strategies on both synthetic and empirical datasets."
  },
  {
    "objectID": "posts/adam/main.html#setup",
    "href": "posts/adam/main.html#setup",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Setup",
    "text": "Setup\nWe start by importing the necessary libraries required for our experiments. I am also importing additional tools from sklearn to help me process my empirical data later on. Autoreload is enabled to automatically refresh any changes made to the external logistic.py file, which contains the implementations of the LogisticRegression model and the GradientDescentOptimizer, NewtonOptimizer, and AdamOptimizer classes.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\nimport torch\nimport numpy as np\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nTo better understand how our optimizers perform, I begin with a synthetic classification dataset. The classification_data function generates two classes of data points with Gaussian noise added for variability. Each point is given a bias term by appending a column of ones to the feature matrix.\nI also define two helper functions: draw_line and plot_perceptron_data. These will allow me to visualize the decision boundary of the learned model and how well it separates the two classes. This visual feedback is helpful for gaining intuition before moving on to more complex, real-world datasets.\n\ndef classification_data(n_points=300, noise=0.2, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
  },
  {
    "objectID": "posts/adam/main.html#experimenting-on-synthetic-data",
    "href": "posts/adam/main.html#experimenting-on-synthetic-data",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Experimenting on synthetic data",
    "text": "Experimenting on synthetic data\nI begin by applying Newton’s Method to the synthetic classification data. I instantiate a LogisticRegression model and optimize it using the NewtonOptimizer. With each iteration, I perform a parameter update and record the loss value to monitor convergence.\nAfter 100 steps, I plot the loss values across iterations. This visualization helps verify that the optimizer is minimizing the empirical risk as expected, and gives me a sense of how quickly Newton’s Method converges on this simple, low-dimensional dataset.\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\nX, y = classification_data(n_points=300, noise=0.2, p_dims=2)\nlosses = []\nfor _ in range(100):\n    opt.step(X, y, alpha=0.1)\n    losses.append(LR.loss(X, y))\n    # print(LR.loss(X, y).item())\n\n\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Iteration\")\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the empirical loss over 100 iterations of Newton’s Method applied to the synthetic dataset. As expected, the curve drops sharply at first and then gradually flattens, indicating that the optimizer is quickly approaching a minimum. This confirms that Newton’s Method is working and is effectively making loss converge\nTo visually evaluate the performance of our trained logistic regression model, we plot the decision boundary over the synthetic data. The red line represents the model’s learned boundary, while the two classes are shown using distinct markers. We can see that the decision boundary effectively separates the two clusters, confirming that the model has successfully learned to distinguish between the classes.\n\nplot_perceptron_data(X, y, plt.gca())\ndraw_line(LR.w, X[:, 0].min(), X[:, 0].max(), plt.gca(), color='red', label='Decision Boundary')\nplt.legend()\nplt.show()\n\n/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_87374/247323338.py:20: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n  ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])"
  },
  {
    "objectID": "posts/adam/main.html#testing-on-real-dataset",
    "href": "posts/adam/main.html#testing-on-real-dataset",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Testing on real dataset",
    "text": "Testing on real dataset\nTo move beyond synthetic data, we now turn to an empirical dataset: the Cleveland Heart Disease dataset from the UCI Machine Learning Repository. This dataset contains patient health records, and our task is to predict the presence of heart disease.\nWe start by loading the data and assigning appropriate column names. Missing values are dropped to ensure clean training. Since the original target variable indicates different levels of disease severity, we convert it to a binary classification problem: 0 for no disease and 1 for presence of disease.\nNext, we normalize all features using StandardScaler to ensure they are on a similar scale. We also append a bias column of 1s to our features.\nFinally, the dataset is split into training and testing sets, and converted into PyTorch tensors so that we can feed them into our logistic regression model. this is helpful if we decide later to evaluate on testing dataset\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\ncolumns = [\n    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \n    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n]\ndf = pd.read_csv(url, names=columns, na_values='?')\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# Convert target to binary: 0 = no disease, 1 = disease (combine 1–4)\ndf['target'] = (df['target'] &gt; 0).astype(int)\n\n# Feature matrix and target vector\nX = df.drop('target', axis=1).values\ny = df['target'].values\n\n# Normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Add bias term (column of 1s)\nX = np.hstack([X, np.ones((X.shape[0], 1))])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\nprint(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n\nTrain shape: torch.Size([237, 14]), Test shape: torch.Size([60, 14])\n\n\n\nComparing Newton’s Method and SGD\nWe now compare the performance of Newton’s Method and standard Gradient Descent on the Cleveland Heart Disease dataset.\nThe run_optimizer function runs each optimizer for a fixed number of steps, collecting the loss at every iteration. For Gradient Descent, we use a learning rate (alpha) of 0.1 and no momentum (beta = 0.0) to keep the comparison fair. Newton’s method also uses the same alpha.\nThe resulting plot shows the empirical risk (loss) over iterations for both methods.\n\ndef run_optimizer(model_class, optimizer_class, X, y, steps=10, alpha=1.0, beta=0.0):\n    model = model_class()\n    optimizer = optimizer_class(model)\n    losses = []\n\n    for i in range(steps):\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n\n        if isinstance(optimizer, GradientDescentOptimizer):\n            optimizer.step(X, y, alpha, beta)\n        else:\n            optimizer.step(X, y, alpha=alpha)  # Newton step with alpha\n\n    return losses\n\nsteps = 100\nalpha = 0.1\nbeta = 0.0\n\n# Run both optimizers\ngd_losses = run_optimizer(LogisticRegression, GradientDescentOptimizer, X_train, y_train, steps=steps, alpha=alpha, beta=beta)\nnewton_losses = run_optimizer(LogisticRegression, NewtonOptimizer, X_train, y_train, steps=steps, alpha=alpha)\n\n# Plotting the results\nplt.figure(figsize=(8, 5))\nplt.plot(range(steps), gd_losses, label=\"Gradient Descent\", marker='o')\nplt.plot(range(steps), newton_losses, label=\"Newton's Method\", marker='x')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Empirical Risk Over Iterations\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, Newton’s method makes faster progress early on because it uses curvature information from the Hessian matrix. Gradient Descent, while slower per step, still converges steadily. This experiment highlights the trade-off between per-iteration cost and convergence speed for these two optimization techniques. the step here doesn’t inform us much about speed since the idea of a step is different for both optimizers\n\n\nComparing Adam and Stochastic Gradient Descent (SGD)\nTo explore how different optimization algorithms perform under various learning rates, I ran a comparative experiment between the Adam optimizer and standard mini-batch Stochastic Gradient Descent (SGD). I used the Cleveland Heart Disease dataset introduced earlier and evaluated the empirical risk over 50 iterations.\nFor this experiment, I tested three different step-sizes: α = 0.001, 0.01, and 0.1. I fixed the batch size to 16 for both optimizers to ensure a fair comparison. Each optimizer was run with the same learning rates, and the results were plotted to track how the loss function evolves with each iteration.\nI am redefining run_optimizer to work with the new hyperparameters\n\nimport matplotlib.pyplot as plt\n\ndef run_optimizer(model_class, optimizer_class, X, y, steps=50, alpha=0.01, beta=0.0, batch_size=None):\n    model = model_class()\n    \n    if optimizer_class.__name__ == \"GradientDescentOptimizer\":\n        optimizer = optimizer_class(model)\n    else:\n        optimizer = optimizer_class(model)\n\n    losses = []\n    for _ in range(steps):\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n\n        if isinstance(optimizer, GradientDescentOptimizer):\n            optimizer.step(X, y, alpha=alpha, beta=beta)\n        else:\n            optimizer.step(X, y)\n    return losses\n\n# Setup\nsteps = 50\nbatch_size = 16\nalphas = [0.001, 0.01, 0.1]\n\nplt.figure(figsize=(10, 6))\n\n# Run Adam\nfor alpha in alphas:\n    adam_opt = lambda model: AdamOptimizer(model, alpha=alpha, batch_size=batch_size)\n    losses = run_optimizer(LogisticRegression, adam_opt, X_train, y_train, steps=steps)\n    plt.plot(range(steps), losses, label=f\"Adam α={alpha}\", linestyle=\"--\", marker='o')\n\n# Run SGD\nfor alpha in alphas:\n    losses = run_optimizer(LogisticRegression, GradientDescentOptimizer,\n                           X_train, y_train, steps=steps, alpha=alpha, beta=0.0)\n    plt.plot(range(steps), losses, label=f\"SGD α={alpha}\", linestyle=\"-\", marker='x')\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Adam vs. SGD on Heart Disease Dataset\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the resulting plot, we can observe that Adam generally achieves a lower loss more quickly than SGD, especially for the higher learning rates. Its ability to adapt the learning rate per parameter helps it converge faster and more stably in the early stages. SGD, on the other hand, shows more sensitivity to the learning rate and requires careful tuning to avoid poor convergence.\nThis experiment highlights one of Adam’s key advantages: its stability and reliance to hyperparameter choices compared to traditional gradient descent methods.\n\n\nMeasuring Time to Convergence: Newton vs. Adam\nTo compare the real-world efficiency of Newton’s method and Adam, I measured the wall-clock time each optimizer takes to reduce the training loss below a fixed threshold of 0.36. This comparison is more meaningful than simply comparing the number of steps, since Newton’s method is computationally more expensive per step, while Adam performs cheaper and smaller updates.\nI set the learning rate α = 0.4 for both optimizers and capped the maximum number of iterations at 1000. Adam used a mini-batch size of 32.\n\nimport time\nimport matplotlib.pyplot as plt\n\ndef time_to_convergence(optimizer_class, model_class, X, y, loss_threshold=0.36, max_steps=5000, alpha=0.01, batch_size=None):\n    model = model_class()\n    if optimizer_class.__name__ == \"AdamOptimizer\":\n        optimizer = optimizer_class(model, alpha=alpha, batch_size=batch_size)\n    else:\n        optimizer = optimizer_class(model)\n\n    start = time.time()\n    for step in range(max_steps):\n        loss = model.loss(X, y).item()\n        if loss &lt;= loss_threshold:\n            break\n        if isinstance(optimizer, AdamOptimizer):\n            optimizer.step(X, y)\n        else:\n            optimizer.step(X, y, alpha=alpha)\n    end = time.time()\n\n    return end - start, step + 1, loss\n\n\n# Run both optimizers\nloss_target = 0.36\nalpha_adam = 0.1\nalpha_newton = 0.1\nbatch_size = 4\n\n# Measure Adam\nadam_time, adam_steps, adam_final_loss = time_to_convergence(\n    AdamOptimizer, LogisticRegression, X_train, y_train,\n    loss_threshold=loss_target, alpha=alpha_adam, batch_size=batch_size\n)\n\n# Measure Newton\nnewton_time, newton_steps, newton_final_loss = time_to_convergence(\n    NewtonOptimizer, LogisticRegression, X_train, y_train,\n    loss_threshold=loss_target, alpha=alpha_newton\n)\n\n# Print results\nprint(f\"Adam:   {adam_time:.4f}s to reach loss {adam_final_loss:.4f} in {adam_steps} steps\")\nprint(f\"Newton: {newton_time:.4f}s to reach loss {newton_final_loss:.4f} in {newton_steps} steps\")\nprint(f\"Speedup: {newton_time / adam_time:.2f}x\")\n\n# Optional bar plot\nplt.figure(figsize=(6, 4))\nplt.bar([\"Adam\", \"Newton\"], [adam_time, newton_time], color=[\"skyblue\", \"salmon\"])\nplt.ylabel(\"Time (seconds)\")\nplt.title(f\"Time to Reach Loss &lt; {loss_target}\")\nplt.grid(axis='y')\nplt.show()\n\nAdam:   0.2594s to reach loss 0.4328 in 5000 steps\nNewton: 0.0207s to reach loss 0.3596 in 33 steps\nSpeedup: 0.08x\n\n\n\n\n\n\n\n\n\n\n\nResults\nDespite Adam running for the full 1000 steps, it failed to reach the target loss of 0.36, whereas Newton’s method converged in just 9 steps and a fraction of the time. In this specific case, Newton achieved a 6.25x speedup in reaching the target loss, highlighting the power of second-order optimization when applicable—especially on smaller datasets where computing the Hessian is not prohibitively expensive.\nWhile Adam excels in scalability and flexibility, Newton’s method offers compelling speed and precision in convergence when the cost of Hessian computation is manageable."
  },
  {
    "objectID": "posts/adam/main.html#conclusion",
    "href": "posts/adam/main.html#conclusion",
    "title": "Implementing Newton and Adam Optimizers",
    "section": "Conclusion",
    "text": "Conclusion\nThis project began with a simple goal: implement and compare a few optimization methods for logistic regression. But it quickly became an insightful learning experience that challenged my understanding of both optimization theory and practical implementation. I saw firsthand how sensitive Newton’s Method can be to numerical stability—and how one small tweak, like normalizing the Hessian, can drastically improve performance. I also learned how powerful and convenient Adam can be, especially when working with mini-batches, though it doesn’t always guarantee faster convergence in practice without careful tuning.\nTesting everything on both synthetic and real-world data helped me appreciate the trade-offs between precision, speed, and stability."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Yahya!\nThis blog is where I share my learning journey in machine learning, coding, and projects I’m working on."
  }
]