[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Yahya!\nThis blog is where I share my learning journey in machine learning, coding, and projects I’m working on."
  },
  {
    "objectID": "posts/Blog1/blog1.html",
    "href": "posts/Blog1/blog1.html",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\n\nCode\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n\nCode\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\n\nCode\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n\nCode\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\n\nCode\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n\n0.984375\n\n\n\n\nCode\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\n\nCode\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\n\nCode\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\n\n\nCode\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/Blog1/blog1.html#abstract",
    "href": "posts/Blog1/blog1.html#abstract",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "",
    "text": "This analysis explores penguin species classification using machine learning models based on physical measurements from the Palmer Penguins dataset. We apply exploratory data analysis (EDA) to visualize feature distributions and quantify feature separability using Intersection over Union (IoU). Through this process, we identify Flipper Length (mm) and Delta 13 C (o/oo) as the most discriminative quantitative features and select Island as the best categorical feature. We train and evaluate multiple models, including Logistic Regression, Decision Trees, and Random Forest, achieving high accuracy on the test set, with Random Forest performing best. Decision boundary visualizations and a confusion matrix confirm that Gentoo penguins are easily separable, while Adelie and Chinstrap penguins exhibit some overlap. This study highlights the impact of feature selection and model choice in species classification, demonstrating the strength of tree-based ensemble models for this task.\n\n\nWe begin by loading the Palmer Penguins dataset, which contains detailed measurements of three penguin species observed in Antarctica. This dataset is widely used for classification tasks and provides valuable insights into species differentiation based on physical attributes.\nBy loading the dataset into a Pandas DataFrame, we gain access to numerical and categorical features that will later be used for exploratory data analysis and machine learning modeling.\n\n\nCode\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\n\nTo prepare the dataset for machine learning, we perform several preprocessing steps:\n\nDropping Irrelevant Columns: Features such as studyName, Sample Number, Individual ID, Date Egg, Comments, and Region do not contribute to species classification and are removed.\nHandling Missing Values: Any rows with missing data are discarded to ensure clean input.\nFiltering Outliers: Entries where Sex is \".\" are removed to maintain data consistency.\nEncoding Categorical Features:\n\nThe species labels are transformed into numerical values using LabelEncoder.\nOne-hot encoding is applied to categorical variables like Sex and Island, converting them into binary columns suitable for machine learning models.\n\n\nAfter completing these steps, the dataset is structured and ready for exploration.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nTo gain insights into the relationships between quantitative features and species differentiation, we use a pair plot. This visualization helps us identify patterns and clusters that could be useful for classification.\n\nSelected Features:\nWe focus on six key quantitative features:\n\nCulmen Length (mm)\n\nCulmen Depth (mm)\n\nFlipper Length (mm)\n\nBody Mass (g)\n\nDelta 15 N (o/oo)\n\nDelta 13 C (o/oo)\n\nColoring by Species:\nUsing Seaborn’s pairplot, we scatter-plot each feature against every other feature. The points are color-coded by species, allowing us to observe feature distributions and separability among species.\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \n                         \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nsns.pairplot(X_train[quantitative_features].assign(Species=le.inverse_transform(y_train)), hue=\"Species\") #assign is coloring the points by species\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe pair plot provides a comprehensive visualization of how different quantitative features relate to each other across the three penguin species. The scatter plots reveal clustering patterns, while the diagonal histograms illustrate individual feature distributions.\n\n\n\nGentoo Penguins Are Distinct:\n\nGentoo penguins (orange) are well-separated from the other two species across most features.\n\nTheir flipper length and body mass are notably larger, making these strong distinguishing features.\n\nAdelie vs. Chinstrap Penguins Have Overlapping Features:\n\nWhile still somewhat distinguishable, Adelie (green) and Chinstrap (blue) penguins share more similar feature distributions.\n\nSome overlap exists in culmen length, culmen depth, and body mass, making classification between these two species more challenging.\n\nStrong Separability Across All Features:\n\nEach feature individually does a good job of separating species, but some pairs of features work better together.\n\nFor example, culmen length vs. culmen depth shows strong species clustering.\n\n\n\n\n\nSince we are aiming to classify species using only two quantitative features, we need to carefully select the best combination.\nMoving forward, we will explore these features quantitatively and evaluate their classification power through machine learning models.\n\n\n\n\nTo better understand how the quantitative features are distributed across different penguin species, we plot density histograms (KDE plots). This allows us to examine:\n\nWhether each feature follows a normal distribution across species.\n\nHow the means and standard deviations of each feature differ between species.\n\nWhich features provide clear separation between species and which ones have more overlap.\n\nBy analyzing these distributions, we can gain insights into the most discriminative features for classification and validate our earlier observations from the pair plot.\n\n\nCode\n# Select numerical features\nnumeric_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n# Set up the figure\nfig, axes = plt.subplots(len(numeric_features), 1, figsize=(8, 5 * len(numeric_features)))\n\n# Plot histogram curves colored by species\nfor i, feature in enumerate(numeric_features):\n    ax = axes[i]\n    sns.kdeplot(data=train, x=feature, hue=\"Species\", fill=True, common_norm=False, ax=ax, alpha=0.3)\n    ax.set_title(f\"Density Plot of {feature} by Species\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe density plots confirm that most quantitative features follow an approximately normal distribution for each species. This suggests that modeling assumptions based on normality (e.g., logistic regression) may be reasonable.\n\n\n\nClear Separability in Some Features\n\nCulmen length and flipper length show well-separated peaks, indicating strong potential for classification.\n\nGentoo penguins (orange) consistently occupy distinct regions, especially in body mass and culmen length.\n\nOverlap in Certain Features\n\nCulmen depth and Delta 15 N exhibit significant overlap between Adelie and Chinstrap penguins.\n\nThis suggests that using these features alone may not be sufficient for high classification accuracy.\n\nImplication for Feature Selection\n\nOverlapping distributions might still be useful—features with moderate overlap could help define class boundaries.\n\nTo quantify separability, we will compute the Intersection over Union (IoU) between distributions. This will help us determine which features provide the best class separation for classification tasks.\n\n\n\n\n\n\nTo quantify how well each feature separates penguin species, we calculate the Intersection over Union (IoU) for the probability density functions of each species. IoU provides a numerical measure of how much overlap exists between the distributions, with lower IoU scores indicating better feature separability.\n\n\n\nWe compute Kernel Density Estimation (KDE) for each species to approximate the continuous probability distribution of each feature.\n\nThe intersection between two species’ distributions is calculated as the minimum value at each point.\n\nThe union is the maximum value at each point.\n\nIoU is measured by dividing intersection area over union\nA lower IoU score means that the species distributions are more distinct, making the feature a stronger candidate for classification.\n\n\n\nCode\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\n# Select numerical features\nspecies = train[\"Species\"].unique()\n\niou_scores = {}\n\n# Compute IoU for each numerical feature\nfor feature in numeric_features:    \n\n    x_min, x_max = train[feature].min(), train[feature].max()\n    x_range = np.linspace(x_min, x_max, 500)\n    \n    kde_dict = {}\n\n    # Compute histogram curve for each species\n    for spec in species:\n        subset = train[train[\"Species\"] == spec][feature].dropna()\n        kde = gaussian_kde(subset)(x_range)  # Get KDE values\n        kde /= kde.sum()  # Normalize to sum to 1 (PDF-like)\n        kde_dict[spec] = kde\n\n    # Compute IoU for each species pair\n    iou_values = []\n    species_pairs = [(species[i], species[j]) for i in range(len(species)) for j in range(i + 1, len(species))]\n    \n    for s1, s2 in species_pairs:\n        intersection = np.minimum(kde_dict[s1], kde_dict[s2]).sum()\n        union = np.maximum(kde_dict[s1], kde_dict[s2]).sum()\n        iou = intersection / union\n        iou_values.append(iou)\n    \n    # Average IoU across species pairs\n    iou_scores[feature] = np.mean(iou_values)\n\n# Convert IoU scores to DataFrame\niou_df = pd.DataFrame.from_dict(iou_scores, orient=\"index\", columns=[\"IoU Score\"])\n\n# Display IoU scores\nprint(\"IoU Scores for Feature Separation:\")\nprint(iou_df.sort_values(by=\"IoU Score\", ascending=True))\n\n\nIoU Scores for Feature Separation:\n                     IoU Score\nFlipper Length (mm)   0.199396\nDelta 13 C (o/oo)     0.237063\nDelta 15 N (o/oo)     0.250275\nCulmen Length (mm)    0.259422\nBody Mass (g)         0.341673\nCulmen Depth (mm)     0.353053\n\n\n\n\n\n\nFlipper Length (mm) and Delta 13 C (o/oo) have the lowest IoU scores, indicating that they offer the best species separation.\n\nCulmen Depth (mm) and Body Mass (g) have higher IoU scores, meaning that species distributions overlap more, making classification harder.\n\nFlipper Length (mm), in particular, appears to be a very strong candidate for a distinguishing feature, as its species distributions have minimal overlap.\n\n\n\n\n\nIn this section, we examine the maximum category proportion across three categorical features—Island, Sex, and Clutch Completion. A lower maximum proportion indicates a more balanced feature, where the data points are more evenly distributed among the categories. We then select the feature with the lowest maximum proportion, as it is generally more information for classification.\n\n\nCode\n# Define categorical feature groups\nfeature_groups = {\n    \"Island\": [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"],\n    \"Sex\": [\"Sex_FEMALE\", \"Sex_MALE\"],\n    \"Clutch Completion\": [\"Clutch Completion_Yes\", \"Clutch Completion_No\"]\n}\n\n# We'll store the maximum occurrence for each group,\n# but ultimately we'll pick the feature with the *lowest* maximum occurrence.\nmax_percentages = {}\n\nfor group_name, feature_list in feature_groups.items():\n    # Calculate the proportion for each category\n    category_counts = X_train[feature_list].sum(axis=0) / len(X_train)\n    \n    # The maximum proportion in this group\n    max_prop = category_counts.max() * 100\n    max_percentages[group_name] = max_prop\n\n# Convert to DataFrame\nmax_percentage_df = pd.DataFrame.from_dict(max_percentages, orient=\"index\", columns=[\"Max Category %\"])\n\n#we will pick the feature with the lowest maximum percentage since it means the feature is more balanced\nbest_feature = max_percentage_df[\"Max Category %\"].idxmin()\n\nprint(\"\\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\\n\")\nprint(max_percentage_df.sort_values(by=\"Max Category %\", ascending=True))\n\nprint(f\"\\n Based on more balanced distribution, the best categorical feature is: {best_feature}\")\n\n\n\nFeature Maximum Occurrence Percentages (Lower is Better for Class Separation):\n\n                   Max Category %\nIsland                  48.828125\nSex                     51.562500\nClutch Completion       89.062500\n\n Based on more balanced distribution, the best categorical feature is: Island\n\n\nBased on these results, Island looks like it is the most balanced categorical feature, with the smallest maximum proportion among its categories. This indicates it has greater potential to help our model distinguish between different penguin species more effectively compared to the other options.\n\n\nCode\nsorted_IOU = sorted(iou_scores.items(), key=lambda x: x[1], reverse=False)\nsorted_IOU\n\n\n[('Flipper Length (mm)', 0.1993963154709996),\n ('Delta 13 C (o/oo)', 0.23706316175669087),\n ('Delta 15 N (o/oo)', 0.25027533453950507),\n ('Culmen Length (mm)', 0.25942230819048473),\n ('Body Mass (g)', 0.341672714279104),\n ('Culmen Depth (mm)', 0.3530527962578189)]\n\n\n\n\n\nNow that we have selected our features, we train a Logistic Regression model using:\n- The two best quantitative features, identified based on their IoU scores (features with the least overlap).\n- The best categorical feature, determined from the most balanced distribution among categories.\nBy fitting the model on the training data, we can evaluate its performance and see how well it distinguishes between penguin species.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\ncols = [sorted_IOU[i][0] for i in range(2)]\ncols += feature_groups[best_feature]\nLR = LogisticRegression(max_iter= 1500)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n\n0.984375\n\n\n\n\nCode\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n\n0.9558823529411765\n\n\n\n\n\nThe Logistic Regression model achieves:\n- 94.9% accuracy on the training set, indicating that it successfully captures the patterns in the data.\n- 95.6% accuracy on the test set, suggesting that the model generalizes well to unseen data.\nThe result is good with logistic regression! let us try a couple more models from Scikit-learn. mainly, random forest and decision trees.\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nTR = DecisionTreeClassifier()\nTR.fit(X_train[cols], y_train)\nTR_score = TR.score(X_train[cols], y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nRF_score = RF.score(X_train[cols], y_train)\nprint(f\"Decision Tree Accuracy: {TR_score}\")\nprint(f\"Random Forest Accuracy: {RF_score}\")\n\n\nDecision Tree Accuracy: 1.0\nRandom Forest Accuracy: 1.0\n\n\n\n\n\nAfter achieving strong results with Logistic Regression, we now test two tree-based models from Scikit-learn:\n- Decision Tree Classifier: A simple, interpretable model that splits data based on feature thresholds.\n- Random Forest Classifier: A method that builds multiple decision trees and averages their predictions for better generalization.\nBoth models achieve 100% accuracy on the training set, indicating they perfectly classify the training data. While this is promising, it may also suggest potential overfitting, which we will further evaluate by testing on unseen data. let us try them on test set\n\n\nCode\nTR_test_score = TR.score(X_test[cols], y_test)\nprint(f\"Decision Tree Test Accuracy: {TR_test_score}\")\n\nRF_test_score = RF.score(X_test[cols], y_test)\nprint(f\"Random Forest Test Accuracy: {RF_test_score}\")\n\n\nDecision Tree Test Accuracy: 0.9411764705882353\nRandom Forest Test Accuracy: 0.9558823529411765\n\n\n\n\n\nThe Decision Tree model experiences a slight drop in accuracy compared to training, suggesting some overfitting to the training set.\n\nThe Random Forest model maintains a high test accuracy, showing better generalization due to its ensemble approach.\n\nOverall, Random Forest emerges as the best-performing model, combining high accuracy with strong generalization. This makes it a robust choice for classifying penguin species.\n\n\n\n\nThis function plots decision regions for a given classification model, allowing us to visually inspect how the model separates penguin species based on the selected features.\n\n\nCode\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \n\n\n\n\nCode\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots above display the decision regions of our Logistic Regression model, showing how it classifies penguin species based on Flipper Length (mm) and Delta 13 C (o/oo) across different islands.\n\n\n\nDistinct Separation on Dream Island:\n\nChinstrap Penguins (green) are clearly separated from Gentoo Penguins (blue).\n\nThe model effectively differentiates species here, suggesting strong feature separability.\n\nOverlap in Biscoe and Torgersen Islands:\n\nIn Island_Biscoe and Island_Torgersen, the decision boundaries are less distinct, and Adelie Penguins (red) share space with Gentoo Penguins (blue).\n\nThis suggests some overlap in feature distributions, making classification more challenging.\n\nDecision Boundaries Align with Feature Trends:\n\nThe separation between species aligns with what we observed in the pair plots and IoU analysis—certain species are inherently more distinguishable based on our selected features.\n\n\nOverall, this visualization helps us interpret model decisions and highlights where our chosen features perform well versus where improvements might be needed. let’s now check out confusion matrix\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_test[cols])\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 7))\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix for Logistic Regression on Test Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows that Logistic Regression performs well, with only a few misclassifications:\n\nAdelie: 29 correct, 1 misclassified as Chinstrap, 1 as Gentoo.\n\nChinstrap: 10 correct, 1 misclassified as Adelie.\n\nGentoo: Perfect classification (26/26 correct).\n\nKey Takeaways:\n- Gentoo penguins are easiest to classify, aligning with earlier findings.\n- Adelie and Chinstrap have some overlap, leading to minor misclassification.\n- Using more complex models (like Random Forest) may further refine classification.\nA better performance for our model could be to pick better features using a better algorithms."
  },
  {
    "objectID": "posts/Blog1/blog1.html#discussion",
    "href": "posts/Blog1/blog1.html#discussion",
    "title": "Understanding Feature Separability in Penguin Classification",
    "section": "Discussion",
    "text": "Discussion\nThrough this analysis, I gained insights into the importance of feature selection in classification tasks. My IoU-based approach helped quantify feature separability, leading to better-informed choices. Visualization techniques, such as pair plots and decision boundaries, provided valuable interpretability into the model’s behavior. The performance comparison between Logistic Regression, Decision Trees, and Random Forest demonstrated that Decision Trees models generalize best, minimizing overfitting while maintaining high accuracy. One key takeaway is that some species are inherently harder to classify due to overlapping feature distributions, reinforcing the need for careful feature engineering and model selection. Future improvements could involve non-linear models like SVMs and better feature selection to further enhance classification accuracy. This project shows how data-driven feature selection and model evaluation can lead to meaningful and accurate species classification."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Machine Learning Blog",
    "section": "",
    "text": "Understanding Feature Separability in Penguin Classification\n\n\n\n\n\n\nMachine Learning\n\n\nProjects\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nYahya Rahhawi\n\n\n\n\n\n\nNo matching items"
  }
]